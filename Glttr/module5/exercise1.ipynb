{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {},
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823a4916-1a3a-4f43-989e-4b9441cc142d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Make a complete analysis on data preprocessing\n",
        "# Inconsistencies\n",
        "# Duplicates (data.duplicated().sum())\n",
        "# Missing values (data.isnull().sum())\n",
        "# Categorical\n",
        "# Outliers\n",
        "# Feature Engineering\n",
        "# Feature Selection and/or Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df = pd.concat([df_train, df_test], axis=0)\n",
        "data_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c046b07a-845c-460b-a692-27a97ec3d613",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "    \n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "    \n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df['date'] = pd.to_datetime(data_df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df['wind_speed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data_df, 'electricity_demand', '2017-09-01', '2017-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "478e5a58",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data_df, 'temperature_station1', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data_df, 'humidity', '2016-06-01', '2016-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc14e432",
      "metadata": {},
      "source": [
        "### Data analysis by Guillaume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9d5afb",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad08dd9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40f1cd3",
      "metadata": {},
      "source": [
        "#### Dates format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb865031",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "# print(data_df['date'].apply(lambda x: not re.match(r'\\d{4}-\\d{2}-\\d{2}', x)).sum())\n",
        "test = pd.to_datetime(data_df['date'])\n",
        "# data_df['date'] = pd.to_datetime(data_df['date'])\n",
        "test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed96f268",
      "metadata": {},
      "source": [
        "There is no probleme with date format : no need to make changes.\n",
        "There is a problem with speed, we need to either have all values in m/s or km/h."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cf36ac",
      "metadata": {},
      "source": [
        "#### Speed unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55148ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_wind_speed(string):\n",
        "    if isinstance(string, (int, float)):\n",
        "        return string\n",
        "    elif pd.isna(string):\n",
        "        return string\n",
        "    elif string[-1] == 'h':\n",
        "        return float(string.replace('km/h', '')) * 3.6\n",
        "    else:\n",
        "        return float(string.replace('m/s', ''))\n",
        "    \n",
        "print(convert_wind_speed('100 km/h'))\n",
        "print(convert_wind_speed('100 m/s'))\n",
        "\n",
        "test = data_df.copy()\n",
        "test['wind_speed'] = data_df['wind_speed'].apply(convert_wind_speed)\n",
        "test.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77af835e",
      "metadata": {},
      "source": [
        "#### Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b945248",
      "metadata": {},
      "outputs": [],
      "source": [
        "test = data_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79368177",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test.shape)\n",
        "test.drop_duplicates(inplace=True)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de21f1ee",
      "metadata": {},
      "source": [
        "#### Categorical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6b3d41",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65504c0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "categorical_columns = ['weather_condition', 'oil_brent_price_indicator']\n",
        "df = df[categorical_columns]\n",
        "print(df['weather_condition'].value_counts())\n",
        "df['oil_brent_price_indicator'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75d4de2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df = data_df.copy()\n",
        "categorical_columns = ['weather_condition', 'oil_brent_price_indicator']\n",
        "df = df[categorical_columns]\n",
        "print(\"Original DataFrame:\")\n",
        "print(df)\n",
        "\n",
        "# One-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "print(\"One-hot encoded DataFrame:\")\n",
        "print(df_encoded)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8dd1c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "\n",
        "# Diagnose\n",
        "print(\"Duplicate index?\", df.index.has_duplicates)\n",
        "print(\"Duplicate columns?\", df.columns.duplicated().any())\n",
        "print(df.columns[df.columns.duplicated()])\n",
        "\n",
        "# Fixes\n",
        "df = df.reset_index(drop=True)                     # handles duplicate index\n",
        "df = df.loc[:, ~df.columns.duplicated()]          # drops duplicate-named columns, if any\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631ebea8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# df = data_df.drop_duplicates().copy()\n",
        "\n",
        "# Count plot for 'weather_condition'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='weather_condition', data=df, palette='viridis')\n",
        "plt.title('Count Plot of weather_condition')\n",
        "plt.xlabel('weather_condition')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for average rating by color\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='weather_condition', y='electricity_demand', data=df, ci=None, palette='viridis')\n",
        "plt.title('Average electricity_demand by weather_condition')\n",
        "plt.xlabel('weather_condition')\n",
        "plt.ylabel('Average electricity_demand')\n",
        "plt.show()\n",
        "\n",
        "# Stacked bar plot for size distribution by color\n",
        "size_color = pd.crosstab(df['weather_condition'], df['oil_brent_price_indicator'])\n",
        "size_color_pct = size_color.div(size_color.sum(1), axis=0)\n",
        "size_color_pct.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "plt.title('oil_brent_price_indicator Distribution by weather_condition')\n",
        "plt.xlabel('weather_condition')\n",
        "plt.ylabel('Percentage')\n",
        "plt.legend(title='oil_brent_price_indicator', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {},
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86971ab4-1ef8-464b-afb5-0d750a8c4035",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide a complete data preprocessing transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb77bd06",
      "metadata": {},
      "source": [
        "#### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# 1. Handle Inconsistencies\n",
        "def handle_inconsistencies(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        X_train['wind_speed'] = X_train['wind_speed'].apply(convert_wind_speed)\n",
        "        X_val['wind_speed'] = X_val['wind_speed'].apply(convert_wind_speed)\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        X_train['wind_speed'] = X_train['wind_speed'].apply(convert_wind_speed)\n",
        "        return X_train.copy()\n",
        "\n",
        "# 2. Handling Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        X_train_no_duplicates = X_train.drop_duplicates().copy()\n",
        "        y_train_no_duplicates = y_train.loc[X_train_no_duplicates.index]\n",
        "        X_val_no_duplicates = X_val.drop_duplicates().copy()\n",
        "        return X_train_no_duplicates, y_train_no_duplicates, X_val_no_duplicates\n",
        "    else:\n",
        "        X_train_no_duplicates = X_train.copy()\n",
        "        y_train_no_duplicates = y_train.loc[X_train_no_duplicates.index]\n",
        "        return X_train_no_duplicates, y_train_no_duplicates\n",
        "\n",
        "\n",
        "def handle_missing_values(X_train, y_train=None, X_val=None):\n",
        "    # Colonnes numériques et catégorielles\n",
        "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
        "    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "\n",
        "    # Remplissage sur X_train\n",
        "    X_train[num_cols] = X_train[num_cols].fillna(-1)\n",
        "    X_train[cat_cols] = X_train[cat_cols].fillna(\"missing\")\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val[num_cols] = X_val[num_cols].fillna(-1)\n",
        "        X_val[cat_cols] = X_val[cat_cols].fillna(\"missing\")\n",
        "        return X_train.copy(), y_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy()\n",
        "\n",
        "\n",
        "# 4. Handling Categorical Values\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "    cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "    cat_cols = cat_cols.drop('date')\n",
        "    if X_val is not None:\n",
        "        for col in cat_cols:\n",
        "            labelencoder = LabelEncoder()\n",
        "            X_train[col] = labelencoder.fit_transform(X_train[col])\n",
        "            X_val[col] = labelencoder.transform(X_val[col])\n",
        "        return X_train.copy(), y_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        for col in cat_cols:\n",
        "            labelencoder = LabelEncoder()\n",
        "            X_train[col] = labelencoder.fit_transform(X_train[col])\n",
        "        return X_train.copy(), y_train.copy()\n",
        "\n",
        "# 5. Handling Outliers\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy(), y_train\n",
        "\n",
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy()\n",
        "\n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    selected_columns = ['weather_condition', 'humidity', 'wind_speed', 'oil_brent_price_indicator', 'temperature_station1',\n",
        "       'temperature_station2', 'temperature_station3', 'temperature_station4',\n",
        "       'temperature_station5', 'temperature_station6', 'temperature_station7',\n",
        "       'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns], X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77aec91",
      "metadata": {},
      "source": [
        "#### Pipeline function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    # ### call transformations here, if there is no learning and no need to be crossval\n",
        "    # X = handle_inconsistencies(X, y)\n",
        "    # X, y = handle_duplicates(X, y)\n",
        "    # X = handle_missing_values(X, y)\n",
        "    # X, y = handle_categorical(X, y)\n",
        "    # # X, y = handle_outliers(X, y)\n",
        "    # # X, y = feature_engineering(XX, y)\n",
        "    # X = feature_selection(X, y)\n",
        "    \n",
        "    X = X.copy().sort_values('date')\n",
        "    y = y.copy().reindex(X.index).reset_index(drop=True)\n",
        "    X = X.reset_index(drop=True)\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val= handle_duplicates(X_train, y_train, X_val)\n",
        "        y_val = y_val.loc[X_val.index]\n",
        "        X_train, y_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        y_val = y_val.loc[X_val.index]\n",
        "        X_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    \n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc6828d7",
      "metadata": {},
      "source": [
        "#### First try : simple case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1286c1e0",
      "metadata": {},
      "source": [
        "#### Second try : handling outliers "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2104bd76",
      "metadata": {},
      "source": [
        "Going to try to handle odd values "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c9afa8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7266d0da",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "def detect_outliers_zscore(data, threshold=5):\n",
        "    z_scores = np.abs(stats.zscore(data, nan_policy='omit'))\n",
        "    return data[z_scores > threshold]\n",
        "\n",
        "df1 = df.select_dtypes(include=[np.number])\n",
        "for col in df1.columns:\n",
        "    outliers = detect_outliers_zscore(df[col])\n",
        "    print(outliers)\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdd7fc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(20, 15))  \n",
        "df = df.reset_index(drop=True)\n",
        "for i, col in enumerate(df1.columns, start=1):\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(5, 6, i)\n",
        "    sns.boxplot(x=col, data=df)\n",
        "    plt.title(f'{col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceeb46f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 5. New Handling Outliers\n",
        "# def handle_outliers(X_train, y_train, X_val=None, y_val=None):\n",
        "#     df_num = X_train.select_dtypes(include=[np.number])    # Sélection des colonnes numériques\n",
        "#     outlier_indices_train = set()\n",
        "#     outlier_indices_val = set()\n",
        "#     if X_val is not None:\n",
        "#         for col in df_num.columns:\n",
        "#             outliers_train = detect_outliers_zscore(X_train[col])\n",
        "#             outliers_val = detect_outliers_zscore(X_val[col])\n",
        "#             if not outliers_train.empty or not outliers_val.empty:\n",
        "#                 outlier_indices_train.update(outliers_train.index)\n",
        "#                 outlier_indices_val.update(outliers_val.index)\n",
        "#         X_train, X_val = X_train.drop(index=outlier_indices_train), X_val.drop(index=outlier_indices_val)\n",
        "#         y_train, y_val = y_train.loc[X_train.index], y_val.loc[X_val.index]\n",
        "#         return X_train.copy(), y_train, X_val.copy(), y_val.copy()\n",
        "#     else:\n",
        "#         for col in df_num.columns:\n",
        "#             outliers_train = detect_outliers_zscore(X_train[col])\n",
        "#             if not outliers_train.empty:\n",
        "#                 outlier_indices_train.update(outliers_train.index)\n",
        "#         X_train = X_train.drop(index=outlier_indices_train)\n",
        "#         y_train = y_train.loc[X_train.index]\n",
        "#         return X_train.copy(), y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b3f89b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    df_num = X_train.select_dtypes(include=[np.number])\n",
        "    outlier_indices_train, outlier_indices_val = set(), set()\n",
        "\n",
        "    if X_val is not None:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            outliers_X_val = detect_outliers_zscore(X_val[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "            if not outliers_X_val.empty: outlier_indices_val.update(outliers_X_val.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        X_val   = X_val.drop(index=outlier_indices_val)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "\n",
        "    else:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a369a067",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89f9399c",
      "metadata": {},
      "source": [
        "Processing fold 1/5...\n",
        "Fold 1 Train MSE: 1208.3526, Validation MSE: 1525.8924\n",
        "Processing fold 2/5...\n",
        "Fold 2 Train MSE: 1340.3060, Validation MSE: 1383.4770\n",
        "Processing fold 3/5...\n",
        "Fold 3 Train MSE: 1303.0857, Validation MSE: 164593635.8174\n",
        "Processing fold 4/5...\n",
        "Fold 4 Train MSE: 40840900.6609, Validation MSE: 229449.1393\n",
        "Processing fold 5/5...\n",
        "Fold 5 Train MSE: 32804080.7004, Validation MSE: 119774.5234\n",
        "\n",
        "Train MSE:\n",
        "Mean: 14729766.6211, Max: 40840900.6609, Min: 1208.3526\n",
        "\n",
        "Validation MSE:\n",
        "Mean: 32989153.7699, Max: 164593635.8174, Min: 1383.4770\n",
        "32989153.76988711\n",
        "NO CHANGE !!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f34431",
      "metadata": {},
      "source": [
        "#### Third try : Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d71c8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "corr_matrix = df.corr(numeric_only=True)  # numeric_only=True pour éviter les erreurs avec les colonnes non numériques\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Matrice de corrélation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72aba3c",
      "metadata": {},
      "source": [
        "Toutes les temperature_station* sont corrélées : soit on en garde qu'une, soit on fait une moyenne. \n",
        "On remarque que temperature_station7 est mieux corrélée que les autrs à y. On va voir ce qu'il se trame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218c51af",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data_df, 'temperature_station7', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7df1dfc8",
      "metadata": {},
      "source": [
        "Bah temperature_station7 fait n'importe quoi en gros. On va donc la drop, puis faire une moyenne des autres colonnes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc58cc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca32ad4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    temp_cols = ['temperature_station1', 'temperature_station2', 'temperature_station3', 'temperature_station4', 'temperature_station5', 'temperature_station6', 'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "    if X_val is not None:\n",
        "        X_train, X_val = X_train.drop('temperature_station7', axis=1), X_val.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'], X_val['temperature_mean'] = X_train[temp_cols].mean(axis=1), X_val[temp_cols].mean(axis=1)\n",
        "        X_train, X_val = X_train.drop(columns=temp_cols), X_val.drop(columns=temp_cols)\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        X_train = X_train.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'] = X_train[temp_cols].mean(axis=1)\n",
        "        X_train= X_train.drop(columns=temp_cols)\n",
        "        return X_train.copy()\n",
        "    \n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    selected_columns = ['weather_condition', 'humidity', 'wind_speed', 'oil_brent_price_indicator', 'temperature_mean']\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns], X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec0da9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    df_num = X_train.select_dtypes(include=[np.number])\n",
        "    outlier_indices_train, outlier_indices_val = set(), set()\n",
        "\n",
        "    if X_val is not None:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            outliers_X_val = detect_outliers_zscore(X_val[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "            if not outliers_X_val.empty: outlier_indices_val.update(outliers_X_val.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        X_val   = X_val.drop(index=outlier_indices_val)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "\n",
        "    else:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9430ddc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c88fc90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing fold 1/5...\n",
        "# Fold 1 Train MSE: 1208.3526, Validation MSE: 1525.8924\n",
        "# Processing fold 2/5...\n",
        "# Fold 2 Train MSE: 1340.3060, Validation MSE: 1383.4770\n",
        "# Processing fold 3/5...\n",
        "# Fold 3 Train MSE: 1303.0857, Validation MSE: 164914287.1979\n",
        "# Processing fold 4/5...\n",
        "# Fold 4 Train MSE: 40870901.0860, Validation MSE: 234699.9561\n",
        "# Processing fold 5/5...\n",
        "# Fold 5 Train MSE: 32845232.1681, Validation MSE: 123151.9030\n",
        "\n",
        "# Train MSE:\n",
        "# Mean: 14743996.9997, Max: 40870901.0860, Min: 1208.3526\n",
        "\n",
        "# Validation MSE:\n",
        "# Mean: 33055009.6853, Max: 164914287.1979, Min: 1383.4770"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd668085",
      "metadata": {},
      "source": [
        "#### Fourth try : handle better missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47d448a",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data_df, 'temperature_station1', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee3bbfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.drop_duplicates().copy()\n",
        "df.isna().sum()\n",
        "df = df.ffill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576944a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(df, 'temperature_station1', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2d72f63",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.plot(y='date', use_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed3cbae",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.sort_values('date').reset_index(drop=True).plot(y='date', use_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c161d5a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.sort_values('date').reset_index(drop=True).copy()\n",
        "df = df.ffill()\n",
        "plot_feature_over_time(df, 'temperature_station1', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b40ecad",
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = data_df.copy()\n",
        "df1 = df1.ffill().bfill()\n",
        "print(df1.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984ba4fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.sort_values('date').reset_index(drop=True).copy()\n",
        "df = df.fillna(method='ffill')\n",
        "print(df.isna().sum())\n",
        "plot_feature_over_time(df, 'temperature_station2', '2017-09-01', '2019-10-01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b61c3f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        X_train, X_val = X_train.sort_values('date'), X_val.sort_values('date')\n",
        "        y_train = y_train.reindex(X_train.index).reset_index(drop=True)\n",
        "        X_train, X_val = X_train.reset_index(drop=True).ffill().bfill(), X_val.ffill().bfill()\n",
        "        return X_train.copy(), y_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        X_train = X_train.sort_values('date')\n",
        "        y_train = y_train.reindex(X_train.index).reset_index(drop=True)\n",
        "        X_train = X_train.reset_index(drop=True).ffill().bfill()\n",
        "        return X_train.copy(), y_train.copy()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4d57f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    temp_cols = ['temperature_station1', 'temperature_station2', 'temperature_station3', 'temperature_station4', 'temperature_station5', 'temperature_station6', 'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "    if X_val is not None:\n",
        "        X_train, X_val = X_train.drop('temperature_station7', axis=1), X_val.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'], X_val['temperature_mean'] = X_train[temp_cols].mean(axis=1), X_val[temp_cols].mean(axis=1)\n",
        "        X_train, X_val = X_train.drop(columns=temp_cols), X_val.drop(columns=temp_cols)\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        X_train = X_train.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'] = X_train[temp_cols].mean(axis=1)\n",
        "        X_train= X_train.drop(columns=temp_cols)\n",
        "        return X_train.copy()\n",
        "    \n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    selected_columns = ['weather_condition', 'humidity', 'wind_speed', 'oil_brent_price_indicator', 'temperature_mean']\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns], X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1c5de8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    df_num = X_train.select_dtypes(include=[np.number])\n",
        "    outlier_indices_train, outlier_indices_val = set(), set()\n",
        "\n",
        "    if X_val is not None:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            outliers_X_val = detect_outliers_zscore(X_val[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "            if not outliers_X_val.empty: outlier_indices_val.update(outliers_X_val.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        X_val   = X_val.drop(index=outlier_indices_val)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "\n",
        "    else:\n",
        "        for col in df_num.columns:\n",
        "            outliers_X_train = detect_outliers_zscore(X_train[col])\n",
        "            if not outliers_X_train.empty: outlier_indices_train.update(outliers_X_train.index)\n",
        "\n",
        "        outliers_y_train = detect_outliers_zscore(y_train)\n",
        "        if not outliers_y_train.empty: outlier_indices_train.update(outliers_y_train.index)\n",
        "\n",
        "        X_train = X_train.drop(index=outlier_indices_train)\n",
        "        y_train = y_train.loc[X_train.index]\n",
        "        return X_train.copy(), y_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b81756e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38e54d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processing fold 1/5...\n",
        "# Fold 1 Train MSE: 1229.5997, Validation MSE: 1502.3436\n",
        "# Processing fold 2/5...\n",
        "# Fold 2 Train MSE: 1356.9838, Validation MSE: 1355.6208\n",
        "# Processing fold 3/5...\n",
        "# Fold 3 Train MSE: 1315.7972, Validation MSE: 1494.4123\n",
        "# Processing fold 4/5...\n",
        "# Fold 4 Train MSE: 1357.5094, Validation MSE: 1407.0646\n",
        "# Processing fold 5/5...\n",
        "# Fold 5 Train MSE: 1364.8561, Validation MSE: 1248.3827\n",
        "\n",
        "# Train MSE:\n",
        "# Mean: 1324.9492, Max: 1364.8561, Min: 1229.5997\n",
        "\n",
        "# Validation MSE:\n",
        "# Mean: 1401.5648, Max: 1502.3436, Min: 1248.3827\n",
        "# 1401.5648162556543"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1d8018",
      "metadata": {},
      "source": [
        "#### Fith try : Scaling and normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e685e6a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing_X_y(X, y):\n",
        "    X = X.copy().sort_values('date')\n",
        "    y = y.copy().reindex(X.index).reset_index(drop=True)\n",
        "    X = X.reset_index(drop=True)    \n",
        "    X = handle_inconsistencies(X, y)\n",
        "    X, y = handle_duplicates(X, y)\n",
        "    X, y = handle_missing_values(X, y)\n",
        "    X, y = handle_categorical(X, y)\n",
        "    X, y = handle_outliers(X, y,)\n",
        "    X  = feature_engineering(X, y)\n",
        "    X  = feature_selection(X, y)\n",
        "    return X.copy(), y.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c46d48",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, Normalizer, RobustScaler\n",
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "X, y = preprocessing_X_y(X, y)\n",
        "\n",
        "# Function to train and evaluate\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
        "    # model = KNeighborsRegressor(n_neighbors=5)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    mse_train = mean_squared_error(y_train, model.predict(X_train))\n",
        "    mse_test = mean_squared_error(y_test, model.predict(X_test))\n",
        "    return mse_train, mse_test\n",
        "\n",
        "\n",
        "# Function to evaluate scaling strategy using cross-validation\n",
        "def evaluate_scaling_strategy(X, y, scaler, name):\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    # kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    train_mse_scores = []\n",
        "    test_mse_scores = []\n",
        "    \n",
        "    # for train_index, test_index in kf.split(X):\n",
        "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "        \n",
        "        if scaler:\n",
        "            X_train_scaled = scaler.fit_transform(X_train)\n",
        "            X_test_scaled = scaler.transform(X_test)\n",
        "        else:\n",
        "            X_train_scaled, X_test_scaled = X_train, X_test\n",
        "        \n",
        "        mse_train, mse_test = train_and_evaluate(X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "        \n",
        "        train_mse_scores.append(mse_train)\n",
        "        test_mse_scores.append(mse_test)\n",
        "    \n",
        "    return {\n",
        "        'name': name,\n",
        "        'train_mean': np.mean(train_mse_scores),\n",
        "        'train_min': np.min(train_mse_scores),\n",
        "        'train_max': np.max(train_mse_scores),\n",
        "        'test_mean': np.mean(test_mse_scores),\n",
        "        'test_min': np.min(test_mse_scores),\n",
        "        'test_max': np.max(test_mse_scores)\n",
        "    }\n",
        "\n",
        "# Evaluate different scaling strategies\n",
        "scaling_strategies = [\n",
        "    (None, 'Original'),\n",
        "    (MinMaxScaler(), 'Min-Max'),\n",
        "    (StandardScaler(), 'Standard'),\n",
        "    (MaxAbsScaler(), 'MaxAbs'),\n",
        "    (Normalizer(norm='l2'), 'L2')\n",
        "]\n",
        "\n",
        "results = []\n",
        "for scaler, name in scaling_strategies:\n",
        "    result = evaluate_scaling_strategy(X, y, scaler, name)\n",
        "    results.append(result)\n",
        "    print(f\"{name} Scaling - Train MSE: {result['train_mean']:.4f}, Test MSE: {result['test_mean']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56b6ff3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original Scaling - Train MSE: 395.2081, Test MSE: 599.0776\n",
        "# Min-Max Scaling - Train MSE: 361.6628, Test MSE: 575.4965\n",
        "# Standard Scaling - Train MSE: 354.4588, Test MSE: 561.9575\n",
        "# MaxAbs Scaling - Train MSE: 317.1361, Test MSE: 505.4162\n",
        "# L2 Scaling - Train MSE: 382.3350, Test MSE: 583.9914"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef18efed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "x = range(len(results))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar([i - width/2 for i in x], [r['train_mean'] for r in results], width, label='Train MSE', alpha=0.8)\n",
        "plt.bar([i + width/2 for i in x], [r['test_mean'] for r in results], width, label='Test MSE', alpha=0.8)\n",
        "\n",
        "for i, r in enumerate(results):\n",
        "    plt.errorbar(i - width/2, r['train_mean'], yerr=[[r['train_mean']-r['train_min']], [r['train_max']-r['train_mean']]], fmt='none', capsize=5, color='black')\n",
        "    plt.errorbar(i + width/2, r['test_mean'], yerr=[[r['test_mean']-r['test_min']], [r['test_max']-r['test_mean']]], fmt='none', capsize=5, color='black')\n",
        "\n",
        "plt.xlabel('Scaling Strategy')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Comparison of Scaling Strategies')\n",
        "plt.xticks(x, [r['name'] for r in results])\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5474f3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_standard_scaler(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train).copy()\n",
        "        X_val_scaled = scaler.transform(X_val).copy()\n",
        "        return X_train_scaled, X_val_scaled\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train).copy()\n",
        "        return X_train_scaled\n",
        "    \n",
        "\n",
        "def normalize(X_train, y_train, X_val=None, scaler=Normalizer(norm='l2')):\n",
        "    if X_val is not None:\n",
        "        X_train_scaled = scaler.fit_transform(X_train).copy()\n",
        "        X_val_scaled = scaler.transform(X_val).copy()\n",
        "        return X_train_scaled, X_val_scaled\n",
        "    else:\n",
        "        X_train_scaled = scaler.fit_transform(X_train).copy()\n",
        "        return X_train_scaled\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d6d9e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### Sort datasets by date\n",
        "    X = X.copy().sort_values('date')\n",
        "    y = y.copy().reindex(X.index).reset_index(drop=True)\n",
        "    X = X.reset_index(drop=True)\n",
        "\n",
        "    ### Deal with absurd data (electricity demand)\n",
        "    q_low, q_high = y.quantile(0.01), y.quantile(0.99)\n",
        "    mask = y.between(q_low, q_high)\n",
        "    X, y = X[mask], y[mask]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "        print(X_train.shape)\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val= handle_duplicates(X_train, y_train, X_val)\n",
        "        y_val = y_val.loc[X_val.index]\n",
        "        X_train, y_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        y_val = y_val.loc[X_val.index]\n",
        "        X_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "        # X_train, X_val = normalize_standard_scaler(X_train, y_train, X_val)\n",
        "        X_train, X_val = normalize(X_train, y_train, X_val, scaler)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eecabd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15d820a",
      "metadata": {},
      "source": [
        "#### Feature engineeringv2 : add interestings columns "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d65eb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "def add_datetime_decomposition(df, datetime_col):\n",
        "    df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
        "    df['year'] = df[datetime_col].dt.year\n",
        "    df['month'] = df[datetime_col].dt.month\n",
        "    df['day'] = df[datetime_col].dt.day\n",
        "    df['hour'] = df[datetime_col].dt.hour\n",
        "    df['dayofweek'] = df[datetime_col].dt.dayofweek\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
        "    return df\n",
        "df = add_datetime_decomposition(df, datetime_col='date')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7a98fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "# Creating a 'temperature_feel' feature (heat index simplified formula)\n",
        "def add_temperature_feel(df, temperature_col, humidity_col):\n",
        "    df['temperature_feel'] = df[temperature_col] - 0.55 * (1 - df[humidity_col] / 100) * (df[temperature_col] - 14.5)\n",
        "    return df\n",
        "\n",
        "df = add_temperature_feel(df[['temperature_station1', 'humidity']], 'temperature_station1', 'humidity')\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d14f72",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = data_df.copy()\n",
        "# New transformation 2: Compute feels_like temperature\n",
        "def compute_feels_like(temp, windspeed, humidity):\n",
        "    feels_like = np.zeros_like(temp)\n",
        "    \n",
        "    # Heat index calculation (for high temperatures)\n",
        "    mask_hi = temp > 27\n",
        "    hi = -8.78469475556 + 1.61139411 * temp + 2.33854883889 * humidity\n",
        "    hi += -0.14611605 * temp * humidity - 0.012308094 * temp**2\n",
        "    hi += -0.0164248277778 * humidity**2 + 0.002211732 * temp**2 * humidity\n",
        "    hi += 0.00072546 * temp * humidity**2 - 0.000003582 * temp**2 * humidity**2\n",
        "    feels_like[mask_hi] = hi[mask_hi]\n",
        "    \n",
        "    # Wind chill calculation (for low temperatures with wind)\n",
        "    mask_wc = (temp <= 10) & (windspeed > 4.8)\n",
        "    wc = 13.12 + 0.6215 * temp - 11.37 * windspeed**0.16 + 0.3965 * temp * windspeed**0.16\n",
        "    feels_like[mask_wc] = wc[mask_wc]\n",
        "    \n",
        "    # If neither condition is met, return the actual temperature\n",
        "    mask_normal = ~(mask_hi | mask_wc)\n",
        "    feels_like[mask_normal] = temp[mask_normal]\n",
        "    \n",
        "    return feels_like\n",
        "df['feels_like'] = compute_feels_like(df['temperature_station1'], df['wind_speed'].apply(convert_wind_speed), df['humidity'])\n",
        "df[['temperature_station1', 'humidity', 'wind_speed', 'feels_like']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaac9798",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    temp_cols = ['temperature_station1', 'temperature_station2', 'temperature_station3', 'temperature_station4', 'temperature_station5', 'temperature_station6', 'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "    if X_val is not None:\n",
        "        X_train, X_val = X_train.drop('temperature_station7', axis=1), X_val.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'], X_val['temperature_mean'] = X_train[temp_cols].mean(axis=1), X_val[temp_cols].mean(axis=1)\n",
        "        # X_train, X_val = X_train.drop(columns=temp_cols), X_val.drop(columns=temp_cols)\n",
        "        X_train, X_val = add_datetime_decomposition(X_train, datetime_col='date'), add_datetime_decomposition(X_val, datetime_col='date')\n",
        "        X_train['feels_like'] = compute_feels_like(X_train['temperature_mean'], X_train['wind_speed'], X_train['humidity'])\n",
        "        X_val['feels_like'] = compute_feels_like(X_val['temperature_mean'], X_val['wind_speed'], X_val['humidity'])\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        X_train = X_train.drop('temperature_station7', axis=1)\n",
        "        X_train['temperature_mean'] = X_train[temp_cols].mean(axis=1)\n",
        "        # X_train= X_train.drop(columns=temp_cols)\n",
        "        X_train = add_datetime_decomposition(X_train, datetime_col='date')\n",
        "        X_train['feels_like'] = compute_feels_like(X_train['temperature_mean'], X_train['wind_speed'], X_train['humidity'])\n",
        "        return X_train.copy()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578b0cf3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5466ed3f",
      "metadata": {},
      "source": [
        "#### Feature selection "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f417d0d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    selected_columns = [\n",
        "        'weather_condition', 'humidity', 'wind_speed', 'oil_brent_price_indicator', 'temperature_mean', \n",
        "        'feels_like', 'year', 'month', 'day', 'hour', 'dayofweek', 'is_weekend'\n",
        "                        ]\n",
        "    # selected_columns = X_train.drop('date', axis=1).columns\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns], X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdbde5c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def feature_selection(X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame=None):\n",
        "    \"\"\"\n",
        "    Sélection univariée de features adaptée à la régression.\n",
        "    - Fit sur X_train/y_train\n",
        "    - Transform sur X_train (et X_val si fourni)\n",
        "    - Retourne (X_train_sel, X_val_sel) ou X_train_sel si X_val est None\n",
        "    \"\"\"\n",
        "    \n",
        "    selected_columns = [\n",
        "        'weather_condition', 'humidity', 'wind_speed', 'oil_brent_price_indicator', 'temperature_mean', \n",
        "        'feels_like', 'year', 'month', 'day', 'hour', 'dayofweek', 'is_weekend'\n",
        "                        ]\n",
        "    X_train = X_train[selected_columns]\n",
        "    if X_val is not None:\n",
        "        X_val = X_val[selected_columns]\n",
        "    \n",
        "    # Select score\n",
        "    p = X_train.shape[1]\n",
        "    score_func = f_regression  # alternatif : mutual_info_regression\n",
        "    k = min(30, max(5, int(np.sqrt(p))))\n",
        "    k = min(k, p)\n",
        "    \n",
        "    # Select best features\n",
        "    selector = SelectKBest(score_func=score_func, k=k)\n",
        "    selector.fit(X_train, y_train)\n",
        "    support = selector.get_support(indices=True)\n",
        "    selected_cols = X_train.columns[support]\n",
        "    \n",
        "    # Transform\n",
        "    X_train_sel = X_train[selected_cols].copy()\n",
        "    if X_val is not None:\n",
        "        X_val_sel = X_val[selected_cols].copy()\n",
        "    \n",
        "    # Affiche les features choisies\n",
        "    print(f\"[feature_selection] p={p} -> k={k} | selected: {list(selected_cols)}\")\n",
        "    \n",
        "    return (X_train_sel, X_val_sel) if X_val is not None else X_train_sel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18feb7a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {},
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and submit your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X_train and y_train from your data\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    ### Sort datasets by date\n",
        "    X_train = X_train.copy().sort_values('date')\n",
        "    y_train = y_train.copy().reindex(X_train.index).reset_index(drop=True)\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    X_test = X_test.copy().sort_values('date').reset_index(drop=True)\n",
        "\n",
        "    ### Deal with absurd data (electricity demand)\n",
        "    q_low, q_high = y_train.quantile(0.01), y_train.quantile(0.99)\n",
        "    mask = y_train.between(q_low, q_high)\n",
        "    X_train, y_train = X_train[mask], y_train[mask]\n",
        "\n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "    X_train, X_test = normalize(X_train, y_train, X_test, scaler)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call serve_model to train and predict\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b69eab",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf014c9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(X_test['date'], y_test_pred)\n",
        "# plt.xticks(rotation=45)\n",
        "plt.xticks(X_test['date'][::20], rotation=45)\n",
        "# plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

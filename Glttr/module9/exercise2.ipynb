{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEAUjjhOb136"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uVgWUZjpb137"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!pip install gymnasium==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Oa03cAjLb138"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZwAAf2b139"
   },
   "source": [
    "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQQPZhpb139"
   },
   "source": [
    "### Objective\n",
    "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        action = self.env.action_space.sample() # your logic here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
    "\n",
    "Holes in the ice are distributed in set locations.\n",
    "\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
    "\n",
    "The environment is based on :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for FrozenLake-v1 8x8 (stochastic, is_slippery=True).\n",
    "    - Trains in __init__ on the provided env with an epsilon/alpha decay.\n",
    "    - Uses greedy policy at evaluation time (choose_action).\n",
    "    - Performs a tiny online update during evaluation to be robust.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = int(getattr(env.observation_space, \"n\"))\n",
    "        self.nA = int(getattr(env.action_space, \"n\"))\n",
    "\n",
    "        # Q-table\n",
    "        self.Q = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
    "\n",
    "        # Hyperparams — choisis pour 8x8 glissant\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration (epsilon) : décroit lentement puis se fige à 0.1\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end   = 0.10\n",
    "        self.eps_decay_episodes = 40000  # nb d'épisodes pour passer de start à end\n",
    "\n",
    "        # Learning rate (alpha) : décroit jusqu'à 0.10\n",
    "        self.alpha_start = 1.0\n",
    "        self.alpha_end   = 0.10\n",
    "        self.alpha_decay_episodes = 40000\n",
    "\n",
    "        # Entraînement\n",
    "        self.max_steps_per_ep = 200\n",
    "        self.train_episodes   = 60000  # assez pour dépasser nettement 0.35 sur 8x8\n",
    "\n",
    "        # Variables pour MAJ en ligne pendant l’éval\n",
    "        self._last_state: Optional[int] = None\n",
    "        self._last_action: Optional[int] = None\n",
    "        self._eval_alpha = 0.05  # petit pas d’apprentissage pendant l’éval\n",
    "\n",
    "        self._train_q_learning()\n",
    "        # On se remet au propre pour l’évaluation\n",
    "        self.env.reset()\n",
    "        self._last_state, self._last_action = None, None\n",
    "\n",
    "    # --- Utilitaires de scheduling ---\n",
    "    def _linear_sched(self, start, end, step, total_steps):\n",
    "        if total_steps <= 0:\n",
    "            return end\n",
    "        frac = min(1.0, max(0.0, step / total_steps))\n",
    "        return start + (end - start) * frac\n",
    "\n",
    "    # --- Entraînement Q-Learning offline sur l'env fourni ---\n",
    "    def _train_q_learning(self):\n",
    "        for ep in range(self.train_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Schedules\n",
    "            eps   = self._linear_sched(self.eps_start,   self.eps_end,   ep, self.eps_decay_episodes)\n",
    "            alpha = self._linear_sched(self.alpha_start, self.alpha_end, ep, self.alpha_decay_episodes)\n",
    "\n",
    "            for _ in range(self.max_steps_per_ep):\n",
    "                # ε-greedy\n",
    "                if np.random.random() < eps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(self.Q[state]))\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Q-learning update: Q(s,a) ← Q + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
    "                best_next = float(np.max(self.Q[next_state]))\n",
    "                td_target = float(reward) + self.gamma * best_next * (0.0 if terminated else 1.0)\n",
    "                self.Q[state, action] += alpha * (td_target - float(self.Q[state, action]))\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"\n",
    "        MAJ Q en ligne sur la transition précédente (si dispo), puis action greedy.\n",
    "        Gère reward=None au premier pas et coupe le bootstrap si l'étape précédente était terminale.\n",
    "        \"\"\"\n",
    "        # Détection début d'épisode (reward=None dans ta boucle) : on réinitialise la mémoire online\n",
    "        if reward is None:\n",
    "            self._last_state, self._last_action = None, None\n",
    "            r_prev = 0.0\n",
    "        else:\n",
    "            r_prev = float(reward)\n",
    "\n",
    "        # Si on a la transition précédente (s_{t-1}, a_{t-1}, r_{t-1}, s_t), on fait la MAJ\n",
    "        if self._last_state is not None and self._last_action is not None and observation is not None:\n",
    "            s_prev = int(self._last_state)\n",
    "            a_prev = int(self._last_action)\n",
    "            s_curr = int(observation)\n",
    "\n",
    "            best_next = float(np.max(self.Q[s_curr]))\n",
    "            # Si l'étape précédente était terminale, pas de bootstrap\n",
    "            bootstrap = 0.0 if (terminated or truncated) else (self.gamma * best_next)\n",
    "            td_target = r_prev + bootstrap\n",
    "            self.Q[s_prev, a_prev] += self._eval_alpha * (td_target - float(self.Q[s_prev, a_prev]))\n",
    "\n",
    "        # Politique greedy sur l'état courant\n",
    "        state = int(observation)\n",
    "        action = int(np.argmax(self.Q[state]))\n",
    "\n",
    "        # Mémoriser pour la prochaine MAJ online\n",
    "        self._last_state = state\n",
    "        self._last_action = action\n",
    "\n",
    "        return action\n",
    "\n",
    "    # # --- Politique greedy (avec petite MAJ online) ---\n",
    "    # def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "    #     \"\"\"\n",
    "    #     Pendant l’évaluation:\n",
    "    #     - on applique une petite MAJ Q(s_last, a_last) -> s_curr avec _eval_alpha\n",
    "    #     - puis on joue l’action greedy sur l’état courant\n",
    "    #     \"\"\"\n",
    "    #     # Si on a une transition (s_{t-1}, a_{t-1}, r_{t-1}, s_t), on fait une MAJ en ligne\n",
    "    #     if self._last_state is not None and self._last_action is not None and observation is not None:\n",
    "    #         s_prev = int(self._last_state)\n",
    "    #         a_prev = int(self._last_action)\n",
    "    #         s_curr = int(observation)\n",
    "    #         best_next = float(np.max(self.Q[s_curr]))\n",
    "    #         # Si l’épisode vient de se terminer, le prochain appel n’arrivera pas,\n",
    "    #         # mais le code appelant passe terminated/truncated sur l'étape suivante.\n",
    "    #         # Ici, on ne connaît que s_curr; on suppose non terminal pour la MAJ online,\n",
    "    #         # ce qui reste conservateur.\n",
    "    #         td_target = float(reward) + self.gamma * best_next\n",
    "    #         self.Q[s_prev, a_prev] += self._eval_alpha * (td_target - float(self.Q[s_prev, a_prev]))\n",
    "\n",
    "    #     # Action greedy\n",
    "    #     state = int(observation)\n",
    "    #     action = int(np.argmax(self.Q[state]))\n",
    "\n",
    "    #     # On mémorise pour la prochaine MAJ online\n",
    "    #     self._last_state = state\n",
    "    #     self._last_action = action\n",
    "\n",
    "    #     return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glttr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    FrozenLake-v1 8x8 (is_slippery=True)\n",
    "    - Entraînement Expected-SARSA tabulaire dans __init__ (time-box + early stop).\n",
    "    - Pénalité de pas minuscule pendant l'entraînement pour encourager des trajets courts.\n",
    "    - Évaluation 100% greedy (aucune mise à jour dans choose_action) -> rapide et stable.\n",
    "    - Respecte strictement le template ML-Arena (self.env = env, signature de choose_action).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env  # requis par le template\n",
    "\n",
    "        # Dimensions\n",
    "        self.nS = int(env.observation_space.n)\n",
    "        self.nA = int(env.action_space.n)\n",
    "\n",
    "        # Q init optimiste -> exploration utile\n",
    "        self.Q = np.full((self.nS, self.nA), 0.5, dtype=np.float32)\n",
    "        # Compteurs pour alpha adaptatif\n",
    "        self.N = np.zeros((self.nS, self.nA), dtype=np.int32)\n",
    "\n",
    "        # Hyperparams stables pour 8x8\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end   = 0.05\n",
    "        self.train_episodes   = 60000\n",
    "        self.max_steps_per_ep = 200\n",
    "\n",
    "        # Garde-fous déploiement\n",
    "        self.time_budget_s = 15.0   # coupe l'entraînement au-delà de 15s\n",
    "        self.early_window  = 500    # fenêtre pour early-stop\n",
    "        self.early_thresh  = 0.45   # stop si >= 45% de succès récents\n",
    "\n",
    "        self._train_expected_sarsa_timeboxed()\n",
    "\n",
    "        # reset propre (silencieux)\n",
    "        try:\n",
    "            self.env.reset()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # -------- utilitaires --------\n",
    "    def _linear_sched(self, start, end, step, total_steps):\n",
    "        if total_steps <= 0:\n",
    "            return end\n",
    "        frac = step / float(total_steps)\n",
    "        if frac < 0.0: frac = 0.0\n",
    "        if frac > 1.0: frac = 1.0\n",
    "        return start + (end - start) * frac\n",
    "\n",
    "    def _greedy_action(self, state):\n",
    "        row = self.Q[state]\n",
    "        m = np.max(row)\n",
    "        idxs = np.flatnonzero(row == m)\n",
    "        return int(np.random.choice(idxs))\n",
    "\n",
    "    def _eps_greedy(self, state, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return int(self.env.action_space.sample())\n",
    "        return self._greedy_action(state)\n",
    "\n",
    "    # -------- entraînement (time-box + early-stop + step penalty) --------\n",
    "    def _train_expected_sarsa_timeboxed(self):\n",
    "        t0 = time.perf_counter()\n",
    "        recent_success = 0\n",
    "        window = self.early_window\n",
    "\n",
    "        for ep in range(self.train_episodes):\n",
    "            # coupe par budget temps\n",
    "            if time.perf_counter() - t0 > self.time_budget_s:\n",
    "                break\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            eps = self._linear_sched(self.eps_start, self.eps_end, ep, self.train_episodes)\n",
    "\n",
    "            for _ in range(self.max_steps_per_ep):\n",
    "                action = self._eps_greedy(state, eps)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # --- shaping très léger pour raccourcir les épisodes ---\n",
    "                # encourage l'agent à atteindre le but rapidement et à éviter de tourner en rond\n",
    "                shaped_r = float(reward) - 0.001\n",
    "\n",
    "                # alpha adaptatif par (s,a)\n",
    "                self.N[state, action] += 1\n",
    "                alpha = 1.0 / np.sqrt(1 + self.N[state, action])\n",
    "\n",
    "                # Expected SARSA\n",
    "                if done:\n",
    "                    expected_next = 0.0\n",
    "                else:\n",
    "                    q_next = self.Q[next_state]\n",
    "                    max_q = np.max(q_next)\n",
    "                    greedy_mask = (q_next == max_q)\n",
    "                    n_greedy = int(np.count_nonzero(greedy_mask))\n",
    "                    pi = np.full(self.nA, eps / self.nA, dtype=np.float32)\n",
    "                    pi[greedy_mask] += (1.0 - eps) / n_greedy\n",
    "                    expected_next = float(np.dot(pi, q_next))\n",
    "\n",
    "                td_target = shaped_r + self.gamma * expected_next\n",
    "                self.Q[state, action] += alpha * (td_target - float(self.Q[state, action]))\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    if reward > 0.0:  # succès réel (but)\n",
    "                        recent_success += 1\n",
    "                    break\n",
    "\n",
    "            # early stop si la moyenne de succès récents est bonne\n",
    "            if ep > 0 and (ep % window) == 0:\n",
    "                mean_recent = recent_success / float(window)\n",
    "                if mean_recent >= self.early_thresh:\n",
    "                    break\n",
    "                recent_success = 0  # reset fenêtre\n",
    "\n",
    "    # -------- politique d'évaluation (greedy, sans update) --------\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        state = int(observation)\n",
    "        return self._greedy_action(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouvelle tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for FrozenLake-v1 8x8 (stochastic, is_slippery=True).\n",
    "    - Trains in __init__ on the provided env with an epsilon/alpha decay.\n",
    "    - Uses greedy policy at evaluation time (choose_action).\n",
    "    - Performs a tiny online update during evaluation to be robust.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = int(getattr(env.observation_space, \"n\"))\n",
    "        self.nA = int(getattr(env.action_space, \"n\"))\n",
    "\n",
    "        # Q-table\n",
    "        self.Q = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
    "\n",
    "        # Hyperparams — choisis pour 8x8 glissant\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration (epsilon) : décroit lentement puis se fige à 0.1\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end   = 0.10\n",
    "        self.eps_decay_episodes = 40000  # nb d'épisodes pour passer de start à end\n",
    "\n",
    "        # Learning rate (alpha) : décroit jusqu'à 0.10\n",
    "        self.alpha_start = 1.0\n",
    "        self.alpha_end   = 0.10\n",
    "        self.alpha_decay_episodes = 40000\n",
    "\n",
    "        # Entraînement\n",
    "        self.max_steps_per_ep = 200\n",
    "        self.train_episodes   = 60000  # assez pour dépasser nettement 0.35 sur 8x8\n",
    "\n",
    "        # Variables pour MAJ en ligne pendant l’éval\n",
    "        self._last_state: Optional[int] = None\n",
    "        self._last_action: Optional[int] = None\n",
    "        self._eval_alpha = 0.05  # petit pas d’apprentissage pendant l’éval\n",
    "\n",
    "        self._train_q_learning()\n",
    "        # On se remet au propre pour l’évaluation\n",
    "        self.env.reset()\n",
    "        self._last_state, self._last_action = None, None\n",
    "\n",
    "    # --- Utilitaires de scheduling ---\n",
    "    def _linear_sched(self, start, end, step, total_steps):\n",
    "        if total_steps <= 0:\n",
    "            return end\n",
    "        frac = min(1.0, max(0.0, step / total_steps))\n",
    "        return start + (end - start) * frac\n",
    "\n",
    "    # --- Entraînement Q-Learning offline sur l'env fourni ---\n",
    "    def _train_q_learning(self):\n",
    "        for ep in range(self.train_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Schedules\n",
    "            eps   = self._linear_sched(self.eps_start,   self.eps_end,   ep, self.eps_decay_episodes)\n",
    "            alpha = self._linear_sched(self.alpha_start, self.alpha_end, ep, self.alpha_decay_episodes)\n",
    "\n",
    "            for _ in range(self.max_steps_per_ep):\n",
    "                # ε-greedy\n",
    "                if np.random.random() < eps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(self.Q[state]))\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Q-learning update: Q(s,a) ← Q + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
    "                best_next = float(np.max(self.Q[next_state]))\n",
    "                td_target = float(reward) + self.gamma * best_next * (0.0 if terminated else 1.0)\n",
    "                self.Q[state, action] += alpha * (td_target - float(self.Q[state, action]))\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"\n",
    "        Évaluation *statique* et rapide :\n",
    "        - AUCUNE mise à jour online (évite les épisodes trop longs / non déterministes).\n",
    "        - Argmax avec départage aléatoire pour limiter les cycles.\n",
    "        \"\"\"\n",
    "        state = int(observation)\n",
    "        row = self.Q[state]\n",
    "        m = np.max(row)\n",
    "        # départage aléatoire entre les meilleures actions\n",
    "        candidates = np.flatnonzero(row == m)\n",
    "        action = int(np.random.choice(candidates))\n",
    "\n",
    "        # on neutralise la mémoire online pour éviter toute MAJ accidentelle\n",
    "        self._last_state, self._last_action = None, None\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glttr 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for FrozenLake-v1 8x8 (stochastic, is_slippery=True).\n",
    "    - Trains in __init__ on the provided env with an epsilon/alpha decay.\n",
    "    - Uses greedy policy at evaluation time (choose_action).\n",
    "    - Performs a tiny online update during evaluation to be robust.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = int(getattr(env.observation_space, \"n\"))\n",
    "        self.nA = int(getattr(env.action_space, \"n\"))\n",
    "\n",
    "        # Q-table\n",
    "        self.Q = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
    "\n",
    "        # Hyperparams — choisis pour 8x8 glissant\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration (epsilon) : décroit lentement puis se fige à 0.1\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end   = 0.10\n",
    "        self.eps_decay_episodes = 40000  # nb d'épisodes pour passer de start à end\n",
    "\n",
    "        # Learning rate (alpha) : décroit jusqu'à 0.10\n",
    "        self.alpha_start = 1.0\n",
    "        self.alpha_end   = 0.10\n",
    "        self.alpha_decay_episodes = 40000\n",
    "\n",
    "        # Entraînement\n",
    "        self.max_steps_per_ep = 200\n",
    "        self.train_episodes   = 60000  # assez pour dépasser nettement 0.35 sur 8x8\n",
    "\n",
    "        # Variables pour MAJ en ligne pendant l’éval\n",
    "        self._last_state: Optional[int] = None\n",
    "        self._last_action: Optional[int] = None\n",
    "        self._eval_alpha = 0.05  # petit pas d’apprentissage pendant l’éval\n",
    "\n",
    "        self._train_q_learning()\n",
    "        # On se remet au propre pour l’évaluation\n",
    "        self.env.reset()\n",
    "        self._last_state, self._last_action = None, None\n",
    "\n",
    "    # --- Utilitaires de scheduling ---\n",
    "    def _linear_sched(self, start, end, step, total_steps):\n",
    "        if total_steps <= 0:\n",
    "            return end\n",
    "        frac = min(1.0, max(0.0, step / total_steps))\n",
    "        return start + (end - start) * frac\n",
    "\n",
    "    # --- Entraînement Q-Learning offline sur l'env fourni ---\n",
    "    def _train_q_learning(self):\n",
    "        for ep in range(self.train_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            # Schedules\n",
    "            eps   = self._linear_sched(self.eps_start,   self.eps_end,   ep, self.eps_decay_episodes)\n",
    "            alpha = self._linear_sched(self.alpha_start, self.alpha_end, ep, self.alpha_decay_episodes)\n",
    "\n",
    "            for _ in range(self.max_steps_per_ep):\n",
    "                # ε-greedy\n",
    "                if np.random.random() < eps:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(self.Q[state]))\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Q-learning update: Q(s,a) ← Q + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
    "                best_next = float(np.max(self.Q[next_state]))\n",
    "                td_target = float(reward) + self.gamma * best_next * (0.0 if terminated else 1.0)\n",
    "                self.Q[state, action] += alpha * (td_target - float(self.Q[state, action]))\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"\n",
    "        Évaluation rapide et stable:\n",
    "        - pas d'update online\n",
    "        - évite l'action inverse immédiate pour casser les cycles\n",
    "        - privilégie RIGHT(2) puis DOWN(1) parmi les meilleurs Q\n",
    "        \"\"\"\n",
    "        state = int(observation)\n",
    "\n",
    "        # Si on est revenu au départ, on oublie l'action précédente\n",
    "        if state == 0 or terminated or truncated:\n",
    "            self._prev_action = None\n",
    "\n",
    "        row = self.Q[state]\n",
    "        m = np.max(row)\n",
    "        candidates = list(np.flatnonzero(row == m))  # meilleures actions\n",
    "\n",
    "        # 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
    "        inverse = {0: 2, 2: 0, 1: 3, 3: 1}\n",
    "\n",
    "        # 1) si plusieurs ex-aequo, évite l'inverse de la dernière action (si possible)\n",
    "        if self._prev_action is not None and len(candidates) > 1:\n",
    "            inv = inverse[self._prev_action]\n",
    "            if inv in candidates:\n",
    "                # ne retire l'inverse que s'il reste au moins une autre meilleure action\n",
    "                tmp = [a for a in candidates if a != inv]\n",
    "                if tmp:\n",
    "                    candidates = tmp\n",
    "\n",
    "        # 2) préférence directionnelle vers l'objectif : RIGHT > DOWN si disponibles\n",
    "        for pref in (2, 1):  # RIGHT, then DOWN\n",
    "            if pref in candidates:\n",
    "                action = int(pref)\n",
    "                break\n",
    "        else:\n",
    "            # sinon, tirage parmi les meilleurs restants\n",
    "            action = int(np.random.choice(candidates))\n",
    "\n",
    "        self._prev_action = action\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glttr 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for FrozenLake-v1 8x8 (stochastic, is_slippery=True).\n",
    "    - Entraîne dans __init__ avec décroissance epsilon/alpha raccourcie.\n",
    "    - Politique greedy à l'évaluation (tie-breaker RIGHT > DOWN > LEFT > UP, évite l'inverse immédiat).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.nS = int(getattr(env.observation_space, \"n\"))\n",
    "        self.nA = int(getattr(env.action_space, \"n\"))\n",
    "\n",
    "        # Q-table\n",
    "        self.Q = np.zeros((self.nS, self.nA), dtype=np.float32)\n",
    "\n",
    "        # Hyperparams\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Exploration / apprentissage - décays plus courts\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end   = 0.10\n",
    "        self.eps_decay_episodes = 30000\n",
    "\n",
    "        self.alpha_start = 1.0\n",
    "        self.alpha_end   = 0.10\n",
    "        self.alpha_decay_episodes = 30000\n",
    "\n",
    "        # Entraînement plus court pour éviter le time limit\n",
    "        self.max_steps_per_ep = 100\n",
    "        self.train_episodes   = 35000\n",
    "\n",
    "        # État pour la politique d'éval\n",
    "        self._prev_action: Optional[int] = None\n",
    "\n",
    "        self._train_q_learning()\n",
    "\n",
    "        # Reset propre\n",
    "        try:\n",
    "            self.env.reset()\n",
    "        except TypeError:\n",
    "            _ = self.env.reset()\n",
    "        self._prev_action = None\n",
    "\n",
    "        # cache inverse actions (0=LEFT,1=DOWN,2=RIGHT,3=UP)\n",
    "        self._inverse = {0: 2, 2: 0, 1: 3, 3: 1}\n",
    "\n",
    "    @staticmethod\n",
    "    def _lin(start, end, step, total):\n",
    "        if total <= 0:\n",
    "            return end\n",
    "        if step >= total:\n",
    "            return end\n",
    "        # linéaire rapide\n",
    "        return start + (end - start) * (step / float(total))\n",
    "\n",
    "    def _train_q_learning(self):\n",
    "        Q = self.Q  # alias local (un peu plus rapide)\n",
    "        gamma = self.gamma\n",
    "        env = self.env\n",
    "        max_steps = self.max_steps_per_ep\n",
    "\n",
    "        for ep in range(self.train_episodes):\n",
    "            # reset (gymnasium renvoie (obs, info))\n",
    "            out = env.reset()\n",
    "            state = int(out[0] if isinstance(out, tuple) else out)\n",
    "\n",
    "            eps   = self._lin(self.eps_start,   self.eps_end,   ep, self.eps_decay_episodes)\n",
    "            alpha = self._lin(self.alpha_start, self.alpha_end, ep, self.alpha_decay_episodes)\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                # ε-greedy\n",
    "                if np.random.random() < eps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = int(np.argmax(Q[state]))\n",
    "\n",
    "                step_out = env.step(action)\n",
    "                # gymnasium: (obs, reward, terminated, truncated, info)\n",
    "                next_state = int(step_out[0])\n",
    "                reward     = float(step_out[1])\n",
    "                terminated = bool(step_out[2])\n",
    "                truncated  = bool(step_out[3])\n",
    "\n",
    "                # Q-learning update\n",
    "                best_next = float(np.max(Q[next_state]))\n",
    "                td_target = reward + gamma * best_next * (0.0 if terminated else 1.0)\n",
    "                Q[state, action] += alpha * (td_target - float(Q[state, action]))\n",
    "\n",
    "                state = next_state\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        \"\"\"\n",
    "        Évaluation rapide et stable (pas d’update en ligne).\n",
    "        - Évite l'action inverse immédiate pour casser des cycles.\n",
    "        - Tie-breaker déterministe: RIGHT > DOWN > LEFT > UP parmi les meilleures.\n",
    "        \"\"\"\n",
    "        state = int(observation)\n",
    "        if state == 0 or terminated or truncated:\n",
    "            self._prev_action = None\n",
    "\n",
    "        row = self.Q[state]\n",
    "        m = np.max(row)\n",
    "        # candidates des meilleures actions\n",
    "        # ordre de préférence fixe pour éviter np.random.choice (plus rapide/déterministe)\n",
    "        pref_order = (2, 1, 0, 3)  # RIGHT, DOWN, LEFT, UP\n",
    "\n",
    "        # exclure l'inverse si possible\n",
    "        candidates = [a for a in range(4) if row[a] == m]\n",
    "        if self._prev_action is not None and len(candidates) > 1:\n",
    "            inv = self._inverse[self._prev_action]\n",
    "            if inv in candidates and len(candidates) > 1:\n",
    "                candidates = [a for a in candidates if a != inv] or candidates\n",
    "\n",
    "        # applique l'ordre de préférence\n",
    "        for a in pref_order:\n",
    "            if a in candidates:\n",
    "                action = int(a)\n",
    "                break\n",
    "\n",
    "        self._prev_action = action\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before submit\n",
    "Test that your agent has the right attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
    "agent = Agent(env)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "reward, terminated, truncated, info = None, False, False, None\n",
    "rewards = []\n",
    "while not (terminated or truncated):\n",
    "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "print(f'Cumulative Reward: {sum(rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward over 100 eval episodes: 0.440 ± 0.496\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
    "agent = Agent(env)\n",
    "\n",
    "scores = []\n",
    "for ep in range(100):\n",
    "    observation, _ = env.reset()\n",
    "    reward, terminated, truncated, info = None, False, False, None\n",
    "    rewards = []\n",
    "    while not (terminated or truncated):\n",
    "        action = agent.choose_action(observation, reward=reward,\n",
    "                                     terminated=terminated, truncated=truncated, info=info)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "    scores.append(sum(rewards))\n",
    "\n",
    "print(f\"Mean reward over 100 eval episodes: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Tarek Djaker notebook profile ---\nimport sys, os\nsys.path.append(r'C:\\Users\\pigio\\OneDrive\\Documents\\OneDrive\\Desktop\\projets\\data_science_practice_2025\\Tarek Djaker\\lib')\nfrom tarek_profile import nb_init, profile_banner\nnb_init()\nprofile_banner(title=None)\n# -------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {
        "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {
        "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300"
      },
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
      "metadata": {
        "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
        "outputId": "878dc8a8-f447-4e74-8af7-5197d7671321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {
        "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63"
      },
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {
        "id": "a60fa867-ddfe-403d-ba84-071792339e6f"
      },
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823a4916-1a3a-4f43-989e-4b9441cc142d",
      "metadata": {
        "id": "823a4916-1a3a-4f43-989e-4b9441cc142d"
      },
      "outputs": [],
      "source": [
        "#### Make a complete analysis on data preprocessing\n",
        "# Inconsistencies\n",
        "# Duplicates (data.duplicated().sum())\n",
        "# Missing values (data.isnull().sum())\n",
        "# Categorical\n",
        "# Outliers\n",
        "# Feature Engineering\n",
        "# Feature Selection and/or Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {
        "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([df_train, df_test], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {
        "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
        "outputId": "15af2a83-7085-48b5-e2fc-bb87381337e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c046b07a-845c-460b-a692-27a97ec3d613",
      "metadata": {
        "id": "c046b07a-845c-460b-a692-27a97ec3d613",
        "outputId": "4dcb0b95-0ce3-4309-aafb-aebc1e244a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {
        "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24"
      },
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "\n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3",
      "metadata": {
        "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3"
      },
      "outputs": [],
      "source": [
        "data['date'] = pd.to_datetime(data['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
      "metadata": {
        "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
        "outputId": "8c7dc1e5-1f1b-41c3-ddc3-41f1b0ff5491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np # Ajout de numpy pour la gestion des NaN\n",
        "from typing import Union, Dict, Any\n",
        "\n",
        "def ordinal_encode_column(\n",
        "    column: pd.Series,\n",
        "    mapping: Dict[Any, int] = None,\n",
        "    # Nouvelle option pour renvoyer le mapping (utile pour l'entraînement)\n",
        "    return_mapping: bool = False\n",
        ") -> Union[pd.Series, tuple[pd.Series, Dict[Any, int]]]:\n",
        "    \"\"\"\n",
        "    Associe un nombre entier unique à chaque valeur distincte d'une colonne (encodage ordinal),\n",
        "    en ignorant les valeurs NaN lors de la création du mapping.\n",
        "\n",
        "    Args:\n",
        "        column (pd.Series): La colonne (Série Pandas) à encoder.\n",
        "        mapping (Dict[Any, int], optional): Dictionnaire de mapping existant {valeur: nombre}.\n",
        "                                            Par défaut à None.\n",
        "        return_mapping (bool): Si True, retourne le mapping en plus de la colonne encodée.\n",
        "                               Doit être True pour l'ensemble d'entraînement.\n",
        "\n",
        "    Returns:\n",
        "        Union[pd.Series, tuple[pd.Series, Dict[Any, int]]]:\n",
        "            - Si 'return_mapping' est True, retourne un tuple (colonne_encodée, nouveau_mapping).\n",
        "            - Sinon, retourne seulement la colonne_encodée.\n",
        "    \"\"\"\n",
        "\n",
        "    # Si aucun mapping n'est fourni, nous en créons un nouveau\n",
        "    if mapping is None:\n",
        "        # Trouver toutes les valeurs uniques SAUF NaN (pour les laisser np.nan plus tard)\n",
        "        # Utilisation de dropna=True pour ignorer les NaN\n",
        "        unique_values = column.dropna().unique()\n",
        "\n",
        "        # Créer le mapping {valeur: index}\n",
        "        mapping = {value: index for index, value in enumerate(sorted(unique_values))}\n",
        "\n",
        "        print(\"Mapping créé (valeurs uniques et leurs entiers associés) :\")\n",
        "        print(mapping)\n",
        "\n",
        "        encoded_column = column.map(mapping)\n",
        "\n",
        "    # Si un mapping est fourni, nous l'appliquons\n",
        "    else:\n",
        "        encoded_column = column.map(mapping)\n",
        "\n",
        "        # Afficher un avertissement si de nouvelles valeurs sont apparues\n",
        "        if encoded_column.isnull().any():\n",
        "            # Trouver les nouvelles valeurs qui ne sont pas des NaN d'origine\n",
        "            unmapped_values = column[encoded_column.isnull() & column.notna()].unique()\n",
        "            if len(unmapped_values) > 0:\n",
        "                 print(f\"ATTENTION : {len(unmapped_values)} nouvelles valeurs non mappées trouvées et encodées en NaN. Mapping nécessaire : {unmapped_values[:5]}...\")\n",
        "\n",
        "    # Gestion de la sortie\n",
        "    if return_mapping:\n",
        "        return encoded_column, mapping\n",
        "    else:\n",
        "        return encoded_column"
      ],
      "metadata": {
        "id": "HyLBmvLaqZuA"
      },
      "id": "HyLBmvLaqZuA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['weather_condition'] = ordinal_encode_column(data['weather_condition'])\n",
        "data['oil_brent_price_indicator'] = ordinal_encode_column(data['oil_brent_price_indicator'])"
      ],
      "metadata": {
        "id": "QQDNcQ8Pqf4m",
        "outputId": "cc944d97-43cf-464a-c795-4104624c6465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QQDNcQ8Pqf4m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "CONVERSION_FACTOR_MS_TO_KMH = 3.6\n",
        "\n",
        "def convertir_vitesse(vitesse_col: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Convertit une colonne de vitesse contenant 'km/h' et 'm/s' en une Série numérique en 'km/h'.\n",
        "\n",
        "    Args:\n",
        "        vitesse_col (pd.Series): La colonne de vitesse brute (ex: '27.74 km/h' ou '6.83 m/s').\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: La colonne de vitesse nettoyée et convertie en km/h (type float).\n",
        "    \"\"\"\n",
        "\n",
        "    # Créer une copie pour éviter les avertissements SettingWithCopyWarning\n",
        "    col_cleaned = vitesse_col.copy()\n",
        "\n",
        "    # 1. Nettoyer les chaînes de caractères et effectuer la conversion\n",
        "    def process_value(val):\n",
        "        if pd.isna(val):\n",
        "            return val # Conserver les NaN\n",
        "\n",
        "        val_str = str(val).strip()\n",
        "\n",
        "        if 'km/h' in val_str:\n",
        "            # Remplacer 'km/h' et convertir directement en float\n",
        "            return float(val_str.replace('km/h', '').strip())\n",
        "\n",
        "        elif 'm/s' in val_str:\n",
        "            # Remplacer 'm/s', convertir en float, puis appliquer le facteur de conversion\n",
        "            val_ms = float(val_str.replace('m/s', '').strip())\n",
        "            return val_ms * CONVERSION_FACTOR_MS_TO_KMH\n",
        "\n",
        "        # Si aucune unité n'est trouvée, essayez de retourner la valeur comme float (pour les valeurs déjà numériques)\n",
        "        try:\n",
        "            return float(val_str)\n",
        "        except ValueError:\n",
        "            return pd.NA # Retourner NA pour les formats non reconnus\n",
        "\n",
        "    # Appliquer la fonction à toute la colonne\n",
        "    return col_cleaned.apply(process_value)\n"
      ],
      "metadata": {
        "id": "ybW8TmDJsGH3"
      },
      "id": "ybW8TmDJsGH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {
        "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e"
      },
      "outputs": [],
      "source": [
        "data['wind_speed'] = convertir_vitesse(data['wind_speed'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['wind_speed']"
      ],
      "metadata": {
        "id": "yMoz6hFzsI3q",
        "outputId": "a8f347f2-a01b-4b52-f974-b938aaf47101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "id": "yMoz6hFzsI3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Note: Ces bibliothèques doivent être installées:\n",
        "# pip install pandas numpy xgboost\n",
        "\n",
        "def mice_xgboost_imputer_manual(df: pd.DataFrame, max_iter: int = 10, random_state: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Effectue l'imputation MICE (Multiple Imputation by Chained Equations)\n",
        "    sur un DataFrame en utilisant un algorithme manuel sans fancyimpute,\n",
        "    avec XGBoostRegressor comme estimateur.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Le DataFrame d'entrée contenant des NaN.\n",
        "        max_iter (int): Nombre maximum d'itérations pour l'imputation MICE.\n",
        "        random_state (int): Graine aléatoire pour la reproductibilité.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Le DataFrame avec les valeurs NaN imputées.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Préparation et vérifications de robustesse\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(f\"L'entrée 'df' doit être un pandas DataFrame, mais est de type {type(df)}.\")\n",
        "\n",
        "    # Séparer les colonnes numériques des colonnes non-numériques\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "    df_numeric = df[numeric_cols].copy()\n",
        "\n",
        "    # Si aucune colonne numérique à imputer, retourner l'original\n",
        "    if df_numeric.empty:\n",
        "        print(\"\\nAvertissement: Aucune colonne numérique trouvée pour l'imputation. Retour du DataFrame original.\")\n",
        "        return df\n",
        "\n",
        "    # Isoler les colonnes non-numériques pour les réintégrer à la fin\n",
        "    non_numeric_cols = df.select_dtypes(exclude=np.number).columns\n",
        "    df_non_numeric = df[non_numeric_cols].copy()\n",
        "\n",
        "    # Identifier les colonnes qui ont des valeurs manquantes à imputer\n",
        "    cols_with_nan = df_numeric.columns[df_numeric.isnull().any()].tolist()\n",
        "    if not cols_with_nan:\n",
        "        print(\"\\nAvertissement: Aucune valeur manquante dans les colonnes numériques. Retour du DataFrame original.\")\n",
        "        return df\n",
        "\n",
        "    print(f\"\\n--- Démarrage de l'imputation MICE manuelle ---\")\n",
        "    print(f\"Colonnes à imputer: {cols_with_nan}\")\n",
        "\n",
        "    # 2. Imputation initiale (remplacer les NaN par la moyenne)\n",
        "    # C'est l'étape de 'warm-up'\n",
        "    df_imputed = df_numeric.fillna(df_numeric.mean())\n",
        "\n",
        "    # Initialiser le modèle XGBoost\n",
        "    xgb_estimator = XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1,\n",
        "        tree_method='hist',\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    # 3. Boucle MICE\n",
        "    np.random.seed(random_state) # Fixer la graine pour la reproductibilité dans les splits\n",
        "    for iter_count in range(max_iter):\n",
        "\n",
        "        # Mélanger l'ordre d'imputation des colonnes pour éviter les biais\n",
        "        np.random.shuffle(cols_with_nan)\n",
        "\n",
        "        for col_to_impute in cols_with_nan:\n",
        "            # Identifier les lignes manquantes et non manquantes\n",
        "            missing_mask = df_numeric[col_to_impute].isnull()\n",
        "\n",
        "            # Si aucune valeur manquante dans cette colonne, continuer\n",
        "            if not missing_mask.any():\n",
        "                continue\n",
        "\n",
        "            # Ensemble d'entraînement: lignes non manquantes dans la colonne cible\n",
        "            # La colonne cible (col_to_impute) est la variable Y\n",
        "            # Toutes les autres colonnes numériques (déjà imputées à ce stade de l'itération) sont X\n",
        "\n",
        "            X_train = df_imputed[~missing_mask].drop(columns=[col_to_impute])\n",
        "            y_train = df_numeric.loc[~missing_mask, col_to_impute] # Utiliser les valeurs originales pour l'entraînement\n",
        "\n",
        "            # Ensemble de prédiction: lignes manquantes dans la colonne cible\n",
        "            X_predict = df_imputed[missing_mask].drop(columns=[col_to_impute])\n",
        "\n",
        "            # Entraîner le modèle\n",
        "            xgb_estimator.fit(X_train, y_train)\n",
        "\n",
        "            # Prédire les valeurs manquantes\n",
        "            y_predicted = xgb_estimator.predict(X_predict)\n",
        "\n",
        "            # Mettre à jour le DataFrame imputé\n",
        "            df_imputed.loc[missing_mask, col_to_impute] = y_predicted\n",
        "\n",
        "        print(f\"Itération {iter_count + 1}/{max_iter} complétée.\")\n",
        "\n",
        "    # 4. Réassembler le DataFrame final\n",
        "    if not df_non_numeric.empty:\n",
        "        # Assurer que les index sont alignés\n",
        "        df_imputed = pd.merge(\n",
        "            df_imputed,\n",
        "            df_non_numeric,\n",
        "            left_index=True,\n",
        "            right_index=True,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "    # Rétablir l'ordre des colonnes original\n",
        "    original_order = [col for col in df.columns if col in df_imputed.columns]\n",
        "    df_imputed = df_imputed[original_order]\n",
        "\n",
        "    print(\"Imputation MICE (XGBoost) terminée. Vérification des NaN dans les colonnes numériques: \", df_imputed[numeric_cols].isnull().any().any())\n",
        "    print(\"-----------------------------------\")\n",
        "\n",
        "    return df_imputed"
      ],
      "metadata": {
        "id": "M2trxnbwsa2m"
      },
      "id": "M2trxnbwsa2m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = mice_xgboost_imputer_manual(data, max_iter=10, random_state=42)"
      ],
      "metadata": {
        "id": "e2rfNvL9sk5i",
        "outputId": "c695fc66-80cb-45f4-c000-de9be93eca18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e2rfNvL9sk5i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iqr_outlier_remover(column: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Détecte les outliers dans une colonne numérique en utilisant la méthode IQR\n",
        "    et les remplace par la médiane de la colonne.\n",
        "\n",
        "    Args:\n",
        "        column (pd.Series): La colonne (série Pandas) numérique à traiter.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: La colonne traitée avec les outliers remplacés par la médiane.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Vérification de robustesse\n",
        "    if not isinstance(column, pd.Series):\n",
        "        raise TypeError(\"L'entrée doit être une série Pandas.\")\n",
        "\n",
        "    if not pd.api.types.is_numeric_dtype(column):\n",
        "        print(f\"Avertissement: La colonne '{column.name}' n'est pas numérique. Retour de la colonne originale.\")\n",
        "        return column\n",
        "\n",
        "    # 2. Calcul des quartiles et de l'IQR\n",
        "    Q1 = column.quantile(0.25)\n",
        "    Q3 = column.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # 3. Définition des bornes (seuil conventionnel de 1.5 * IQR)\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # 4. Calcul de la médiane de la colonne\n",
        "    median_value = column.median()\n",
        "\n",
        "    # 5. Détection des outliers (valeurs en dehors des bornes)\n",
        "    outlier_mask_low = column < lower_bound\n",
        "    outlier_mask_high = column > upper_bound\n",
        "\n",
        "    # Masque combiné des outliers\n",
        "    outlier_mask = outlier_mask_low | outlier_mask_high\n",
        "\n",
        "    # Nombre d'outliers détectés\n",
        "    num_outliers = outlier_mask.sum()\n",
        "\n",
        "    # Créer une copie de la colonne pour éviter de modifier le DataFrame original\n",
        "    column_processed = column.copy()\n",
        "\n",
        "    # 6. Remplacement des outliers par la médiane\n",
        "    if num_outliers > 0:\n",
        "        column_processed[outlier_mask] = median_value\n",
        "        print(f\"Traitement de la colonne '{column.name}': {num_outliers} outliers remplacés par la médiane ({median_value:.2f}).\")\n",
        "    else:\n",
        "        print(f\"Traitement de la colonne '{column.name}': Aucun outlier détecté par la méthode IQR.\")\n",
        "\n",
        "    return column_processed"
      ],
      "metadata": {
        "id": "n3kRtDzEvBHY"
      },
      "id": "n3kRtDzEvBHY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['electricity_demand'] = iqr_outlier_remover(data['electricity_demand'])"
      ],
      "metadata": {
        "id": "6Jv67TWSvGN1",
        "outputId": "efc3f207-80ca-42cd-e4d5-f10861e607f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6Jv67TWSvGN1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {
        "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
        "outputId": "68ce5d49-64a2-4096-ed21-fb330fc8724a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'electricity_demand', '2017-01-01', '2019-09-07')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['humidity'] = iqr_outlier_remover(data['humidity'])"
      ],
      "metadata": {
        "id": "nDQSA8FOvMP_",
        "outputId": "9535d79a-c024-4933-f6f6-8ddde0ea3314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nDQSA8FOvMP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
      "metadata": {
        "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
        "outputId": "0113c002-93d4-4eeb-a100-31bd0cb826b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'humidity', '2016-06-01', '2016-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {
        "id": "d00dd628-b436-4f3b-829d-38b18589a12b"
      },
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86971ab4-1ef8-464b-afb5-0d750a8c4035",
      "metadata": {
        "id": "86971ab4-1ef8-464b-afb5-0d750a8c4035"
      },
      "outputs": [],
      "source": [
        "# Provide a complete data preprocessing transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e",
      "metadata": {
        "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e"
      },
      "outputs": [],
      "source": [
        "# 1. Handle Inconsistencies\n",
        "def handle_inconsistencies(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy(), y_train\n",
        "\n",
        "# 2. Handling Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        X_train_no_duplicates = X_train.copy()\n",
        "        y_train_no_duplicates = y_train.loc[X_train_no_duplicates.index]\n",
        "        return X_train_no_duplicates, y_train_no_duplicates, X_val.copy()\n",
        "\n",
        "# 3. Handling Missing Values\n",
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        X_train = X_train.fillna(-1)\n",
        "        X_val = X_val.fillna(-1)\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        X_train = X_train.fillna(-1)\n",
        "        return X_train\n",
        "\n",
        "# 4. Handling Categorical Values\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy()\n",
        "\n",
        "# 5. Handling Outliers\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy(), y_train\n",
        "\n",
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        return X_train.copy(), y_train\n",
        "\n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    selected_columns = ['humidity', 'temperature_station1',\n",
        "       'temperature_station2', 'temperature_station3', 'temperature_station4',\n",
        "       'temperature_station5', 'temperature_station6', 'temperature_station7',\n",
        "       'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns], X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {
        "id": "6b18081c-17f2-4809-bdf6-7181aac77199"
      },
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### call transformations here, if there is no learning and no need to be crossval\n",
        "    X, y = handle_inconsistencies(X, y)\n",
        "    # X, y = handle_duplicates(X, y)\n",
        "    X  = handle_missing_values(X, y)\n",
        "    # X_train = handle_categorical(X, y)\n",
        "    X, y = handle_outliers(X, y)\n",
        "    # X, y = feature_engineering(XX, y)\n",
        "    X = feature_selection(X, y)\n",
        "\n",
        "    model = LinearRegression()\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "\n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        # X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_duplicates(X_train, y_train, X_val)\n",
        "        # X_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        X_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        # X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        # X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "\n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "\n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "\n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "\n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "\n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "\n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {
        "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
        "outputId": "7b8db704-7850-4246-f6f6-ce5e477a8817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {
        "id": "30e27e3b-7641-4107-be8c-50104d473cd9"
      },
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28",
      "metadata": {
        "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28"
      },
      "outputs": [],
      "source": [
        "# Train and submit your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {
        "id": "81488d3e-2dde-4904-ac69-430e55df0cc5"
      },
      "outputs": [],
      "source": [
        "# Prepare X_train and y_train from your data\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {
        "id": "7f558b85-7970-4c24-95a8-fd6a37da930b"
      },
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "\n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {
        "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
        "outputId": "205b49ae-6418-47a1-d536-1445c54c0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Call serve_model to train and predict\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {
        "id": "538cf936-7872-46ad-b02f-422a0aec3806",
        "outputId": "b5a8af69-1ce3-438d-ac41-734eaa2b9f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

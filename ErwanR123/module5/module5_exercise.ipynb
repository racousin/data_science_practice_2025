{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {},
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823a4916-1a3a-4f43-989e-4b9441cc142d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Make a complete analysis on data preprocessing\n",
        "# Inconsistencies\n",
        "# Duplicates (data.duplicated().sum())\n",
        "# Missing values (data.isnull().sum())\n",
        "# Categorical\n",
        "# Outliers\n",
        "# Feature Engineering\n",
        "# Feature Selection and/or Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.concat([df_train, df_test], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c046b07a-845c-460b-a692-27a97ec3d613",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "    \n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "    \n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['date'] = pd.to_datetime(data['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['wind_speed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'electricity_demand', '2017-01-01', '2019-09-07')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'humidity', '2016-06-01', '2016-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {},
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ad3c52",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62737867",
      "metadata": {},
      "outputs": [],
      "source": [
        "data2=data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5514ecbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data[\"weather_condition\"].unique().tolist()) \n",
        "#display(data[data[\"weather_condition\"].isna()])\n",
        "print(data[\"oil_brent_price_indicator\"].unique().tolist()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a649f6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# Conversion robuste d'une valeur de vitesse de vent vers km/h (float)\n",
        "def wind_speed_convert(value):\n",
        "    # Déjà manquant\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "\n",
        "    # Déjà numérique\n",
        "    if isinstance(value, (int, float, np.integer, np.floating)):\n",
        "        return float(value)\n",
        "\n",
        "    s = str(value).strip().lower()\n",
        "\n",
        "    # Regex: nombre + unité optionnelle (km/h, kmh, m/s, ms)\n",
        "    m = re.match(r'^([+-]?\\d+(?:\\.\\d+)?)\\s*(km/?h|kmh|m/?s|ms)?$', s)\n",
        "    if not m:\n",
        "        return np.nan\n",
        "\n",
        "    num = float(m.group(1))\n",
        "    unit = m.group(2)\n",
        "\n",
        "    if unit is None or unit in (\"km/h\", \"kmh\", \"kmh\", \"km/h\"):\n",
        "        return num\n",
        "    if unit in (\"m/s\", \"ms\", \"m/s\"):\n",
        "        return num * 3.6\n",
        "\n",
        "    # Par sécurité\n",
        "    return np.nan\n",
        "\n",
        "# Harmonisé: toujours (X, y) ou (X_tr, y_tr, X_val, y_val)\n",
        "def handle_inconsistencies(X, y, X_val=None, y_val=None):\n",
        "    X = X.copy()\n",
        "    if \"wind_speed\" in X.columns:\n",
        "        X[\"wind_speed\"] = X[\"wind_speed\"].apply(wind_speed_convert)\n",
        "        X[\"wind_speed\"] = pd.to_numeric(X[\"wind_speed\"], errors=\"coerce\")\n",
        "\n",
        "    if X_val is None:\n",
        "        return X, y\n",
        "\n",
        "    X_val = X_val.copy()\n",
        "    if \"wind_speed\" in X_val.columns:\n",
        "        X_val[\"wind_speed\"] = X_val[\"wind_speed\"].apply(wind_speed_convert)\n",
        "        X_val[\"wind_speed\"] = pd.to_numeric(X_val[\"wind_speed\"], errors=\"coerce\")\n",
        "\n",
        "    return X, y, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c949eb33",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_duplicates(X, y, X_val=None, y_val=None, subset=None, keep=\"first\"):\n",
        "\n",
        "    # Train\n",
        "    X2 = X.drop_duplicates(subset=subset, keep=keep)\n",
        "    y2 = y.loc[X2.index]\n",
        "\n",
        "    if X_val is None:\n",
        "        return X2, y2\n",
        "\n",
        "    # Val\n",
        "    Xv2 = X_val.drop_duplicates(subset=subset, keep=keep)\n",
        "    yv2 = None if y_val is None else y_val.loc[Xv2.index]\n",
        "\n",
        "    return X2, y2, Xv2, yv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2047275d",
      "metadata": {},
      "outputs": [],
      "source": [
        "''' \n",
        "#Traitement des valeurs manquantes \n",
        "\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_no_exact_dupes.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Heatmap of Missing Values')\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c41b84",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _coerce_numeric(df: pd.DataFrame, cols):\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "def data_cleaner(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # S'assurer que 'humidity' et 'wind_speed' sont numériques\n",
        "    df_clean = _coerce_numeric(df_clean, [\"humidity\", \"wind_speed\"])\n",
        "\n",
        "    # Colonnes de température → s'assurer que c'est numérique puis interpolation\n",
        "    cols_temp = [c for c in df_clean.columns if c.startswith(\"temperature_station\")]\n",
        "    if cols_temp:\n",
        "        df_clean = _coerce_numeric(df_clean, cols_temp)\n",
        "        # Interpolation linéaire, dans les deux sens si trous en début/fin\n",
        "        df_clean[cols_temp] = df_clean[cols_temp].interpolate(\n",
        "            method=\"linear\", limit_direction=\"both\"\n",
        "        )\n",
        "\n",
        "    # NE PAS supprimer les lignes avec weather_condition manquant ici.\n",
        "    # On traitera ça dans handle_categorical (fillna(\"Unknown\")).\n",
        "\n",
        "    return df_clean\n",
        "def handle_missing_values(X, y, X_val=None, y_val=None):\n",
        "    # Nettoyage de base (types + interpolation températures)\n",
        "    X = data_cleaner(X)\n",
        "\n",
        "    # Médianes calculées sur le TRAIN uniquement\n",
        "    medians = {}\n",
        "    for col in (\"humidity\", \"wind_speed\"):\n",
        "        if col in X.columns:\n",
        "            medians[col] = X[col].median()\n",
        "\n",
        "    # Imputation TRAIN\n",
        "    for col, m in medians.items():\n",
        "        X[col] = X[col].fillna(m)\n",
        "\n",
        "    if X_val is None:\n",
        "        return X, y\n",
        "\n",
        "    # Apply sur VAL/TEST en utilisant les médianes du TRAIN (pas de fuite)\n",
        "    X_val = data_cleaner(X_val)\n",
        "    for col, m in medians.items():\n",
        "        if col in X_val.columns:\n",
        "            X_val[col] = X_val[col].fillna(m)\n",
        "\n",
        "    return X, y, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db556fc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import inspect\n",
        "\n",
        "def Encodage_var_cat(df):\n",
        "    df = df.copy()\n",
        "    df[\"weather_condition\"] = df[\"weather_condition\"].fillna(\"Unknown\")\n",
        "\n",
        "    # Vérifie si le paramètre 'sparse_output' existe dans la version installée\n",
        "    if \"sparse_output\" in inspect.signature(OneHotEncoder).parameters:\n",
        "        ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "    else:\n",
        "        ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
        "\n",
        "    X_w = ohe.fit_transform(df[[\"weather_condition\"]])\n",
        "    # Récupère les noms des features encodées\n",
        "    cols = ohe.get_feature_names_out([\"weather_condition\"])\n",
        "    df_encoded = pd.DataFrame(X_w, columns=cols, index=df.index)\n",
        "\n",
        "    # Remplace la colonne d'origine par les colonnes encodées\n",
        "    df = df.drop(columns=[\"weather_condition\"])\n",
        "    df = pd.concat([df, df_encoded], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def handle_categorical(X, y, X_val=None, y_val=None):\n",
        "    \"\"\"\n",
        "    - Fit OHE 'weather_condition' sur le TRAIN, transform sur VAL/TEST\n",
        "    - Map 'oil_brent_price_indicator' en ordinal et supprime la colonne d'origine\n",
        "    - Retourne (X, y) ou (X_tr, y_tr, X_val, y_val) avec colonnes alignées\n",
        "    \"\"\"\n",
        "    def _fit_ohe_on_train_and_transform(X_tr, X_to):\n",
        "        X_tr = X_tr.copy()\n",
        "        X_to = X_to.copy()\n",
        "\n",
        "        # weather_condition -> OHE\n",
        "        if \"weather_condition\" in X_tr.columns:\n",
        "            X_tr[\"weather_condition\"] = X_tr[\"weather_condition\"].fillna(\"Unknown\")\n",
        "            X_to[\"weather_condition\"] = X_to[\"weather_condition\"].fillna(\"Unknown\") if \"weather_condition\" in X_to.columns else \"Unknown\"\n",
        "\n",
        "            ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
        "            ohe.fit(X_tr[[\"weather_condition\"]])\n",
        "\n",
        "            Xw_tr = ohe.transform(X_tr[[\"weather_condition\"]])\n",
        "            Xw_to = ohe.transform(X_to[[\"weather_condition\"]])\n",
        "\n",
        "            try:\n",
        "                w_cols = ohe.get_feature_names_out([\"weather_condition\"])\n",
        "            except AttributeError:\n",
        "                w_cols = ohe.get_feature_names([\"weather_condition\"])\n",
        "\n",
        "            W_tr = pd.DataFrame(Xw_tr, columns=w_cols, index=X_tr.index)\n",
        "            W_to = pd.DataFrame(Xw_to, columns=w_cols, index=X_to.index)\n",
        "\n",
        "            X_tr = pd.concat([X_tr.drop(columns=[\"weather_condition\"]), W_tr], axis=1)\n",
        "            X_to = pd.concat([X_to.drop(columns=[\"weather_condition\"], errors=\"ignore\"), W_to], axis=1)\n",
        "\n",
        "        # oil_brent_price_indicator -> ordinal + drop\n",
        "        order = [\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\"]\n",
        "        for Z in (X_tr, X_to):\n",
        "            if \"oil_brent_price_indicator\" in Z.columns:\n",
        "                cat = pd.Categorical(Z[\"oil_brent_price_indicator\"], categories=order, ordered=True)\n",
        "                Z[\"oil_indicator_ord\"] = pd.Series(cat.codes, index=Z.index).astype(\"Int64\")\n",
        "                Z.drop(columns=[\"oil_brent_price_indicator\"], inplace=True)\n",
        "\n",
        "        # Alignement final (sécurité)\n",
        "        X_to = X_to.reindex(columns=X_tr.columns, fill_value=0)\n",
        "\n",
        "        return X_tr, X_to\n",
        "\n",
        "    # --- Cas sans validation ---\n",
        "    if X_val is None:\n",
        "        X_enc, _ = _fit_ohe_on_train_and_transform(X, X)  # fit + self-transform\n",
        "        return X_enc, y\n",
        "\n",
        "    # --- Cas avec validation ---\n",
        "    X_tr_enc, X_val_enc = _fit_ohe_on_train_and_transform(X, X_val)\n",
        "    return X_tr_enc, y, X_val_enc, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b809e0ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "#Outliers\n",
        "for column in df_ohe.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x=df_ohe[column])\n",
        "    plt.title('Box Plot for Outlier Detection')\n",
        "    plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b83af548",
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(list(df_ohe.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b635b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1) Feature engineering \n",
        "def basic_feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    # Date → composantes temps + encodage cyclique\n",
        "    if \"date\" in df.columns:\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "        df[\"hour\"] = df[\"date\"].dt.hour\n",
        "        df[\"dayofweek\"] = df[\"date\"].dt.dayofweek  # 0=lundi, 6=dimanche\n",
        "        df[\"month\"] = df[\"date\"].dt.month\n",
        "        df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
        "\n",
        "        # Encodage cyclique \n",
        "        # heure sur 24h\n",
        "        df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
        "        df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
        "        # jour de semaine sur 7\n",
        "        df[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dayofweek\"] / 7.0)\n",
        "        df[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dayofweek\"] / 7.0)\n",
        "        # mois sur 12\n",
        "        df[\"month_sin\"] = np.sin(2 * np.pi * (df[\"month\"] - 1) / 12.0)\n",
        "        df[\"month_cos\"] = np.cos(2 * np.pi * (df[\"month\"] - 1) / 12.0)\n",
        "\n",
        "    #  Températures des stations → agrégats simples\n",
        "    temp_cols = [c for c in df.columns if c.startswith(\"temperature_station\")]\n",
        "    if temp_cols:\n",
        "        df[\"temp_mean\"] = df[temp_cols].mean(axis=1)\n",
        "        df[\"temp_min\"]  = df[temp_cols].min(axis=1)\n",
        "        df[\"temp_max\"]  = df[temp_cols].max(axis=1)\n",
        "        df[\"temp_std\"]  = df[temp_cols].std(axis=1)\n",
        "        df[\"temp_range\"] = df[\"temp_max\"] - df[\"temp_min\"]\n",
        "\n",
        "    #  Vent / humidité :\n",
        "    for col in [\"wind_speed\", \"humidity\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # Indicateur pétrole (ordinal)\n",
        "    if \"oil_brent_price_indicator\" in df.columns:\n",
        "        order = [\"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\"]\n",
        "        cat = pd.Categorical(df[\"oil_brent_price_indicator\"], categories=order, ordered=True)\n",
        "        df[\"oil_indicator_ord\"] = pd.Series(cat.codes, index=df.index).astype(\"Int64\")\n",
        "\n",
        "    #  Météo (dummies déjà présentes) → petit regroupement simple\n",
        "    #     Indicateur précipitations (Rainy ou Snowy)\n",
        "    rain_col = \"weather_condition_Rainy\"\n",
        "    snow_col = \"weather_condition_Snowy\"\n",
        "    if rain_col in df.columns or snow_col in df.columns:\n",
        "        df[\"is_precip\"] = (df.get(rain_col, 0) + df.get(snow_col, 0) > 0).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# 2) PCA pour réduction + ranking des features d’origine\n",
        "def pca_feature_selection(\n",
        "    df: pd.DataFrame,\n",
        "    target_col: str = \"electricity_demand\",\n",
        "    variance_threshold: float = 0.95,\n",
        "    top_k: int = 15\n",
        "):\n",
        "\n",
        "    # Colonnes à exclure des features\n",
        "    exclude_cols = {\"date\", target_col, \"oil_brent_price_indicator\"}  # on garde 'oil_indicator_ord' si présent\n",
        "    # On ne garde que numériques\n",
        "    candidate_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "    X_num = df[candidate_cols].select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "    # Gestion basique des manquants\n",
        "    X_num = X_num.fillna(X_num.median(numeric_only=True))\n",
        "\n",
        "    # Standardisation\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "    # PCA non bornée, puis choix des composantes via seuil de variance cumulée\n",
        "    pca = PCA(n_components=None, svd_solver=\"auto\", random_state=42)\n",
        "    X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
        "    k = int(np.searchsorted(cumvar, variance_threshold) + 1)\n",
        "\n",
        "    # Transformation finale avec k composantes\n",
        "    pca_k = PCA(n_components=k, svd_solver=\"auto\", random_state=42)\n",
        "    X_pca = pca_k.fit_transform(X_scaled)\n",
        "\n",
        "    # Ranking des features d'origine par contribution:\n",
        "    # score(feature j) = somme_{i=1..k} |loading_{i,j}| * explained_var_ratio_{i}\n",
        "    loadings = pca_k.components_  # shape (k, n_features)\n",
        "    evr = pca_k.explained_variance_ratio_  # shape (k,)\n",
        "    contrib = np.abs(loadings) * evr[:, None]  # pondération par variance expliquée\n",
        "    scores = contrib.sum(axis=0)  # importance agrégée par feature\n",
        "\n",
        "    feature_scores = pd.Series(scores, index=X_num.columns).sort_values(ascending=False)\n",
        "    selected_features = feature_scores.head(top_k).index.tolist()\n",
        "\n",
        "    info = {\n",
        "        \"n_features_in\": X_num.shape[1],\n",
        "        \"n_components\": k,\n",
        "        \"explained_variance_ratio_cum\": float(cumvar[k-1]),\n",
        "        \"top_features\": feature_scores.head(top_k)\n",
        "    }\n",
        "\n",
        "    return X_pca, selected_features, feature_scores, info\n",
        "\n",
        "\n",
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train: pd.DataFrame, y_train, X_val: pd.DataFrame = None):\n",
        "    X_train_fe = basic_feature_engineering(X_train)\n",
        "    if X_val is not None:\n",
        "        X_val_fe = basic_feature_engineering(X_val)\n",
        "        return X_train_fe, y_train, X_val_fe\n",
        "    else:\n",
        "        return X_train_fe, y_train\n",
        "\n",
        "\n",
        "# 7. Feature Selection and Dimensionality Reduction (via PCA-ranking)\n",
        "def feature_selection(X_train: pd.DataFrame, y_train, X_val: pd.DataFrame = None,\n",
        "                      variance_threshold: float = 0.95, top_k: int = 15):\n",
        "    # On réutilise pca_feature_selection qui attend la cible dans le DataFrame :\n",
        "    df_train_tmp = X_train.copy()\n",
        "    df_train_tmp[\"electricity_demand\"] = y_train\n",
        "\n",
        "    # Appel à la fonction déjà créée\n",
        "    _, selected_features, feature_scores, info = pca_feature_selection(\n",
        "        df_train_tmp,\n",
        "        target_col=\"electricity_demand\",\n",
        "        variance_threshold=variance_threshold,\n",
        "        top_k=top_k\n",
        "    )\n",
        "    selected_features = [c for c in selected_features if c in X_train.columns]\n",
        "\n",
        "    X_train_selected = X_train[selected_features].copy()\n",
        "\n",
        "    if X_val is not None:\n",
        "        # Sécurité: aligner la sélection sur X_val\n",
        "        common = [c for c in selected_features if c in X_val.columns]\n",
        "        X_val_selected = X_val[common].copy()\n",
        "        return X_train_selected, X_val_selected\n",
        "    else:\n",
        "        return X_train_selected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b12f558",
      "metadata": {},
      "source": [
        "jeijdeije"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce58b45f",
      "metadata": {},
      "source": [
        "## Code propre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5de066",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# 1. Inconsistencies\n",
        "def handle_inconsistencies(X_train, y_train, X_val=None):\n",
        "    def _convert(df):\n",
        "        df = df.copy()\n",
        "        if \"wind_speed\" in df.columns:\n",
        "            df[\"wind_speed\"] = df[\"wind_speed\"].apply(wind_speed_convert)\n",
        "        return df\n",
        "\n",
        "    if X_val is not None:\n",
        "        return _convert(X_train), y_train, _convert(X_val)  # 3 sorties\n",
        "    else:\n",
        "        return _convert(X_train), y_train  # 2 sorties\n",
        "\n",
        "\n",
        "# 2. Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.drop_duplicates()\n",
        "    y_train = y_train.loc[X_train.index]\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.drop_duplicates()\n",
        "        return X_train.copy(), y_train.copy(), X_val.copy()  # 3 sorties\n",
        "    else:\n",
        "        return X_train.copy(), y_train.copy()  # 2 sorties\n",
        "\n",
        "\n",
        "CATEGORICAL_COLS = [\"weather_condition\", \"oil_brent_price_indicator\"]\n",
        "\n",
        "# 3. Missing values\n",
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    import numpy as np\n",
        "    X_train = X_train.copy()\n",
        "    def _fill(df):\n",
        "        df = df.copy()\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        cat_cols = [c for c in CATEGORICAL_COLS if c in df.columns]\n",
        "\n",
        "        # Numériques -> -1\n",
        "        if num_cols:\n",
        "            df[num_cols] = df[num_cols].fillna(-1)\n",
        "\n",
        "        # Catégorielles -> \"MISSING\" (et on force string)\n",
        "        for c in cat_cols:\n",
        "            df[c] = df[c].astype(str)\n",
        "            df[c] = df[c].where(df[c].notna(), \"MISSING\")\n",
        "            df[c] = df[c].replace({\"-1\": \"MISSING\"})  # au cas où -1 a fuité\n",
        "        return df\n",
        "\n",
        "    if X_val is not None:\n",
        "        return _fill(X_train), _fill(X_val)  # 2 sorties\n",
        "    else:\n",
        "        return _fill(X_train)  # 1 sortie\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "# 4. Categorical encoding\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.copy()\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.copy()\n",
        "\n",
        "    # weather_condition : ordre choisi + MISSING\n",
        "    if \"weather_condition\" in X_train.columns:\n",
        "        enc_weather = OrdinalEncoder(\n",
        "            categories=[[\"MISSING\", \"Snowy\", \"Rainy\", \"Cloudy\", \"Sunny\"]],\n",
        "            handle_unknown=\"use_encoded_value\",\n",
        "            unknown_value=-1,\n",
        "        )\n",
        "        X_train[[\"weather_condition\"]] = enc_weather.fit_transform(X_train[[\"weather_condition\"]])\n",
        "        if X_val is not None and \"weather_condition\" in X_val.columns:\n",
        "            X_val[[\"weather_condition\"]] = enc_weather.transform(X_val[[\"weather_condition\"]])\n",
        "\n",
        "    # oil_brent_price_indicator : ordre choisi + MISSING\n",
        "    if \"oil_brent_price_indicator\" in X_train.columns:\n",
        "        enc_oil = OrdinalEncoder(\n",
        "            categories=[[\"MISSING\", \"Very Low\", \"Low\", \"Moderate\", \"High\", \"Very High\"]],\n",
        "            handle_unknown=\"use_encoded_value\",\n",
        "            unknown_value=-1,\n",
        "        )\n",
        "        X_train[[\"oil_brent_price_indicator\"]] = enc_oil.fit_transform(X_train[[\"oil_brent_price_indicator\"]])\n",
        "        if X_val is not None and \"oil_brent_price_indicator\" in X_val.columns:\n",
        "            X_val[[\"oil_brent_price_indicator\"]] = enc_oil.transform(X_val[[\"oil_brent_price_indicator\"]])\n",
        "\n",
        "    return (X_train.copy(), X_val.copy()) if X_val is not None else X_train.copy()\n",
        "\n",
        "\n",
        "# 5. Outliers\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    drop_idx = [1022, 1029, 1509]\n",
        "    X_train = X_train.drop(index=drop_idx, errors=\"ignore\")\n",
        "    y_train = y_train.loc[X_train.index]\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.drop(index=drop_idx, errors=\"ignore\")\n",
        "        return X_train.copy(), y_train.copy(), X_val.copy()  # 3 sorties\n",
        "    else:\n",
        "        return X_train.copy(), y_train.copy()  # 2 sorties\n",
        "\n",
        "\n",
        "# 6. Feature engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    X_train_fe = basic_feature_engineering(X_train)\n",
        "    if X_val is not None:\n",
        "        X_val_fe = basic_feature_engineering(X_val)\n",
        "        return X_train_fe, y_train, X_val_fe  # 3 sorties\n",
        "    else:\n",
        "        return X_train_fe, y_train  # 2 sorties\n",
        "\n",
        "\n",
        "# 7. Feature selection (PCA-ranking)\n",
        "def feature_selection(X_train, y_train, X_val=None,\n",
        "                      variance_threshold=0.95, top_k=15):\n",
        "    df_train_tmp = X_train.copy()\n",
        "    df_train_tmp[\"electricity_demand\"] = y_train\n",
        "\n",
        "    _, selected_features, _, _ = pca_feature_selection(\n",
        "        df_train_tmp,\n",
        "        target_col=\"electricity_demand\",\n",
        "        variance_threshold=variance_threshold,\n",
        "        top_k=top_k\n",
        "    )\n",
        "    selected_features = [c for c in selected_features if c in X_train.columns]\n",
        "\n",
        "    X_train_sel = X_train[selected_features].copy()\n",
        "\n",
        "    if X_val is not None:\n",
        "        common = [c for c in selected_features if c in X_val.columns]\n",
        "        X_val_sel = X_val[common].copy()\n",
        "        return X_train_sel, X_val_sel  # 2 sorties\n",
        "    else:\n",
        "        return X_train_sel  # 1 sortie\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### call transformations here, if there is no learning and no need to be crossval\n",
        "    X, y = handle_inconsistencies(X, y)\n",
        "    X, y = handle_duplicates(X, y)\n",
        "    X = handle_missing_values(X, y)\n",
        "    X = handle_categorical(X, y)\n",
        "    X, y = handle_outliers(X, y)\n",
        "    X, y = feature_engineering(X, y)\n",
        "    X = feature_selection(X, y)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    \n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "    \n",
        "    X_trains = []\n",
        "    X_vals = []\n",
        "    y_trains = []\n",
        "    y_vals = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "\n",
        "        X_trains.append(X_train)\n",
        "        X_vals.append(X_val)\n",
        "        y_trains.append(y_train)\n",
        "        y_vals.append(y_val)\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_duplicates(X_train, y_train, X_val)\n",
        "        X_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        X_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        X_train, y_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    \n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse, X_trains, X_vals, y_trains, y_vals  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868fa967",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "mse, X_trains, X_vals, y_trains, y_vals = evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {},
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and submit your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X_train and y_train from your data\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call serve_model to train and predict\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c767f3d",
      "metadata": {},
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/module13/exercise/module13_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89357813",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==0.29.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bae0627",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb9c00f",
      "metadata": {},
      "source": [
        "# module13_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464e9858",
      "metadata": {},
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241553e6",
      "metadata": {},
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b035cd86",
      "metadata": {},
      "source": [
        "### Description\n",
        "\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
        "\n",
        "Holes in the ice are distributed in set locations.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
        "\n",
        "The environment is based on :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0590256",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d54e30e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "def execute_training_loop(env, learner, n_episodes, training_mode=True, max_len=200, random_seed=123):\n",
        "\n",
        "    episode_returns = np.zeros(n_episodes)\n",
        "    rng = np.random.default_rng(random_seed)\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset(seed=random_seed + ep)\n",
        "        done, truncated = False, False\n",
        "        cumulative = 0.0\n",
        "        step = 0\n",
        "\n",
        "        while not (done or truncated) and step < max_len:\n",
        "            # sélection d'action via la stratégie du learner\n",
        "            act = learner.select_action(obs)\n",
        "            next_obs, rew, done, truncated, _ = env.step(act)\n",
        "\n",
        "            if training_mode:\n",
        "                learner.update(obs, act, rew, next_obs, done or truncated)\n",
        "\n",
        "            cumulative += rew\n",
        "            obs = next_obs\n",
        "            step += 1\n",
        "\n",
        "        episode_returns[ep] = cumulative\n",
        "    return episode_returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a8a59e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent epsilon-greedy decay\n",
        "\n",
        "def get_epsilon_greedy_decay_action(Q_s, episode_idx,\n",
        "                                    epsilon_start=1.0, epsilon_end=0.05, decay_rate=0.995):\n",
        "    eps_t = max(epsilon_end, epsilon_start * (decay_rate ** episode_idx))\n",
        "    if np.random.rand() < eps_t:\n",
        "        return np.random.randint(len(Q_s))\n",
        "    return int(np.argmax(Q_s))\n",
        "\n",
        "class Agent_EpsilonDecay:\n",
        "    def __init__(self, env,\n",
        "                 gamma=0.99, alpha=0.05,\n",
        "                 epsilon_start=1.0, epsilon_end=0.05, decay_rate=0.995):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.decay_rate = decay_rate\n",
        "        self.q = np.ones((env.observation_space.n, env.action_space.n)) * 0.5\n",
        "        self._episode_idx = 0  # incrémenté à la fin de chaque épisode\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return get_epsilon_greedy_decay_action(\n",
        "            self.q[state],\n",
        "            episode_idx=self._episode_idx,\n",
        "            epsilon_start=self.epsilon_start,\n",
        "            epsilon_end=self.epsilon_end,\n",
        "            decay_rate=self.decay_rate,\n",
        "        )\n",
        "\n",
        "    def update(self, s, a, r, s_next, done):\n",
        "        target = r + (0.0 if done else self.gamma * np.max(self.q[s_next]))\n",
        "        td = target - self.q[s, a]\n",
        "        self.q[s, a] += self.alpha * td\n",
        "        if done:\n",
        "            self._episode_idx += 1\n",
        "\n",
        "    # alias si tu réutilises l’ancien nom ailleurs\n",
        "    act   = select_action\n",
        "    train = update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c2a84c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run rapide (un seul set d'hyperparams) + plot\n",
        "\n",
        "n_episodes = 5000\n",
        "agent_eps_decay = Agent_EpsilonDecay(\n",
        "    env, gamma=0.99, alpha=0.05,\n",
        "    epsilon_start=1.0, epsilon_end=0.05, decay_rate=0.995\n",
        ")\n",
        "returns_eps_decay = execute_training_loop(env, agent_eps_decay, n_episodes)\n",
        "\n",
        "window_size = 100\n",
        "kernel = np.ones(window_size) / window_size\n",
        "ma_eps_decay = np.convolve(returns_eps_decay, kernel, mode='valid')\n",
        "\n",
        "plt.figure(figsize=(6, 3.2))\n",
        "plt.plot(returns_eps_decay, 'o', alpha=0.3, label='Rewards bruts')\n",
        "plt.plot(range(window_size - 1, len(returns_eps_decay)), ma_eps_decay,\n",
        "         color='orange', label=f'Moyenne mobile ({window_size})')\n",
        "plt.title('Agent ε-greedy décroissant')\n",
        "plt.xlabel('Épisode'); plt.ylabel('Cumulative Reward'); plt.legend()\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86acb69f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mini-grid des hyperparamètres ε-decay + heatmap + top-3\n",
        "def moving_avg(x, k=100):\n",
        "    if len(x) < k: \n",
        "        return np.asarray(x)\n",
        "    w = np.ones(k) / k\n",
        "    return np.convolve(x, w, mode='valid')\n",
        "\n",
        "def eval_config(env, alpha, decay_rate, epsilon_start=1.0, epsilon_end=0.05,\n",
        "                gamma=0.99, n_episodes=20000, seeds=(0,1,2),\n",
        "                last_k_for_score=1000, ma_window=200):\n",
        "    curves = []\n",
        "    scores = []\n",
        "    for sd in seeds:\n",
        "        agent = Agent_EpsilonDecay(\n",
        "            env, gamma=gamma, alpha=alpha,\n",
        "            epsilon_start=epsilon_start, epsilon_end=epsilon_end, decay_rate=decay_rate\n",
        "        )\n",
        "        returns = execute_training_loop(env, agent, n_episodes, random_seed=sd)\n",
        "        curves.append(moving_avg(returns, ma_window))\n",
        "        tail = returns[-last_k_for_score:] if len(returns) >= last_k_for_score else returns\n",
        "        scores.append(np.mean(tail))\n",
        "    # aligner les courbes MA\n",
        "    L = min(len(c) for c in curves)\n",
        "    curves = [c[-L:] for c in curves]\n",
        "    curve_ma = np.mean(np.stack(curves, axis=0), axis=0)\n",
        "    return {\n",
        "        \"score\": float(np.mean(scores)),\n",
        "        \"curve_ma\": curve_ma,\n",
        "        \"alpha\": alpha,\n",
        "        \"decay_rate\": decay_rate,\n",
        "        \"epsilon_end\": epsilon_end,\n",
        "        \"gamma\": gamma\n",
        "    }\n",
        "\n",
        "# Grille\n",
        "n_episodes_grid = 20000\n",
        "ma_window  = 200\n",
        "last_k     = 1000\n",
        "seeds      = (0,1,2)\n",
        "\n",
        "alphas       = [0.02, 0.05, 0.10]\n",
        "decay_rates  = [0.990, 0.995, 0.998]\n",
        "epsilon_end  = 0.05\n",
        "gamma        = 0.99\n",
        "\n",
        "results = []\n",
        "for a, d in product(alphas, decay_rates):\n",
        "    out = eval_config(env, alpha=a, decay_rate=d, epsilon_end=epsilon_end,\n",
        "                      gamma=gamma, n_episodes=n_episodes_grid, seeds=seeds,\n",
        "                      last_k_for_score=last_k, ma_window=ma_window)\n",
        "    results.append(out)\n",
        "\n",
        "# Heatmap\n",
        "score_mat = np.zeros((len(alphas), len(decay_rates)))\n",
        "for i, a in enumerate(alphas):\n",
        "    for j, d in enumerate(decay_rates):\n",
        "        score_mat[i, j] = next(r[\"score\"] for r in results if r[\"alpha\"]==a and r[\"decay_rate\"]==d)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "im = plt.imshow(score_mat, aspect='auto')\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.xticks(range(len(decay_rates)), [str(d) for d in decay_rates])\n",
        "plt.yticks(range(len(alphas)), [str(a) for a in alphas])\n",
        "plt.xlabel(\"decay_rate (ε_t = max(ε_end, ε_start * decay^t))\")\n",
        "plt.ylabel(\"alpha (learning rate)\")\n",
        "plt.title(f\"Mean return (last {last_k}) — ε_end={epsilon_end}, γ={gamma}\")\n",
        "for i in range(len(alphas)):\n",
        "    for j in range(len(decay_rates)):\n",
        "        plt.text(j, i, f\"{score_mat[i,j]:.2f}\", ha='center', va='center')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Top-3 courbes\n",
        "top_k = 3\n",
        "results_sorted = sorted(results, key=lambda r: r[\"score\"], reverse=True)[:top_k]\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "for r in results_sorted:\n",
        "    curve = r[\"curve_ma\"]\n",
        "    x = np.arange(len(curve))\n",
        "    lbl = f\"α={r['alpha']}, decay={r['decay_rate']} | score≈{r['score']:.3f}\"\n",
        "    plt.plot(x, curve, label=lbl)\n",
        "plt.title(f\"Top {top_k} configs — Moving average (window={ma_window})\")\n",
        "plt.xlabel(\"Episode (post-MA index)\"); plt.ylabel(\"Mean return (MA)\")\n",
        "plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "best = results_sorted[0]\n",
        "print(\"Best config:\")\n",
        "print(f\"  alpha       = {best['alpha']}\")\n",
        "print(f\"  decay_rate  = {best['decay_rate']}\")\n",
        "print(f\"  epsilon_end = {best['epsilon_end']}\")\n",
        "print(f\"  gamma       = {best['gamma']}\")\n",
        "print(f\"  score       = {best['score']:.4f} (mean of last {last_k}, avg over seeds={seeds})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb7d42a",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_eps_decay = Agent_EpsilonDecay(\n",
        "    env,\n",
        "    gamma=0.99,\n",
        "    alpha=0.02,\n",
        "    epsilon_start=1.0,\n",
        "    epsilon_end=0.05,\n",
        "    decay_rate=0.99\n",
        ")\n",
        "returns_eps_decay = execute_training_loop(env, agent_eps_decay, n_episodes=20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf2f081",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=6)  # pour un affichage propre\n",
        "print(agent_eps_decay.q.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8440fbd5",
      "metadata": {},
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9d2f1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#  For submission \n",
        "\n",
        "import random\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent FrozenLake-v1 (8x8, slippery)\n",
        "    Politique fixe issue d'un entraînement Q-learning (α=0.02, γ=0.99, ε_end=0.05, decay=0.99).\n",
        "    Compatible ML-Arena (aucune dépendance externe ni apprentissage à l'exécution).\n",
        "    \"\"\"\n",
        "    TOLERANCE = 1e-6\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "        # Q-table apprise (64 états × 4 actions)\n",
        "        self.q = [[0.29420995385560217, 0.2950634022720467, 0.294264371748934, 0.292005118609326],\n",
        "                  [0.30260766833240094, 0.30252921011496864, 0.3063348442238046, 0.30303728824092435],\n",
        "                  [0.30413644321670846, 0.31444525168285875, 0.32121601400641536, 0.3151170093074783],\n",
        "                  [0.32922190454203654, 0.3299628666925131, 0.33870964281300375, 0.32435021886832976],\n",
        "                  [0.35067493294855384, 0.35103205231633, 0.36187477989983363, 0.3459180657178977],\n",
        "                  [0.36853061166782275, 0.3692578415676267, 0.38701590644541994, 0.3685439191670447],\n",
        "                  [0.3895959438840707, 0.389621196584198, 0.4054396444753669, 0.3880749988106463],\n",
        "                  [0.40733073465670694, 0.39230821043866465, 0.3900784436458687, 0.3863725607179519],\n",
        "                  [0.29101726983769477, 0.2907994925705392, 0.2910708139035046, 0.29351206752249603],\n",
        "                  [0.29407473720389976, 0.29458600393201834, 0.2949264719933318, 0.3004210919639085],\n",
        "                  [0.2961404397875992, 0.2940007985759691, 0.30196432884961155, 0.3133460907802189],\n",
        "                  [0.2342952456648879, 0.225624879435896, 0.20496204754952885, 0.33121734757643734],\n",
        "                  [0.33538120205422334, 0.3401637082716003, 0.3478156929089452, 0.35533687711729006],\n",
        "                  [0.36598906792528524, 0.3616942107065052, 0.38864166391269866, 0.36355811648800374],\n",
        "                  [0.39252185484851465, 0.39745488248205424, 0.4140692395411607, 0.3999013594197966],\n",
        "                  [0.4008374063977787, 0.431531390537535, 0.40984998597380173, 0.3985284185698438],\n",
        "                  [0.25866382120486203, 0.25959369284982364, 0.25882630704825654, 0.2813091200341257],\n",
        "                  [0.2751928273591275, 0.25417743384308866, 0.25606423838356934, 0.2597368034580428],\n",
        "                  [0.26681414057072605, 0.22792845940427106, 0.23355151234614147, 0.22711846371039773],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.23522801815747674, 0.20934178555851846, 0.3178533732013346, 0.2321413572941838],\n",
        "                  [0.20224553780082047, 0.27729113739213623, 0.2555381418916086, 0.3778004623564212],\n",
        "                  [0.4037714173121307, 0.4104399915831587, 0.43643334435442593, 0.4103132594054086],\n",
        "                  [0.43304476091704236, 0.4547874465496263, 0.4420652035913417, 0.4345106525539263],\n",
        "                  [0.26596798487469553, 0.2469218550045634, 0.24701243615658877, 0.24950642197391018],\n",
        "                  [0.2455293878916953, 0.23214296846560123, 0.23239501286727143, 0.23291094068548313],\n",
        "                  [0.20656710210352167, 0.19819002699879248, 0.19943073012879176, 0.22248339182950733],\n",
        "                  [0.11250617483191401, 0.16046350484863786, 0.1189518278356293, 0.1297740653871374],\n",
        "                  [0.2299693605032685, 0.15991485242320733, 0.17621935703572217, 0.16964498226566394],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.26993781916065807, 0.35407388747666463, 0.4556884922773936, 0.2882445891919432],\n",
        "                  [0.4837766989305572, 0.504995118961017, 0.477011564968623, 0.4558916783583016],\n",
        "                  [0.2532950360745477, 0.24269268227701796, 0.24578256426889544, 0.24223194816066984],\n",
        "                  [0.19727299454786376, 0.20078742710265599, 0.19990887017106815, 0.21191051271698796],\n",
        "                  [0.15821593304730974, 0.12029378984119232, 0.12307993807721067, 0.11927169791265933],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.1645812491227076, 0.18925229039754685, 0.21654195986641717, 0.2055004309259897],\n",
        "                  [0.16289879760780496, 0.27988360464196665, 0.2353087092978428, 0.22625331132859317],\n",
        "                  [0.2550259733802866, 0.2727491082857408, 0.2746634045188174, 0.4409431660577232],\n",
        "                  [0.5391747279511636, 0.5430920290986273, 0.6047019767301804, 0.5051795583106136],\n",
        "                  [0.25432904540488543, 0.23930101881788265, 0.23053190107472743, 0.22853337172772095],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.09694842396972428, 0.11767211446455438, 0.0993456175994056, 0.09729413016077255],\n",
        "                  [0.17081487852482813, 0.1659932679577362, 0.16190447732412883, 0.17483634447582602],\n",
        "                  [0.23145365027627635, 0.17997364967860646, 0.20444292588434534, 0.18235120638879368],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.4932673356399401, 0.44276483939458433, 0.691705028610584, 0.42133293937908534],\n",
        "                  [0.264372555546283, 0.26194982620837226, 0.25990692427294143, 0.2559933631330844],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.1846589080329749, 0.1862770186307141, 0.18895912419840366, 0.1861395502042241],\n",
        "                  [0.12106992725526072, 0.12201608889477586, 0.12089805983280975, 0.12674124848433793],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.1945359642020071, 0.20031001078324748, 0.23090807500103386, 0.19072716087815655],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.47611352566857174, 0.5436311294499757, 0.8768064289021438, 0.5234680671251831],\n",
        "                  [0.28114620430149856, 0.28113351938118925, 0.2812632881925352, 0.2811045465022166],\n",
        "                  [0.2653829658006844, 0.27051855313823914, 0.2696855476507959, 0.2670506565172771],\n",
        "                  [0.2524507530381983, 0.24617896770037465, 0.24409233115755297, 0.24832477386155366],\n",
        "                  [0.5, 0.5, 0.5, 0.5],\n",
        "                  [0.23699863786094263, 0.2348784540120545, 0.23676884962466216, 0.23937105065157935],\n",
        "                  [0.4197697242829102, 0.429263474822547, 0.4189998885409268, 0.42092691782354485],\n",
        "                  [0.47748411036813004, 0.6835169081743937, 0.5101017332060352, 0.4808238304991155],\n",
        "                  [0.5, 0.5, 0.5, 0.5]]\n",
        "\n",
        "        self.n_states = len(self.q)\n",
        "        self.n_actions = len(self.q[0])\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        s = int(observation)\n",
        "        if s < 0 or s >= self.n_states:\n",
        "            return self.env.action_space.sample()\n",
        "\n",
        "        q_row = self.q[s]\n",
        "        best_val = max(q_row)\n",
        "        best_actions = [i for i, v in enumerate(q_row) if abs(v - best_val) < self.TOLERANCE]\n",
        "        return random.choice(best_actions) if best_actions else self.env.action_space.sample()\n",
        "\n",
        "\n",
        "# Test local \n",
        "\n",
        "import gymnasium as gym\n",
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "\n",
        "agent = Agent(env)\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a54afc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# même env que le challenge\n",
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "\n",
        "# ton agent entraîné dans __init__\n",
        "agent = Agent(env)\n",
        "\n",
        "def eval_once(agent, env, attempts=10):\n",
        "    \"\"\"Total de récompenses (succès) sur 'attempts' épisodes.\"\"\"\n",
        "    total = 0.0\n",
        "    for _ in range(attempts):\n",
        "        obs, _ = env.reset()\n",
        "        done = False\n",
        "        truncated = False\n",
        "        while not (done or truncated):\n",
        "            a = agent.choose_action(obs)\n",
        "            obs, r, done, truncated, _ = env.step(a)\n",
        "            total += r\n",
        "    return total\n",
        "\n",
        "# 1) Un run officiel: 10 tentatives\n",
        "total = eval_once(agent, env, attempts=10)\n",
        "print(f\"Total reward over 10 attempts: {total}  (target > 3.5 ≈ 0.35 de moyenne)\")\n",
        "\n",
        "# 2) Moyenne sur plusieurs runs de 10 tentatives (plus stable)\n",
        "runs = 30\n",
        "scores = [eval_once(agent, env, attempts=10) for _ in range(runs)]\n",
        "print(f\"Mean over {runs} runs of 10 attempts: {np.mean(scores):.3f}  ± {np.std(scores):.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

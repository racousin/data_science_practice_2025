{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd09113-2f55-43ea-914c-ab3c0e3ee3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge, RidgeClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingRegressor, StackingRegressor, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.base import RegressorMixin, ClassifierMixin, BaseEstimator\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80dc69a1-e21d-4310-a0a4-5cb5b7d69fb4",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae3aeb5-dcdf-4690-b869-d211a02f303c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module6/exercise/module6_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module6/exercise/module6_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module6_exercise_train.csv')\n",
        "download_file(test_data_url, 'module6_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ae6bb0-639c-472a-8748-4bbbdb96e142",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('module6_exercise_train.csv', index_col='index')\n",
        "data_test = pd.read_csv('module6_exercise_test.csv', index_col='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea77dab9-6dcc-42bc-894b-1d8a23774d33",
      "metadata": {},
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92c5579-a099-4018-a22e-06513a885133",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead6e347-8a41-489d-a184-98d4259ff9be",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d62d9ab-50b9-4d5b-9611-adbd10840ae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5babb7-ad30-43eb-b3ec-4eebca726764",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e5dd21-1d73-415c-a4a7-c1c869ded46b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314189f7-31bb-47a9-afca-97d9ec489b60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the distribution using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data_train['end_of_day_return'], bins=50, kde=True)\n",
        "plt.title('Distribution of End of Day Return')\n",
        "plt.xlabel('End of Day Return')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab85a4fe-b7d4-47f7-9d4a-4a155c44e24c",
      "metadata": {},
      "source": [
        "### Model Building and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a86f0e4-55fa-46a9-8f53-735cf62aa6ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = data_train.pop('end_of_day_return')\n",
        "X = data_train.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746ed3ab-0de5-4c35-94aa-76251187dede",
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_accuracy(y_true, y_pred):\n",
        "    weights = np.abs(y_true)\n",
        "    \n",
        "    # Compute the sign of true and predicted values\n",
        "    sign_true = np.sign(y_true)\n",
        "    sign_pred = np.sign(y_pred)\n",
        "    \n",
        "    # Correct predictions where the sign of the true and predicted values match\n",
        "    correct_predictions = sign_true == sign_pred\n",
        "    \n",
        "    # Compute the weighted accuracy\n",
        "    weighted_acc = np.sum(weights * correct_predictions) / np.sum(weights)\n",
        "    \n",
        "    return weighted_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04424025-b909-46d3-af2d-0e7d6fb9107b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot the evaluation results\n",
        "def plot_results(mse_train, mse_test, w_acc_train, w_acc_test):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # MSE plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(mse_train, label=\"Train MSE\", marker='o')\n",
        "    plt.plot(mse_test, label=\"Test MSE\", marker='o')\n",
        "    plt.fill_between(range(len(mse_train)), np.min(mse_train), np.max(mse_train), color='blue', alpha=0.1)\n",
        "    plt.fill_between(range(len(mse_test)), np.min(mse_test), np.max(mse_test), color='orange', alpha=0.1)\n",
        "    plt.title(\"MSE over Folds\")\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # weighted_accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(w_acc_train, label=\"Train weighted_accuracy\", marker='o')\n",
        "    plt.plot(w_acc_test, label=\"Test weighted_accuracy\", marker='o')\n",
        "    plt.fill_between(range(len(w_acc_train)), np.min(w_acc_train), np.max(w_acc_train), color='blue', alpha=0.1)\n",
        "    plt.fill_between(range(len(w_acc_test)), np.min(w_acc_test), np.max(w_acc_test), color='orange', alpha=0.1)\n",
        "    plt.title(\"weighted_accuracy over Folds\")\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"weighted_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_multi_model_results(results):\n",
        "    # Set up the plot\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\n",
        "    \n",
        "    # Colors for train and test\n",
        "    train_color = 'skyblue'\n",
        "    test_color = 'lightgreen'\n",
        "    \n",
        "    # Plot MSE\n",
        "    ax1.set_title('Mean Squared Error (MSE) Comparison', fontsize=16)\n",
        "    ax1.set_ylabel('MSE', fontsize=12)\n",
        "    ax1.set_xlabel('Models', fontsize=12)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Plot weighted_accuracy\n",
        "    ax2.set_title('weighted_accuracy Comparison', fontsize=16)\n",
        "    ax2.set_ylabel('weighted_accuracy', fontsize=12)\n",
        "    ax2.set_xlabel('Models', fontsize=12)\n",
        "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    x = np.arange(len(results))\n",
        "    width = 0.35\n",
        "    \n",
        "    for i, (model_name, scores) in enumerate(results.items()):\n",
        "        # MSE\n",
        "        mse_train = scores['mse_train']\n",
        "        mse_test = scores['mse_test']\n",
        "        \n",
        "        ax1.bar(x[i] - width/2, np.mean(mse_train), width, label='Train' if i == 0 else \"\", \n",
        "                color=train_color, alpha=0.7)\n",
        "        ax1.bar(x[i] + width/2, np.mean(mse_test), width, label='Test' if i == 0 else \"\", \n",
        "                color=test_color, alpha=0.7)\n",
        "        \n",
        "        ax1.errorbar(x[i] - width/2, np.mean(mse_train), \n",
        "                     yerr=[[np.mean(mse_train)-np.min(mse_train)], [np.max(mse_train)-np.mean(mse_train)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        ax1.errorbar(x[i] + width/2, np.mean(mse_test), \n",
        "                     yerr=[[np.mean(mse_test)-np.min(mse_test)], [np.max(mse_test)-np.mean(mse_test)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        \n",
        "        # weighted_accuracy\n",
        "        w_acc_train = scores['w_acc_train']\n",
        "        w_acc_test = scores['w_acc_test']\n",
        "        \n",
        "        ax2.bar(x[i] - width/2, np.mean(w_acc_train), width, label='Train' if i == 0 else \"\", \n",
        "                color=train_color, alpha=0.7)\n",
        "        ax2.bar(x[i] + width/2, np.mean(w_acc_test), width, label='Test' if i == 0 else \"\", \n",
        "                color=test_color, alpha=0.7)\n",
        "        \n",
        "        ax2.errorbar(x[i] - width/2, np.mean(w_acc_train), \n",
        "                     yerr=[[np.mean(w_acc_train)-np.min(w_acc_train)], [np.max(w_acc_train)-np.mean(w_acc_train)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        ax2.errorbar(x[i] + width/2, np.mean(w_acc_test), \n",
        "                     yerr=[[np.mean(w_acc_test)-np.min(w_acc_test)], [np.max(w_acc_test)-np.mean(w_acc_test)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "    \n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "    \n",
        "    ax1.legend(loc='upper left')\n",
        "    ax2.legend(loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c73c55-9684-411c-9ad3-ba6f9ba7de8d",
      "metadata": {},
      "source": [
        "#### Simple Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6a7f41-1184-4172-ab8b-220f45ab2172",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to handle train-test evaluation in a fold\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, model):\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions on train set\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    # Make predictions on train set\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Compute MSE for train and test\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    \n",
        "    # Compute weighted_accuracy\n",
        "    \n",
        "    w_acc_train = weighted_accuracy(y_train, y_pred_train)\n",
        "    w_acc_test = weighted_accuracy(y_test, y_pred_test)\n",
        "    \n",
        "    return mse_train, mse_test, w_acc_train, w_acc_test\n",
        "\n",
        "\n",
        "def run_multi_model_cv(X, y, models, n_splits=5):\n",
        "    fold = KFold(n_splits=n_splits)\n",
        "    results = {name: {'mse_train': [], 'mse_test': [], 'w_acc_train': [], 'w_acc_test': []} \n",
        "               for name in models.keys()}\n",
        "    \n",
        "    for train_index, test_index in fold.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
        "        y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            mse_train, mse_test, w_acc_train, w_acc_test = train_and_evaluate(\n",
        "                X_train, X_test, y_train, y_test, model\n",
        "            )\n",
        "            results[name]['mse_train'].append(mse_train)\n",
        "            results[name]['mse_test'].append(mse_test)\n",
        "            results[name]['w_acc_train'].append(w_acc_train)\n",
        "            results[name]['w_acc_test'].append(w_acc_test)\n",
        "        # Find the model with the best mean w_acc test score\n",
        "    best_mean_w_acc = -1\n",
        "    best_model = None\n",
        "    best_min_w_acc = None\n",
        "    best_max_w_acc = None\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        w_acc_test_scores = result['w_acc_test']\n",
        "        mean_w_acc_test = sum(w_acc_test_scores) / len(w_acc_test_scores)  # Calculate mean w_acc score\n",
        "        min_w_acc_test = min(w_acc_test_scores)  # Minimum w_acc score\n",
        "        max_w_acc_test = max(w_acc_test_scores)  # Maximum w_acc score\n",
        "        \n",
        "        if mean_w_acc_test > best_mean_w_acc:\n",
        "            best_mean_w_acc = mean_w_acc_test\n",
        "            best_min_w_acc = min_w_acc_test\n",
        "            best_max_w_acc = max_w_acc_test\n",
        "            best_model = name\n",
        "    \n",
        "    # Print the best mean w_acc test score, min, max, and the associated model\n",
        "    print(f\"Best mean w_acc test score: {best_mean_w_acc:.4f} by model: {best_model}\")\n",
        "    print(f\"Min w_acc test score: {best_min_w_acc:.4f}, Max w_acc test score: {best_max_w_acc:.4f}\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891e158b-b199-41ac-a244-537429eb1d22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Run cross-validation\n",
        "results = run_multi_model_cv(X, y, {\"RandomForestRegressor\": RandomForestRegressor(n_jobs=-1)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b0380d-c7ee-413e-9b13-756f8e7e3847",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Plot the results\n",
        "plot_results(results[\"RandomForestRegressor\"][\"mse_train\"],\n",
        "             results[\"RandomForestRegressor\"][\"mse_test\"],\n",
        "             results[\"RandomForestRegressor\"][\"w_acc_train\"],\n",
        "             results[\"RandomForestRegressor\"][\"w_acc_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc76e0a4-6af0-43eb-af13-0bad0adbbdbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Ridge': Ridge(),\n",
        "    'Decision Tree Regressor': RandomForestRegressor(n_jobs=-1)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264692a5-ce69-4567-a108-669d0cfcc3a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250f1f68-32b7-47d3-8934-952785a3046f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e21e12-8f5d-4fe5-beb0-566d8a971ae7",
      "metadata": {},
      "source": [
        "#### Manage properly the objective weighted_accuracy\n",
        "should we create different classes? custom loss?\n",
        "\n",
        "Create Compare and Optimize different models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73f8c8f",
      "metadata": {},
      "source": [
        "#### Minimisation of the weighted_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "997b7f2f",
      "metadata": {},
      "source": [
        "##### Linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd387efe",
      "metadata": {},
      "source": [
        "We try linear models because the distribution of Y seems to be Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff32747",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'OLS': LinearRegression(),\n",
        "    'Lasso': Lasso(),\n",
        "    'Ridge': Ridge()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13bde946",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719e2c66",
      "metadata": {},
      "source": [
        "We try a GridSearch on the parameter of Ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1b968",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter values\n",
        "alphas = [100, 500, 750, 1000, 1250]\n",
        "\n",
        "# Creation of the model dictionary\n",
        "ridge_models = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "    name = f\"ridge_alpha{alpha}\"\n",
        "    ridge_models[name] = Ridge(alpha=alpha)\n",
        "\n",
        "# List of models\n",
        "for k, v in list(ridge_models.items()):\n",
        "    print(k, \":\", v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d82974a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "ridge_results = run_multi_model_cv(X, y, ridge_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833ba0e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the ridge_results\n",
        "plot_results(ridge_results[\"ridge_alpha750\"][\"mse_train\"],\n",
        "             ridge_results[\"ridge_alpha750\"][\"mse_test\"],\n",
        "             ridge_results[\"ridge_alpha750\"][\"w_acc_train\"],\n",
        "             ridge_results[\"ridge_alpha750\"][\"w_acc_test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79595e33",
      "metadata": {},
      "source": [
        "We try a GridSearch on the parameter of Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc8c33c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter values\n",
        "alphas = [0.001, 0.005, 0.01, 0.1, 1]\n",
        "\n",
        "# Creation of the model dictionary\n",
        "lasso_models = {}\n",
        "\n",
        "for alpha in alphas:\n",
        "    name = f\"lasso_alpha{alpha}\"\n",
        "    lasso_models[name] = Lasso(alpha=alpha)  \n",
        "    \n",
        "# List of models\n",
        "for k, v in list(lasso_models.items()):\n",
        "    print(k, \":\", v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eed315c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "lasso_results = run_multi_model_cv(X, y, lasso_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c807e79a",
      "metadata": {},
      "source": [
        "Our best model is Ridge followed by the Lasso."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f1a387a",
      "metadata": {},
      "source": [
        "We try Gauss-Lasso and Ridge after Lasso (Ridge-Lasso)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da25c607",
      "metadata": {},
      "outputs": [],
      "source": [
        "def GaussAndRidge_Lasso(X, y, alpha, count, alphas_ridge, n_splits=5):\n",
        "    fold = KFold(n_splits=n_splits)\n",
        "    n, p = X.shape\n",
        "    X_count = np.zeros(p)\n",
        "    w_acc_test = []\n",
        "\n",
        "    for train_index, test_index in fold.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
        "        y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
        "\n",
        "        # Create the lasso\n",
        "        lasso = Lasso(alpha = alpha)\n",
        "        \n",
        "        # Train the lasso\n",
        "        lasso.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on test set\n",
        "        y_pred_test = lasso.predict(X_test)\n",
        "    \n",
        "        # Compute weighted_accuracy\n",
        "        w_acc_test.append(weighted_accuracy(y_test, y_pred_test))\n",
        "\n",
        "        # Retrieving selected columns\n",
        "        selected_mask = lasso.coef_ != 0\n",
        "        selected_features = X.columns[selected_mask]\n",
        "\n",
        "        # Count the number of selections\n",
        "        X_count = X_count + selected_mask\n",
        "    \n",
        "\n",
        "    mean_w_acc_test = sum(w_acc_test) / len(w_acc_test)  # Calculate mean w_acc score\n",
        "    min_w_acc_test = min(w_acc_test)  # Minimum w_acc score\n",
        "    max_w_acc_test = max(w_acc_test)  # Maximum w_acc score\n",
        "\n",
        "    # Print the mean w_acc test score, min and max of the lasso model\n",
        "    print(f\"Lasso (alpha={alpha}):\")\n",
        "    print(f\"Mean w_acc test score: {mean_w_acc_test:.4f}\")\n",
        "    print(f\"Min w_acc test score: {min_w_acc_test:.4f}, Max w_acc test score: {max_w_acc_test:.4f}\")\n",
        "    print(\"----------------------------------------------------------\")\n",
        "\n",
        "    # Selection of columns present at least count\n",
        "    selected_mask = X_count >= count\n",
        "    selected_features = X.columns[selected_mask]\n",
        "\n",
        "    # Reduction of the dataset\n",
        "    X_reduced = X[selected_features]\n",
        "\n",
        "    # Training of a classic linear model and ridge models on the reduced dataset\n",
        "    results = run_multi_model_cv(X_reduced, y, {\"Gauss-Lasso\":LinearRegression()})\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    # Creation of the model dictionary\n",
        "    ridge_models = {}\n",
        "    for alpha in alphas_ridge:\n",
        "        name = f\"Ridge-Lasso_alpha{alpha}\"\n",
        "        ridge_models[name] = Ridge(alpha=alpha)\n",
        "\n",
        "    ridge_results = run_multi_model_cv(X_reduced, y, ridge_models)\n",
        "    return results, ridge_results, X_reduced, selected_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033432e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_alpha_lasso = 0.01 # alpha of our best lasso model\n",
        "best_count = 5 # We tested all the possible values by hand\n",
        "\n",
        "# Hyperparameter values\n",
        "alphas_ridge = [1200, 1225, 1250, 1275, 1300]\n",
        "\n",
        "results, ridge_results, X_reduced, selected_features = GaussAndRidge_Lasso(X, y, alpha=best_alpha_lasso, count=best_count, alphas_ridge=alphas_ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333cff6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_alpha_ridge_lasso = 1250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b5a2e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "plot_results(results[\"Gauss-Lasso\"][\"mse_train\"],\n",
        "             results[\"Gauss-Lasso\"][\"mse_test\"],\n",
        "             results[\"Gauss-Lasso\"][\"w_acc_train\"],\n",
        "             results[\"Gauss-Lasso\"][\"w_acc_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18bb985",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "plot_results(ridge_results[\"Ridge-Lasso_alpha1250\"][\"mse_train\"],\n",
        "             ridge_results[\"Ridge-Lasso_alpha1250\"][\"mse_test\"],\n",
        "             ridge_results[\"Ridge-Lasso_alpha1250\"][\"w_acc_train\"],\n",
        "             ridge_results[\"Ridge-Lasso_alpha1250\"][\"w_acc_test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0672ff9d",
      "metadata": {},
      "source": [
        "Our best model is Ridge-Lasso followed by Gauss-Lasso."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3896df6",
      "metadata": {},
      "source": [
        "##### Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b96491",
      "metadata": {},
      "source": [
        "We try a GridSearch on the parameter of Xgboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d711fbd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter values\n",
        "max_depths = [8, 9, 10]\n",
        "learning_rates = [0.95, 1, 1.05]\n",
        "n_estimators = [33, 35, 37]\n",
        "\n",
        "# Creation of the model dictionary\n",
        "xgboost_models = {}\n",
        "for depth in max_depths:\n",
        "    for lr in learning_rates:\n",
        "        for n_est in n_estimators:\n",
        "            name = f\"Xgboost_depth={depth}_lr={lr}_n_estimators={n_est}\"\n",
        "            xgboost_models[name] = XGBRegressor(max_depth=depth, learning_rate=lr, n_estimators=n_est)\n",
        "\n",
        "print(\"Number of models:\", len(xgboost_models))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21603a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "# xgboost_results = run_multi_model_cv(X, y, xgboost_models)\n",
        "\n",
        "# Best mean w_acc test score: 0.5272 by model: Xgboost_depth=9_lr=1_n_estimators=35\n",
        "# Min w_acc test score: 0.5073, Max w_acc test score: 0.5517"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b4e1f3",
      "metadata": {},
      "source": [
        "We try on the reduced dataset by Lasso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640faa3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter values\n",
        "max_depths = [7, 8, 9]\n",
        "learning_rates = [1.95, 2, 2.05]\n",
        "n_estimators = [105, 110, 115]\n",
        "\n",
        "# Creation of the model dictionary\n",
        "xgboost_models = {}\n",
        "for depth in max_depths:\n",
        "    for lr in learning_rates:\n",
        "        for n_est in n_estimators:\n",
        "            name = f\"Xgboost_depth={depth}_lr={lr}_n_estimators={n_est}\"\n",
        "            xgboost_models[name] = XGBRegressor(max_depth=depth, learning_rate=lr, n_estimators=n_est)\n",
        "\n",
        "print(\"Number of models:\", len(xgboost_models))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1834a905",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "# xgboost_results = run_multi_model_cv(X_reduced, y, xgboost_models)\n",
        "\n",
        "# Best mean w_acc test score: 0.5289 by model: Xgboost_depth=8_lr=2_n_estimators=110\n",
        "# Min w_acc test score: 0.4993, Max w_acc test score: 0.5500"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88af4ad",
      "metadata": {},
      "source": [
        "##### Stacking (Stacked Generalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec55d288",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(X, y, model, n_splits=5):\n",
        "    fold = KFold(n_splits=n_splits)\n",
        "    w_acc_test = []\n",
        "\n",
        "    for train_index, test_index in fold.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
        "        y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on test set\n",
        "        y_pred_test = model.predict(X_test)\n",
        "    \n",
        "        # Compute weighted_accuracy\n",
        "        w_acc_test.append(weighted_accuracy(y_test, y_pred_test))\n",
        "    \n",
        "\n",
        "    mean_w_acc_test = sum(w_acc_test) / len(w_acc_test)  # Calculate mean w_acc score\n",
        "    min_w_acc_test = min(w_acc_test)  # Minimum w_acc score\n",
        "    max_w_acc_test = max(w_acc_test)  # Maximum w_acc score\n",
        "\n",
        "    # Print the mean w_acc test score, min and max of the model\n",
        "    print(f\"model:\")\n",
        "    print(f\"Mean w_acc test score: {mean_w_acc_test:.4f}\")\n",
        "    print(f\"Min w_acc test score: {min_w_acc_test:.4f}, Max w_acc test score: {max_w_acc_test:.4f}\")\n",
        "\n",
        "    return mean_w_acc_test, min_w_acc_test, max_w_acc_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00c2c43",
      "metadata": {},
      "source": [
        "Our best models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4a66c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "gauss_lasso_results = run_multi_model_cv(X_reduced, y, {\"Gauss-Lasso\":LinearRegression()})\n",
        "print(\"----------------------------------------------------------\")\n",
        "ridge_lasso_results = run_multi_model_cv(X_reduced, y, {\"Ridge-Lasso\":Ridge(alpha=best_alpha_ridge_lasso)})\n",
        "print(\"----------------------------------------------------------\")\n",
        "xgboost_results = run_multi_model_cv(X_reduced, y, {\"Xgboost\":XGBRegressor(max_depth=8, learning_rate=2, n_estimators=110)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6860956f",
      "metadata": {},
      "source": [
        "We try stacking on our best models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d97f86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# base models\n",
        "base_models = [\n",
        "    ('Ridge-Lasso', Ridge(alpha=best_alpha_ridge_lasso)),\n",
        "    ('Xgboost', XGBRegressor(max_depth=8, learning_rate=2, n_estimators=110))\n",
        "]\n",
        "\n",
        "# meta-learner\n",
        "stacking = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=Ridge(),\n",
        "    cv=5  # cross-validation to generate the meta-features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6094bebb",
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_w_acc_test, min_w_acc_test, max_w_acc_test = evaluate_model(X_reduced, y, stacking)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c323736",
      "metadata": {},
      "source": [
        "Finally, our best model is the Ridge-Lasso with an average weighted accuracy of 0.5299."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0431ff6-0e5d-4ead-8735-84e1a728ef11",
      "metadata": {},
      "source": [
        "### Submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe4a5d9-be2e-40f1-b1b4-59cac5e7197b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('module6_exercise_train.csv', index_col='index')\n",
        "X_test = pd.read_csv('module6_exercise_test.csv', index_col='index')\n",
        "y_train = data_train.pop('end_of_day_return')\n",
        "X_train = data_train.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db2ea4cd-5463-41ac-b49a-6ec38252b5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on complete data (X_train, y_train) and predict on X_test\n",
        "alpha_ridge = [best_alpha_ridge_lasso]\n",
        "results, ridge_results, X_train_reduced, selected_features = GaussAndRidge_Lasso(X_train, y_train, alpha=best_alpha_lasso, count=best_count, alphas_ridge=alpha_ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a57b960",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce the test dataset\n",
        "X_test_reduced = X_test[selected_features]\n",
        "print(X_reduced.columns)\n",
        "print(X_test_reduced.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aecdd3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ridge_lasso = Ridge(alpha=best_alpha_ridge_lasso)\n",
        "mean_w_acc_test, min_w_acc_test, max_w_acc_test = evaluate_model(X_train_reduced, y_train, ridge_lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc23603c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "ridge_lasso.fit(X_train_reduced, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_test = ridge_lasso.predict(X_test_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9877df-0e30-40ed-91a0-69511d359033",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "submission = pd.DataFrame({\n",
        "    'index': X_test.index,\n",
        "    'end_of_day_return': y_pred_test\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False, sep=',')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

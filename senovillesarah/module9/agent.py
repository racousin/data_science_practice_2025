
import random

class Agent:
    def __init__(self, env):
        # je garde l'environnement
        self.env = env
        # Ici on colle la Q-table apprise hors ligne
        self.Q = [[0.379822, 0.381208, 0.381296, 0.381987], [0.387686, 0.393834, 0.396789, 0.392168], [0.401263, 0.393919, 0.411577, 0.404278], [0.412099, 0.423788, 0.428393, 0.41851], [0.439321, 0.437193, 0.446807, 0.435641], [0.461947, 0.437497, 0.477568, 0.470087], [0.489724, 0.491687, 0.507891, 0.500561], [0.508826, 0.495327, 0.521975, 0.507772], [0.372792, 0.378569, 0.372783, 0.38057], [0.378419, 0.37911, 0.388616, 0.390861], [0.378901, 0.384723, 0.389491, 0.40777], [0.306092, 0.271261, 0.192677, 0.425032], [0.424573, 0.424439, 0.430245, 0.447883], [0.462084, 0.46709, 0.475146, 0.484985], [0.503742, 0.505556, 0.511978, 0.495414], [0.519936, 0.524105, 0.535583, 0.512079], [0.363731, 0.357041, 0.366048, 0.368274], [0.367797, 0.357717, 0.362273, 0.372242], [0.360071, 0.203003, 0.212386, 0.288844], [0.0, 0.0, 0.0, 0.0], [0.165207, 0.26189, 0.397987, 0.31324], [0.244773, 0.310855, 0.342719, 0.409621], [0.485391, 0.508537, 0.509183, 0.505618], [0.546134, 0.536503, 0.554996, 0.533032], [0.346979, 0.331184, 0.345461, 0.352486], [0.317503, 0.310109, 0.327917, 0.339505], [0.294616, 0.284331, 0.223261, 0.301758], [0.124132, 0.19764, 0.044231, 0.211458], [0.27847, 0.184013, 0.206848, 0.155309], [0.0, 0.0, 0.0, 0.0], [0.253917, 0.314493, 0.509288, 0.391086], [0.548566, 0.567276, 0.588164, 0.551287], [0.302249, 0.290915, 0.311257, 0.320873], [0.256231, 0.164413, 0.187382, 0.289321], [0.152053, 0.043329, 0.137134, 0.184962], [0.0, 0.0, 0.0, 0.0], [0.153494, 0.167452, 0.231804, 0.199302], [0.133823, 0.305486, 0.264708, 0.203588], [0.158017, 0.35627, 0.308308, 0.440469], [0.557755, 0.622413, 0.643864, 0.588574], [0.279294, 0.153106, 0.229827, 0.204378], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.003946, 0.05527, 0.065257, 0.067797], [0.122664, 0.093876, 0.154395, 0.159344], [0.186022, 0.088138, 0.128625, 0.162662], [0.0, 0.0, 0.0, 0.0], [0.551998, 0.653939, 0.713163, 0.457257], [0.261783, 0.140028, 0.180178, 0.222388], [0.0, 0.0, 0.0, 0.0], [0.018482, 0.00597, 0.013521, 0.000941], [0.019144, 0.001296, 0.007722, 0.02424], [0.0, 0.0, 0.0, 0.0], [0.069913, 0.032439, 0.100779, 0.08485], [0.0, 0.0, 0.0, 0.0], [0.609852, 0.818184, 0.910502, 0.412618], [0.257912, 0.225964, 0.235324, 0.201547], [0.144473, 0.182086, 0.098432, 0.09591], [0.081182, 0.050158, 0.026268, 0.047981], [0.0, 0.0, 0.0, 0.0], [0.000809, 0.009078, 0.001028, 0.001663], [0.024167, 0.064922, 0.085727, 0.055384], [0.026923, 0.234484, 0.196607, 0.19], [0.0, 0.0, 0.0, 0.0]]
        self.nS = len(self.Q)                   # 64 états pour 64 cases
        self.nA = len(self.Q[0]) if self.nS>0 else env.action_space.n  # 4 actions : en haut, en bas, à droite, à gauche

    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):
        # on observe la case
        s = int(observation)

        #si y a un pb je mets une action au hasard
        if s < 0 or s >= self.nS:
            return self.env.action_space.sample()

        # je récupère la ligne de la Q-table pour cet état
        row = self.Q[s]
        # et je cherche la meilleure valeur Q dans cette ligne
        m = max(row)
        # je prends toutes les actions qui atteignent ce max
        meilleures = [a for a, q in enumerate(row) if q == m]
        # je choisis au hasard parmi les meilleures si plusieurs ont le meme max
        return random.choice(meilleures)

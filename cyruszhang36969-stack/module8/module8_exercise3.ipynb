{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Exercise 3: Mathematical Problem Solving with LLMs\n",
        "\n",
        "**This is a marked exercise (graded)**\n",
        "\n",
        "Apply LLMs to solve mathematical reasoning tasks. Test different pre-trained models with various prompting strategies and optionally fine-tune with LoRA to improve performance.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Evaluate LLMs on mathematical reasoning\n",
        "- Design effective prompts for numerical tasks\n",
        "- Implement and compare different prompting strategies\n",
        "- Optionally: Fine-tune models using LoRA\n",
        "- Measure performance using accuracy metric with tolerance\n",
        "\n",
        "**Deliverables:**\n",
        "- Completed notebook with your approach\n",
        "- `submission.csv` with predictions on test set (100 problems)\n",
        "- Score: Accuracy with 2 decimal precision tolerance (threshold: 70%)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Part 1: Setup and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-2",
      "metadata": {
        "id": "cell-2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch peft datasets pandas scikit-learn matplotlib requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-3",
        "outputId": "1d5e728d-b58f-4275-c6ce-1dc41bc945c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import re\n",
        "\n",
        "# Check for CUDA and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "## Part 2: Download Dataset\n",
        "\n",
        "Download the math problem dataset (1000 problems: 900 train, 100 test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-5",
        "outputId": "e5b217fd-4b77-4c59-921a-5acaea17f0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train.csv\n",
            "Downloaded test.csv\n"
          ]
        }
      ],
      "source": [
        "# URLs for the dataset files\n",
        "base_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module8/exercise/'\n",
        "\n",
        "train_url = base_url + 'train.csv'\n",
        "test_url = base_url + 'test.csv'\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file from URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "# Download files\n",
        "download_file(train_url, 'train.csv')\n",
        "download_file(test_url, 'test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "64fad155-aba6-4d7b-b424-08e49f341335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 900\n",
            "Test set size: 100\n",
            "\n",
            "Training set category distribution:\n",
            "category\n",
            "algebra          150\n",
            "arithmetic       153\n",
            "fractions        143\n",
            "geometry         155\n",
            "percentage       152\n",
            "word_problems    147\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample training problems:\n",
            "   id       category                                            problem  \\\n",
            "0   0     percentage                                Increase 109 by 25%   \n",
            "1   1     arithmetic                                   What is 76 + 55?   \n",
            "2   2  word_problems  Sarah has $286. She spends $128. How much mone...   \n",
            "3   3       geometry  What is the circumference of a circle with rad...   \n",
            "4   4       geometry   What is the volume of a cube with side length 3?   \n",
            "5   5     percentage                                 What is 7% of 132?   \n",
            "6   6  word_problems  John is 10 years old now. How old was he 15 ye...   \n",
            "7   7      fractions                  What is 1/5 + 2/5? (decimal form)   \n",
            "8   8     percentage                                What is 27% of 164?   \n",
            "9   9      fractions                  What is 2/4 + 1/4? (decimal form)   \n",
            "\n",
            "   solution  \n",
            "0    136.25  \n",
            "1    131.00  \n",
            "2    158.00  \n",
            "3     87.92  \n",
            "4     27.00  \n",
            "5      9.24  \n",
            "6     -5.00  \n",
            "7      0.60  \n",
            "8     44.28  \n",
            "9      0.75  \n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "# Display category distribution\n",
        "print(\"\\nTraining set category distribution:\")\n",
        "print(train_data['category'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nSample training problems:\")\n",
        "print(train_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## Part 3: Baseline - Dummy Model\n",
        "\n",
        "Create a baseline to understand what poor performance looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-8",
        "outputId": "83264b00-9bbe-4a7d-e4bb-356b56abd3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy model (always predicts mean): 150.79\n",
            "This demonstrates very poor performance. Your model should do much better!\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(predictions, ground_truth, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Calculate accuracy with tolerance for floating point comparisons.\n",
        "\n",
        "    Two values are considered equal if their difference is <= tolerance\n",
        "    OR if they round to the same value at 2 decimal places.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for pred, truth in zip(predictions, ground_truth):\n",
        "        # Check if both round to same 2 decimal places\n",
        "        if round(pred, 2) == round(truth, 2):\n",
        "            correct += 1\n",
        "        # Or if absolute difference is very small\n",
        "        elif abs(pred - truth) <= tolerance:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(predictions)\n",
        "\n",
        "# Dummy baseline: always predict the mean\n",
        "mean_solution = train_data['solution'].mean()\n",
        "print(f\"Dummy model (always predicts mean): {mean_solution:.2f}\")\n",
        "print(\"This demonstrates very poor performance. Your model should do much better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## Part 4: Utility Functions\n",
        "\n",
        "Helper functions to extract numerical answers from model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-10",
        "outputId": "9735fee6-1829-4a6d-f495-409e08026b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number extraction tests:\n",
            "  'The answer is 42' -> 42.0\n",
            "  '42' -> 42.0\n",
            "  '15 + 27 = 42' -> 42.0\n",
            "  'Calculating... the result is 42.5!' -> 42.5\n",
            "  'No number here' -> None\n",
            "  'The value is -15' -> -15.0\n",
            "  '...and the final answer is 123.45' -> 123.45\n"
          ]
        }
      ],
      "source": [
        "def extract_number(text):\n",
        "    \"\"\"\n",
        "    Extract the first number from text. Return None if no number found.\n",
        "\n",
        "    Handles various formats:\n",
        "    - \"The answer is 42\"\n",
        "    - \"42\"\n",
        "    - \"= 42\"\n",
        "    - \"Result: 42.5\"\n",
        "    - Negative numbers: \"-15\"\n",
        "    \"\"\"\n",
        "    # Try different patterns in order of specificity\n",
        "    patterns = [\n",
        "        # Look for the final answer structure often used in COT: \\n\\n... The answer is X\n",
        "        r'(?:final\\s+answer|answer|result|equals?|=)\\s*:?\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'(-?\\d+\\.?\\d*)\\s*$',  # Number at the end\n",
        "        r'(-?\\d+\\.?\\d*)',  # Any number\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            try:\n",
        "                return float(match.group(1))\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# Test extraction\n",
        "test_strings = [\n",
        "    \"The answer is 42\",\n",
        "    \"42\",\n",
        "    \"15 + 27 = 42\",\n",
        "    \"Calculating... the result is 42.5!\",\n",
        "    \"No number here\",\n",
        "    \"The value is -15\",\n",
        "    \"...and the final answer is 123.45\"\n",
        "]\n",
        "\n",
        "print(\"Number extraction tests:\")\n",
        "for s in test_strings:\n",
        "    result = extract_number(s)\n",
        "    print(f\"  '{s}' -> {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Part 5: Load Pre-trained Model\n",
        "\n",
        "Load a small, efficient model for math problem solving. **Note: For best results (>= 70% accuracy), a larger model like TinyLlama or Phi-2 is recommended over GPT-2, which is used here for faster testing on CPU.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "outputs": [],
      "source": [
        "# Set model_name to a model capable of reasoning. We use GPT-2 as a fast placeholder.\n",
        "# If possible, change to: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" or \"microsoft/phi-2\"\n",
        "# model_name = \"GPT-2\"  # Placeholder for fast excution\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Placeholder for better accuracy\n",
        "\n",
        "print(f\"Loading model: {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set padding token (necessary for batch generation)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Part 6: Prompting Strategies\n",
        "\n",
        "Test different prompt templates to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cell-14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-14",
        "outputId": "032cb724-84c3-48cf-a842-5ae4b7557995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing problem: Increase 109 by 25%\n",
            "Correct answer: 136.25\n",
            "\n",
            "======================================================================\n",
            "✗ simple:\n",
            "  Response: 124\n",
            "\n",
            "Question 3: What is the difference between the average and median of a set of numbers?\n",
            "\n",
            "Answers...\n",
            "  Extracted: 124.0\n",
            "\n",
            "✗ instruction:\n",
            "  Response: 134\n",
            "\n",
            "Example: How many more than 109 is 134?\n",
            "Solution: 134 is greater than 109 by 25%.\n",
            "\n",
            "Example: How...\n",
            "  Extracted: 1.0\n",
            "\n",
            "✗ cot:\n",
            "  Response: 1. Multiply 109 by 0.8\n",
            "2. Subtract 25% from the result\n",
            "3. Multiply the result by 10\n",
            "4. Divide the re...\n",
            "  Extracted: 3.0\n",
            "\n",
            "✗ few_shot:\n",
            "  Response: 136.25\n",
            "\n",
            "Problem: What is 76 + 55?\n",
            "Answer: 131.0\n",
            "\n",
            "Problem: Sarah has $286. She spends $128. How much ...\n",
            "  Extracted: 131.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_answer(problem, prompt_template=\"simple\", max_new_tokens=100, temperature=0.2):\n",
        "    \"\"\"\n",
        "    Generate answer using different prompt templates.\n",
        "    \"\"\"\n",
        "    if prompt_template == \"simple\":\n",
        "        prompt = f\"{problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"instruction\":\n",
        "        prompt = f\"Solve this math problem precisely and provide only the final numerical answer.\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"cot\":\n",
        "        # Chain-of-Thought: forces model to reason step-by-step\n",
        "        prompt = f\"Solve this math problem step by step, showing all your work and reasoning clearly. Conclude your response with the final numerical answer on a new line, labeled 'Final Answer:'.\\n\\nProblem: {problem}\\nSolution:\\n\"\n",
        "        # Increase max_new_tokens for COT\n",
        "        max_new_tokens = 150\n",
        "\n",
        "    elif prompt_template == \"few_shot\":\n",
        "        # Few-Shot: Includes examples from training data (simple format)\n",
        "        examples = []\n",
        "        # Use a few examples from the head of the training data\n",
        "        for i in range(min(5, len(train_data))):\n",
        "            examples.append(f\"Problem: {train_data['problem'].iloc[i]}\\nAnswer: {train_data['solution'].iloc[i]}\")\n",
        "\n",
        "        examples_text = \"\\n\\n\".join(examples)\n",
        "        prompt = f\"{examples_text}\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    else:\n",
        "        prompt = problem\n",
        "\n",
        "    # Generate\n",
        "    # Use a maximum length that accommodates the prompt and the expected response length\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt from response\n",
        "    # We need to cut off the input text and only keep the generation\n",
        "    response = response[len(prompt):].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Test different prompts on a sample problem\n",
        "test_problem = train_data['problem'].iloc[0]\n",
        "test_solution = train_data['solution'].iloc[0]\n",
        "\n",
        "print(f\"Testing problem: {test_problem}\")\n",
        "print(f\"Correct answer: {test_solution}\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Note: GPT-2 is weak. Expect poor results here, even with good prompts.\n",
        "for template in [\"simple\", \"instruction\", \"cot\", \"few_shot\"]:\n",
        "    response = generate_answer(test_problem, template)\n",
        "    extracted = extract_number(response)\n",
        "\n",
        "    correct = \"✓\" if extracted is not None and round(extracted, 2) == round(test_solution, 2) else \"✗\"\n",
        "\n",
        "    print(f\"{correct} {template}:\")\n",
        "    print(f\"  Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
        "    print(f\"  Extracted: {extracted}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## Part 7: Evaluate on Validation Set\n",
        "\n",
        "Test your best prompting strategy on a subset of training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cell-16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-16",
        "outputId": "b0f568ba-0704-456f-d03c-acc32e49b58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on 50 validation problems...\n",
            "\n",
            "Processed 10/50 problems...\n",
            "Processed 20/50 problems...\n",
            "Processed 30/50 problems...\n",
            "Processed 40/50 problems...\n",
            "Processed 50/50 problems...\n",
            "\n",
            "Validation Accuracy: 6.00%\n",
            "Need to achieve: 70% on test set\n"
          ]
        }
      ],
      "source": [
        "# The Chain-of-Thought (COT) strategy is generally the most effective for reasoning tasks.\n",
        "# Set best_template = \"cot\" for best results with a capable model.\n",
        "best_template = \"cot\"\n",
        "\n",
        "# Evaluate on a small validation set (last 50 training examples)\n",
        "val_data = train_data.tail(50)\n",
        "\n",
        "predictions = []\n",
        "ground_truth = val_data['solution'].tolist()\n",
        "\n",
        "print(f\"Evaluating on {len(val_data)} validation problems...\\n\")\n",
        "\n",
        "for idx, row in val_data.iterrows():\n",
        "    problem = row['problem']\n",
        "    solution = row['solution']\n",
        "\n",
        "    # Use the best template (COT)\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    # If no number extracted, use 0 (will be wrong)\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    if (len(predictions) % 10) == 0:\n",
        "        print(f\"Processed {len(predictions)}/{len(val_data)} problems...\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = check_accuracy(predictions, ground_truth)\n",
        "print(f\"\\nValidation Accuracy: {accuracy:.2%}\")\n",
        "print(f\"Need to achieve: 70% on test set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Part 8: Generate Test Predictions\n",
        "\n",
        "Generate predictions for the test set and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cell-18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-18",
        "outputId": "154b2232-8f37-4690-fa78-8ef9ad3de10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating predictions on 100 test problems...\n",
            "\n",
            "Processed 10/100 problems...\n",
            "Processed 20/100 problems...\n",
            "Processed 30/100 problems...\n",
            "Processed 40/100 problems...\n",
            "Processed 50/100 problems...\n",
            "Processed 60/100 problems...\n",
            "Processed 70/100 problems...\n",
            "Processed 80/100 problems...\n",
            "Processed 90/100 problems...\n",
            "Processed 100/100 problems...\n",
            "\n",
            "All test predictions generated!\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions on test set using the chosen best template\n",
        "print(f\"Generating predictions on {len(test_data)} test problems...\\n\")\n",
        "\n",
        "test_predictions = []\n",
        "\n",
        "for idx, row in test_data.iterrows():\n",
        "    problem = row['problem']\n",
        "\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    # If no number extracted, use 0\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "        print(f\"⚠️  Warning: No number extracted for problem {idx}: {problem[:50]}...\")\n",
        "\n",
        "    test_predictions.append(prediction)\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(test_data)} problems...\")\n",
        "\n",
        "print(\"\\nAll test predictions generated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "## Part 9: Create Submission File\n",
        "\n",
        "Save predictions in the required format for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "cell-20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-20",
        "outputId": "4bc5fc98-38c0-4495-b2cd-29c190301a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file created: submission.csv\n",
            "\n",
            "Submission preview:\n",
            "   id    solution\n",
            "0   0   11.900000\n",
            "1   1    1.000000\n",
            "2   2  204.000000\n",
            "3   3   91.000000\n",
            "4   4   10.200000\n",
            "5   5    7.000000\n",
            "6   6    6.666667\n",
            "7   7  628.000000\n",
            "8   8    8.000000\n",
            "9   9    0.800000\n",
            "\n",
            "✓ All predictions are numerical\n",
            "\n",
            "Prediction statistics:\n",
            "count     100.000000\n",
            "mean       99.372624\n",
            "std       406.691472\n",
            "min       -62.000000\n",
            "25%         1.000000\n",
            "50%         7.500000\n",
            "75%        19.562500\n",
            "max      3600.000000\n",
            "Name: solution, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'solution': test_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: submission.csv\")\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Verify all predictions are numerical\n",
        "non_numeric = submission['solution'].isna().sum()\n",
        "if non_numeric > 0:\n",
        "    print(f\"\\n⚠️  WARNING: {non_numeric} predictions are not numerical!\")\n",
        "    print(\"These will result in incorrect answers. Please fix them.\")\n",
        "else:\n",
        "    print(\"\\n✓ All predictions are numerical\")\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\nPrediction statistics:\")\n",
        "print(submission['solution'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "## Part 10 (Optional): Fine-Tuning with LoRA\n",
        "\n",
        "If prompting doesn't achieve 70% accuracy, consider fine-tuning with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cell-22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-22",
        "outputId": "51f9c12f-a70e-46eb-dcd5-840c363c5705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA fine-tuning is optional.\n",
            "Use this if prompting strategies don't achieve 70% accuracy.\n",
            "\n",
            "Consider:\n",
            "- Prepare training dataset in correct format (e.g., 'Problem: X\n",
            "Solution: Y')\n",
            "- Configure LoRA parameters (r=8, alpha=32)\n",
            "- Train for a few epochs\n",
            "- Evaluate and compare with prompting approaches\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement LoRA fine-tuning (OPTIONAL)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# The following code is a basic framework for LoRA, to be used if Part 7 fails to reach the target accuracy.\n",
        "\n",
        "def create_lora_model(base_model):\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,  # LoRA attention dimension\n",
        "        lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],  # Target attention layers for GPT-2\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "    return get_peft_model(base_model, lora_config)\n",
        "\n",
        "print(\"LoRA fine-tuning is optional.\")\n",
        "print(\"Use this if prompting strategies don't achieve 70% accuracy.\")\n",
        "print(\"\\nConsider:\")\n",
        "print(\"- Prepare training dataset in correct format (e.g., 'Problem: X\\nSolution: Y')\")\n",
        "print(\"- Configure LoRA parameters (r=8, alpha=32)\")\n",
        "print(\"- Train for a few epochs\")\n",
        "print(\"- Evaluate and compare with prompting approaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {
        "id": "cell-23"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. **Which prompting strategy worked best and why?**\n",
        "  - The **Chain-of-Thought (COT)** strategy is expected to work best. This is because mathematical reasoning tasks require the model to perform multi-step logical operations. COT significantly improves the model's reliability and accuracy in complex calculations and logic by forcing it to **display the step-by-step solution process** (e.g., `Problem: X \\nSolution: ...`), simulating a human thought process.\n",
        "\n",
        "2. **What types of math problems were most challenging for the model?**\n",
        "  - **Multi-Step Complex Calculations (Multi-Step Arithmetic)**: Models are prone to operational errors (i.e., \"numerical hallucinations\") when performing long sequences or multi-level arithmetic.\n",
        "  - **Word Traps and Unit Conversion**: Problems that require extracting key numbers and relationships from long descriptions, or involve unit conversions (such as time, currency, or measurements), challenge the model's understanding and memory capacity.\n",
        "  - **Algebraic or Symbolic Manipulation**: Problems involving variables or complex equations, rather than direct numerical operations, typically result in higher error rates.\n",
        "\n",
        "3. **How did you handle number extraction from model outputs?**\n",
        "  - An auxiliary function named `extract_number` was used, which utilizes **Regular Expressions (RegEx)** to extract the numerical answer.\n",
        "  - To accommodate different prompting strategies (especially COT), the regular expression prioritizes patterns containing keywords like **`final answer`**, **`answer`**, or **`result`** to ensure the extraction of the final solution, not intermediate calculation numbers.\n",
        "  - Finally, a pattern searching for a standalone number at the end of the text is included, in case the model only outputs the number.\n",
        "\n",
        "4. **What are the limitations of using LLMs for mathematical reasoning?**\n",
        "  - **Lack of Formal Reasoning and Arithmetic Flaws**: LLMs are essentially based on language pattern prediction; they do not perform precise arithmetic like a calculator, making them susceptible to errors in multiplication, division, or long number operations.\n",
        "  - **Confusion of Knowledge vs. Logic**: Models may memorize common mathematical facts but struggle to apply them to novel, abstract, or logically rigorous problems.\n",
        "  - **Sensitivity to Prompting**: Results are highly sensitive to subtle variations in the prompt, necessitating continuous tuning to find the best performance.\n",
        "\n",
        "5. **If you used LoRA fine-tuning, what were the trade-offs compared to prompting?**\n",
        "  - **Trade-offs**: LoRA fine-tuning generally achieves **higher final accuracy**, especially when the dataset distribution differs from the base model's pre-training data. However, the **disadvantages** are: it requires additional **training time and computational resources**, and necessitates careful preparation of formatted **training data**; whereas prompting strategies (like COT) can be **deployed instantly** without extra training but have a ceiling limit on accuracy.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

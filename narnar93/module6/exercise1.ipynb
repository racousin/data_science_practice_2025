{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd09113-2f55-43ea-914c-ab3c0e3ee3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge, RidgeClassifier\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, BaggingRegressor, StackingRegressor, StackingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.base import RegressorMixin, ClassifierMixin, BaseEstimator\n",
        "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "21344022-d9b6-4037-a5f3-ae6d0bf35d84",
      "metadata": {},
      "source": [
        "pip install xgboost"
      ]
    },
    {
      "cell_type": "raw",
      "id": "c4890946-0919-4326-9f3c-59f8bc399be2",
      "metadata": {},
      "source": [
        "pip install lightgbm "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80dc69a1-e21d-4310-a0a4-5cb5b7d69fb4",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae3aeb5-dcdf-4690-b869-d211a02f303c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module6/exercise/module6_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module6/exercise/module6_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module6_exercise_train.csv')\n",
        "download_file(test_data_url, 'module6_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ae6bb0-639c-472a-8748-4bbbdb96e142",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('module6_exercise_train.csv', index_col='index')\n",
        "data_test = pd.read_csv('module6_exercise_test.csv', index_col='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea77dab9-6dcc-42bc-894b-1d8a23774d33",
      "metadata": {},
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92c5579-a099-4018-a22e-06513a885133",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ead6e347-8a41-489d-a184-98d4259ff9be",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d62d9ab-50b9-4d5b-9611-adbd10840ae5",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b5babb7-ad30-43eb-b3ec-4eebca726764",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e5dd21-1d73-415c-a4a7-c1c869ded46b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314189f7-31bb-47a9-afca-97d9ec489b60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the distribution using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data_train['end_of_day_return'], bins=50, kde=True)\n",
        "plt.title('Distribution of End of Day Return')\n",
        "plt.xlabel('End of Day Return')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab85a4fe-b7d4-47f7-9d4a-4a155c44e24c",
      "metadata": {},
      "source": [
        "### Model Building and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a86f0e4-55fa-46a9-8f53-735cf62aa6ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "y = data_train.pop('end_of_day_return')\n",
        "X = data_train.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746ed3ab-0de5-4c35-94aa-76251187dede",
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_accuracy(y_true, y_pred):\n",
        "    weights = np.abs(y_true)\n",
        "    \n",
        "    # Compute the sign of true and predicted values\n",
        "    sign_true = np.sign(y_true)\n",
        "    sign_pred = np.sign(y_pred)\n",
        "    \n",
        "    # Correct predictions where the sign of the true and predicted values match\n",
        "    correct_predictions = sign_true == sign_pred\n",
        "    \n",
        "    # Compute the weighted accuracy\n",
        "    weighted_acc = np.sum(weights * correct_predictions) / np.sum(weights)\n",
        "    \n",
        "    return weighted_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04424025-b909-46d3-af2d-0e7d6fb9107b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot the evaluation results\n",
        "def plot_results(mse_train, mse_test, w_acc_train, w_acc_test):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # MSE plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(mse_train, label=\"Train MSE\", marker='o')\n",
        "    plt.plot(mse_test, label=\"Test MSE\", marker='o')\n",
        "    plt.fill_between(range(len(mse_train)), np.min(mse_train), np.max(mse_train), color='blue', alpha=0.1)\n",
        "    plt.fill_between(range(len(mse_test)), np.min(mse_test), np.max(mse_test), color='orange', alpha=0.1)\n",
        "    plt.title(\"MSE over Folds\")\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # weighted_accuracy plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(w_acc_train, label=\"Train weighted_accuracy\", marker='o')\n",
        "    plt.plot(w_acc_test, label=\"Test weighted_accuracy\", marker='o')\n",
        "    plt.fill_between(range(len(w_acc_train)), np.min(w_acc_train), np.max(w_acc_train), color='blue', alpha=0.1)\n",
        "    plt.fill_between(range(len(w_acc_test)), np.min(w_acc_test), np.max(w_acc_test), color='orange', alpha=0.1)\n",
        "    plt.title(\"weighted_accuracy over Folds\")\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"weighted_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_multi_model_results(results):\n",
        "    # Set up the plot\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\n",
        "    \n",
        "    # Colors for train and test\n",
        "    train_color = 'skyblue'\n",
        "    test_color = 'lightgreen'\n",
        "    \n",
        "    # Plot MSE\n",
        "    ax1.set_title('Mean Squared Error (MSE) Comparison', fontsize=16)\n",
        "    ax1.set_ylabel('MSE', fontsize=12)\n",
        "    ax1.set_xlabel('Models', fontsize=12)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Plot weighted_accuracy\n",
        "    ax2.set_title('weighted_accuracy Comparison', fontsize=16)\n",
        "    ax2.set_ylabel('weighted_accuracy', fontsize=12)\n",
        "    ax2.set_xlabel('Models', fontsize=12)\n",
        "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
        "    \n",
        "    x = np.arange(len(results))\n",
        "    width = 0.35\n",
        "    \n",
        "    for i, (model_name, scores) in enumerate(results.items()):\n",
        "        # MSE\n",
        "        mse_train = scores['mse_train']\n",
        "        mse_test = scores['mse_test']\n",
        "        \n",
        "        ax1.bar(x[i] - width/2, np.mean(mse_train), width, label='Train' if i == 0 else \"\", \n",
        "                color=train_color, alpha=0.7)\n",
        "        ax1.bar(x[i] + width/2, np.mean(mse_test), width, label='Test' if i == 0 else \"\", \n",
        "                color=test_color, alpha=0.7)\n",
        "        \n",
        "        ax1.errorbar(x[i] - width/2, np.mean(mse_train), \n",
        "                     yerr=[[np.mean(mse_train)-np.min(mse_train)], [np.max(mse_train)-np.mean(mse_train)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        ax1.errorbar(x[i] + width/2, np.mean(mse_test), \n",
        "                     yerr=[[np.mean(mse_test)-np.min(mse_test)], [np.max(mse_test)-np.mean(mse_test)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        \n",
        "        # weighted_accuracy\n",
        "        w_acc_train = scores['w_acc_train']\n",
        "        w_acc_test = scores['w_acc_test']\n",
        "        \n",
        "        ax2.bar(x[i] - width/2, np.mean(w_acc_train), width, label='Train' if i == 0 else \"\", \n",
        "                color=train_color, alpha=0.7)\n",
        "        ax2.bar(x[i] + width/2, np.mean(w_acc_test), width, label='Test' if i == 0 else \"\", \n",
        "                color=test_color, alpha=0.7)\n",
        "        \n",
        "        ax2.errorbar(x[i] - width/2, np.mean(w_acc_train), \n",
        "                     yerr=[[np.mean(w_acc_train)-np.min(w_acc_train)], [np.max(w_acc_train)-np.mean(w_acc_train)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "        ax2.errorbar(x[i] + width/2, np.mean(w_acc_test), \n",
        "                     yerr=[[np.mean(w_acc_test)-np.min(w_acc_test)], [np.max(w_acc_test)-np.mean(w_acc_test)]], \n",
        "                     fmt='none', ecolor='black', capsize=5)\n",
        "    \n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "    \n",
        "    ax1.legend(loc='upper left')\n",
        "    ax2.legend(loc='upper left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c73c55-9684-411c-9ad3-ba6f9ba7de8d",
      "metadata": {},
      "source": [
        "#### Simple Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6a7f41-1184-4172-ab8b-220f45ab2172",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to handle train-test evaluation in a fold\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, model):\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions on train set\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    # Make predictions on train set\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Compute MSE for train and test\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    \n",
        "    # Compute weighted_accuracy\n",
        "    \n",
        "    w_acc_train = weighted_accuracy(y_train, y_pred_train)\n",
        "    w_acc_test = weighted_accuracy(y_test, y_pred_test)\n",
        "    \n",
        "    return mse_train, mse_test, w_acc_train, w_acc_test\n",
        "\n",
        "\n",
        "def run_multi_model_cv(X, y, models, n_splits=5):\n",
        "    fold = KFold(n_splits=n_splits)\n",
        "    results = {name: {'mse_train': [], 'mse_test': [], 'w_acc_train': [], 'w_acc_test': []} \n",
        "               for name in models.keys()}\n",
        "    \n",
        "    for train_index, test_index in fold.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
        "        y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
        "        \n",
        "        for name, model in models.items():\n",
        "            mse_train, mse_test, w_acc_train, w_acc_test = train_and_evaluate(\n",
        "                X_train, X_test, y_train, y_test, model\n",
        "            )\n",
        "            results[name]['mse_train'].append(mse_train)\n",
        "            results[name]['mse_test'].append(mse_test)\n",
        "            results[name]['w_acc_train'].append(w_acc_train)\n",
        "            results[name]['w_acc_test'].append(w_acc_test)\n",
        "        # Find the model with the best mean w_acc test score\n",
        "    best_mean_w_acc = -1\n",
        "    best_model = None\n",
        "    best_min_w_acc = None\n",
        "    best_max_w_acc = None\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        w_acc_test_scores = result['w_acc_test']\n",
        "        mean_w_acc_test = sum(w_acc_test_scores) / len(w_acc_test_scores)  # Calculate mean w_acc score\n",
        "        min_w_acc_test = min(w_acc_test_scores)  # Minimum w_acc score\n",
        "        max_w_acc_test = max(w_acc_test_scores)  # Maximum w_acc score\n",
        "        \n",
        "        if mean_w_acc_test > best_mean_w_acc:\n",
        "            best_mean_w_acc = mean_w_acc_test\n",
        "            best_min_w_acc = min_w_acc_test\n",
        "            best_max_w_acc = max_w_acc_test\n",
        "            best_model = name\n",
        "    \n",
        "    # Print the best mean w_acc test score, min, max, and the associated model\n",
        "    print(f\"Best mean w_acc test score: {best_mean_w_acc:.4f} by model: {best_model}\")\n",
        "    print(f\"Min w_acc test score: {best_min_w_acc:.4f}, Max w_acc test score: {best_max_w_acc:.4f}\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891e158b-b199-41ac-a244-537429eb1d22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Run cross-validation\n",
        "results = run_multi_model_cv(X, y, {\"RandomForestRegressor\": RandomForestRegressor(n_jobs=-1)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b0380d-c7ee-413e-9b13-756f8e7e3847",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Plot the results\n",
        "plot_results(results[\"RandomForestRegressor\"][\"mse_train\"],\n",
        "             results[\"RandomForestRegressor\"][\"mse_test\"],\n",
        "             results[\"RandomForestRegressor\"][\"w_acc_train\"],\n",
        "             results[\"RandomForestRegressor\"][\"w_acc_test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc76e0a4-6af0-43eb-af13-0bad0adbbdbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Ridge': Ridge(),\n",
        "    'Decision Tree Regressor': RandomForestRegressor(n_jobs=-1)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264692a5-ce69-4567-a108-669d0cfcc3a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250f1f68-32b7-47d3-8934-952785a3046f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e21e12-8f5d-4fe5-beb0-566d8a971ae7",
      "metadata": {},
      "source": [
        "#### Manage properly the objective weighted_accuracy\n",
        "should we create different classes? custom loss?\n",
        "\n",
        "Create Compare and Optimize different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f62ac3-7e72-4022-8135-0ffe059aad8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "#Y2=y>0\n",
        "# convertir Y en un varaible binaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10578319-6090-47d5-acac-851f5625be77",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#models = {\n",
        "    #'Ridge': Ridge(),\n",
        "    #'Decision Tree Regressor': DecisionTreeRegressor(),\n",
        "    #'Random Forest Regressor': RandomForestRegressor(),\n",
        "    #'SVR': SVR(),\n",
        "    #'Lasso': Lasso(max_iter=5000),\n",
        "    #'KNN Regressor': KNeighborsRegressor(),\n",
        "    #'Logistic Regression': LogisticRegression(),\n",
        "    #'Decision Tree Classifier': DecisionTreeClassifier(),\n",
        "    #'Random Forest Classifier': RandomForestClassifier(),\n",
        "    #'SVC': SVC(),\n",
        "    #'KNN Classifier': KNeighborsClassifier(),\n",
        "    #'XGBRegressor': XGBRegressor(),\n",
        "    #'LGBMRegressor': LGBMRegressor(verbose=-1)\n",
        "#}\n",
        "\n",
        "# Regression\n",
        "models = {\n",
        "    'Ridge': Ridge(),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
        "    'Random Forest Regressor': RandomForestRegressor(),\n",
        "    'SVR': SVR(),\n",
        "    'Lasso': Lasso(max_iter=5000),\n",
        "    'KNN Regressor': KNeighborsRegressor(),\n",
        "    'XGBRegressor': XGBRegressor(),\n",
        "    'LGBMRegressor': LGBMRegressor(verbose=-1)\n",
        "}\n",
        "\n",
        "\n",
        "#classification\n",
        "modeles_bin = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Decision Tree Classifier': DecisionTreeClassifier(),\n",
        "    'Random Forest Classifier': RandomForestClassifier(),\n",
        "    'SVC': SVC(),\n",
        "    'KNN Classifier': KNeighborsClassifier(),\n",
        "    'XGBClassifier': XGBClassifier(),\n",
        "    'LGBMClassifier': LGBMClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4022e9-1ede-436f-9120-f3b1dcf9c249",
      "metadata": {},
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802a8515-044e-4055-aed5-290c6c8ebb17",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c9f0a5-e4db-4fd7-9ab3-1720893b440c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07850d34-d5c2-499e-9cab-90f2fa0daa86",
      "metadata": {},
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fa8231-608a-4f7b-969a-0f1fa34a2886",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "\n",
        "Y2=(y>0).astype(int)\n",
        "results = run_multi_model_cv(X, Y2, modeles_bin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb22bea-f7e3-4a42-9345-5e3728982a3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models binaire\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "a9b8f241-4a20-4fa0-ac0d-aa8bdd95b64e",
      "metadata": {},
      "source": [
        "# Define the search spaces for each model\n",
        "spaces = {\n",
        "    'RandomForestRegressor': {\n",
        "        'n_estimators': Integer(10, 500),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 20)\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 2.0, 'log-uniform'),\n",
        "        'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "        'colsample_bytree': Real(0.5, 1.0, 'uniform')\n",
        "    },\n",
        "    'LGBMRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
        "        'num_leaves': Integer(20, 300),\n",
        "        'min_child_samples': Integer(1, 100)\n",
        "    },\n",
        "    'SVR': {\n",
        "        'C': Real(0.1, 10.0, 'log-uniform'),\n",
        "        'epsilon': Real(0.001, 1.0, 'log-uniform'),\n",
        "        'kernel': Categorical(['linear', 'rbf', 'poly'])\n",
        "    },\n",
        "    'KNNRegressor': {\n",
        "        'n_neighbors': Integer(1, 50),\n",
        "        'weights': Categorical(['uniform', 'distance']),\n",
        "        'p': Integer(1, 2)\n",
        "    },\n",
        "    'Lasso': {\n",
        "        'alpha': Real(0.0001, 10.0, 'log-uniform')\n",
        "    },\n",
        "    'Ridge': {\n",
        "        'alpha': Real(0.01, 10.0, 'log-uniform'),\n",
        "    },\n",
        "    'LogisticRegression': {\n",
        "        'C': Real(0.01, 10.0, 'log-uniform'),\n",
        "        'solver': Categorical(['lbfgs', 'liblinear'])\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'n_estimators': Integer(10, 500),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 20)\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 2.0, 'log-uniform'),\n",
        "        'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "        'colsample_bytree': Real(0.5, 1.0, 'uniform')\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "raw",
      "id": "38e0855a-61d0-4630-bce3-eec4ac3edd0f",
      "metadata": {},
      "source": [
        "space1 = {\n",
        "    'RandomForestRegressor': {\n",
        "        'n_estimators': Integer(10, 500),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 20)\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 2.0, 'log-uniform'),\n",
        "        'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "        'colsample_bytree': Real(0.5, 1.0, 'uniform')\n",
        "    },\n",
        "    'LGBMRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
        "        'num_leaves': Integer(20, 300),\n",
        "        'min_child_samples': Integer(1, 100)\n",
        "    },\n",
        "    'SVR': {\n",
        "        'C': Real(0.1, 10.0, 'log-uniform'),\n",
        "        'epsilon': Real(0.001, 1.0, 'log-uniform'),\n",
        "        'kernel': Categorical(['linear', 'rbf', 'poly'])\n",
        "    },\n",
        "    'KNNRegressor': {\n",
        "        'n_neighbors': Integer(1, 50),\n",
        "        'weights': Categorical(['uniform', 'distance']),\n",
        "        'p': Integer(1, 2)\n",
        "    },\n",
        "    'Lasso': {\n",
        "        'alpha': Real(0.0001, 10.0, 'log-uniform')\n",
        "    },\n",
        "    'Ridge': {\n",
        "        'alpha': Real(0.01, 10.0, 'log-uniform'),\n",
        "    }\n",
        "}\n",
        "\n",
        "space_bin = {\n",
        "    'LogisticRegression': {\n",
        "        'C': Real(0.01, 10.0, 'log-uniform'),\n",
        "        'solver': Categorical(['lbfgs', 'liblinear'])\n",
        "    },\n",
        "    'RandomForestClassifier': {\n",
        "        'n_estimators': Integer(10, 500),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 20)\n",
        "    },\n",
        "    'XGBClassifier': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 2.0, 'log-uniform'),\n",
        "        'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "        'colsample_bytree': Real(0.5, 1.0, 'uniform')\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "02dcb361-d3f9-4790-8eba-87f619515e6e",
      "metadata": {},
      "source": [
        "pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4dcc43-514f-4614-afbf-8e670e1bb05d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize models\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer, Real, Categorical\n",
        "from skopt.callbacks import DeltaYStopper\n",
        "\n",
        "# Define the search spaces for each model\n",
        "spaces =  {\n",
        "    'RandomForestRegressor': {\n",
        "        'n_estimators': Integer(10, 300),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'min_samples_split': Integer(2, 20),\n",
        "        'min_samples_leaf': Integer(1, 20)\n",
        "    },\n",
        "    'XGBRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 2.0, 'log-uniform'),\n",
        "        'subsample': Real(0.5, 1.0, 'uniform'),\n",
        "        'colsample_bytree': Real(0.5, 1.0, 'uniform')\n",
        "    },\n",
        "    'LGBMRegressor': {\n",
        "        'n_estimators': Integer(10, 200),\n",
        "        'max_depth': Integer(1, 50),\n",
        "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
        "        'num_leaves': Integer(20, 300),\n",
        "        'min_child_samples': Integer(1, 100)\n",
        "    },\n",
        "    'SVR': {\n",
        "        'C': Real(0.1, 10.0, 'log-uniform'),\n",
        "        'epsilon': Real(0.001, 1.0, 'log-uniform'),\n",
        "        'kernel': Categorical(['linear', 'rbf', 'poly'])\n",
        "    },\n",
        "    'KNNRegressor': {\n",
        "        'n_neighbors': Integer(1, 50),\n",
        "        'weights': Categorical(['uniform', 'distance']),\n",
        "        'p': Integer(1, 2)\n",
        "    },\n",
        "    'Lasso': {\n",
        "        'alpha': Real(0.0001, 10.0, 'log-uniform')\n",
        "    },\n",
        "    'Ridge': {\n",
        "        'alpha': Real(0.01, 10.0, 'log-uniform'),\n",
        "    }\n",
        "}\n",
        "\n",
        "def optimizer_callback(res):\n",
        "    if len(res.func_vals) % 5 == 0:  # Print every 5 iterations\n",
        "        print(f\"Iteration {len(res.func_vals)}: Best score = {-res.fun:.4f}\")\n",
        "\n",
        "delta_stopper = DeltaYStopper(delta=0.001, n_best=10)\n",
        "\n",
        "# Function to optimize models\n",
        "def optimize_model(X, y, model, space, n_iter=100):\n",
        "    sfold = KFold(n_splits=5)\n",
        "\n",
        "    scorer = make_scorer(weighted_accuracy, greater_is_better=True)\n",
        "\n",
        "    opt = BayesSearchCV(\n",
        "        model,\n",
        "        space,\n",
        "        n_iter=n_iter,\n",
        "        n_points=5,\n",
        "        cv=sfold,\n",
        "        n_jobs=-1,\n",
        "        scoring=scorer,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    opt.fit(X, y, callback=[optimizer_callback, delta_stopper])\n",
        "\n",
        "    return opt\n",
        "\n",
        "# Optimize models\n",
        "models = {\n",
        "    'RandomForestRegressor': RandomForestRegressor(),\n",
        "    'XGBRegressor': XGBRegressor(),\n",
        "    'LGBMRegressor': LGBMRegressor(verbose=-1),\n",
        "    'SVR': SVR(),\n",
        "    'KNNRegressor': KNeighborsRegressor(),\n",
        "    'Lasso': Lasso(max_iter=5000),\n",
        "    'Ridge': Ridge(),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0096fc-2da4-43d4-b605-a7c8855a013a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "models_opt = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Optimizing {name}...\")\n",
        "    opt = optimize_model(X, y, model, spaces[name])\n",
        "    models_opt[name] = opt\n",
        "    print(f\"Best parameters: {opt.best_params_}\")\n",
        "    print(f\"Best score: {opt.best_score_:.4f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "906862b5-5674-4932-b4a6-9bd8b9d8a904",
      "metadata": {},
      "source": [
        "Optimizing RandomForestRegressor...\n",
        "Iteration 5: Best score = 0.5222\n",
        "Iteration 10: Best score = 0.5319\n",
        "Iteration 15: Best score = 0.5319\n",
        "Iteration 20: Best score = 0.5319\n",
        "Iteration 25: Best score = 0.5319\n",
        "Iteration 30: Best score = 0.5319\n",
        "Iteration 35: Best score = 0.5319\n",
        "Iteration 40: Best score = 0.5335\n",
        "Iteration 45: Best score = 0.5335\n",
        "Iteration 50: Best score = 0.5335\n",
        "Iteration 55: Best score = 0.5335\n",
        "Iteration 60: Best score = 0.5335\n",
        "Iteration 65: Best score = 0.5335\n",
        "Iteration 70: Best score = 0.5380\n",
        "Iteration 75: Best score = 0.5381\n",
        "Iteration 80: Best score = 0.5381\n",
        "Iteration 85: Best score = 0.5381\n",
        "Iteration 90: Best score = 0.5381\n",
        "Iteration 95: Best score = 0.5381\n",
        "Iteration 100: Best score = 0.5381\n",
        "Best parameters: OrderedDict({'max_depth': 46, 'min_samples_leaf': 7, 'min_samples_split': 4, 'n_estimators': 37})\n",
        "Best score: 0.5381\n",
        "\n",
        "Optimizing XGBRegressor...\n",
        "Iteration 5: Best score = 0.5288\n",
        "Iteration 10: Best score = 0.5288\n",
        "Iteration 15: Best score = 0.5317\n",
        "Iteration 20: Best score = 0.5317\n",
        "Iteration 25: Best score = 0.5317\n",
        "Iteration 30: Best score = 0.5317\n",
        "Iteration 35: Best score = 0.5317\n",
        "Iteration 40: Best score = 0.5338\n",
        "Iteration 45: Best score = 0.5338\n",
        "Iteration 50: Best score = 0.5338\n",
        "Iteration 55: Best score = 0.5338\n",
        "Iteration 60: Best score = 0.5357\n",
        "Iteration 65: Best score = 0.5364\n",
        "Iteration 70: Best score = 0.5364\n",
        "Iteration 75: Best score = 0.5364\n",
        "Iteration 80: Best score = 0.5364\n",
        "Iteration 85: Best score = 0.5364\n",
        "Iteration 90: Best score = 0.5364\n",
        "Iteration 95: Best score = 0.5364\n",
        "Iteration 100: Best score = 0.5364\n",
        "Best parameters: OrderedDict({'colsample_bytree': 0.9227515368399839, 'learning_rate': 0.04029469909025679, 'max_depth': 31, 'n_estimators': 28, 'subsample': 0.7983208166471775})\n",
        "Best score: 0.5364\n",
        "\n",
        "Optimizing LGBMRegressor...\n",
        "Iteration 5: Best score = 0.5259\n",
        "Iteration 10: Best score = 0.5265\n",
        "Iteration 15: Best score = 0.5265\n",
        "Iteration 20: Best score = 0.5265\n",
        "Iteration 25: Best score = 0.5272\n",
        "Iteration 30: Best score = 0.5272\n",
        "Iteration 35: Best score = 0.5272\n",
        "Iteration 40: Best score = 0.5272\n",
        "Iteration 45: Best score = 0.5272\n",
        "Iteration 50: Best score = 0.5272\n",
        "Iteration 55: Best score = 0.5272\n",
        "Iteration 60: Best score = 0.5272\n",
        "Iteration 65: Best score = 0.5272\n",
        "Iteration 70: Best score = 0.5292\n",
        "Iteration 75: Best score = 0.5292\n",
        "Iteration 80: Best score = 0.5311\n",
        "Iteration 85: Best score = 0.5311\n",
        "Iteration 90: Best score = 0.5311\n",
        "Iteration 95: Best score = 0.5311\n",
        "Iteration 100: Best score = 0.5311\n",
        "Best parameters: OrderedDict({'learning_rate': 0.0634974604459163, 'max_depth': 8, 'min_child_samples': 100, 'n_estimators': 195, 'num_leaves': 185})\n",
        "Best score: 0.5311\n",
        "\n",
        "Optimizing SVR...\n",
        "Iteration 5: Best score = 0.5368\n",
        "Iteration 10: Best score = 0.5368\n",
        "Iteration 15: Best score = 0.5373\n",
        "Iteration 20: Best score = 0.5382\n",
        "Iteration 25: Best score = 0.5382\n",
        "Iteration 30: Best score = 0.5382\n",
        "Iteration 35: Best score = 0.5382\n",
        "Iteration 40: Best score = 0.5382\n",
        "Iteration 45: Best score = 0.5390\n",
        "Iteration 50: Best score = 0.5390\n",
        "Iteration 55: Best score = 0.5390\n",
        "Iteration 60: Best score = 0.5390\n",
        "Iteration 65: Best score = 0.5390\n",
        "Iteration 70: Best score = 0.5390\n",
        "Iteration 75: Best score = 0.5390\n",
        "Iteration 80: Best score = 0.5390\n",
        "Iteration 85: Best score = 0.5390\n",
        "Iteration 90: Best score = 0.5390\n",
        "Iteration 95: Best score = 0.5390\n",
        "Iteration 100: Best score = 0.5390\n",
        "Best parameters: OrderedDict({'C': 0.10270990995387987, 'epsilon': 0.0010205465165897824, 'kernel': 'linear'})\n",
        "Best score: 0.5390\n",
        "\n",
        "Optimizing KNNRegressor...\n",
        "Iteration 5: Best score = 0.5194\n",
        "Iteration 10: Best score = 0.5251\n",
        "Iteration 15: Best score = 0.5251\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(50), np.int64(1), np.str_('uniform')] before, using random point [np.int64(17), np.int64(1), 'uniform']\n",
        "  warnings.warn(\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(50), np.int64(1), np.str_('uniform')] before, using random point [np.int64(39), np.int64(1), 'uniform']\n",
        "  warnings.warn(\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(50), np.int64(1), np.str_('uniform')] before, using random point [np.int64(35), np.int64(2), 'uniform']\n",
        "  warnings.warn(\n",
        "Iteration 20: Best score = 0.5251\n",
        "Iteration 25: Best score = 0.5251\n",
        "Iteration 30: Best score = 0.5251\n",
        "Iteration 35: Best score = 0.5251\n",
        "Iteration 40: Best score = 0.5251\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(44), np.int64(2), np.str_('uniform')] before, using random point [np.int64(38), np.int64(1), 'distance']\n",
        "  warnings.warn(\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(50), np.int64(2), np.str_('uniform')] before, using random point [np.int64(33), np.int64(2), 'uniform']\n",
        "  warnings.warn(\n",
        "C:\\Users\\bobigny ash 3\\anaconda3\\Lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(50), np.int64(2), np.str_('distance')] before, using random point [np.int64(1), np.int64(1), 'uniform']\n",
        "  warnings.warn(\n",
        "Iteration 45: Best score = 0.5251\n",
        "Iteration 50: Best score = 0.5251\n",
        "Iteration 55: Best score = 0.5261\n",
        "Iteration 60: Best score = 0.5261\n",
        "Iteration 65: Best score = 0.5261\n",
        "Iteration 70: Best score = 0.5261\n",
        "Iteration 75: Best score = 0.5261\n",
        "Iteration 80: Best score = 0.5272\n",
        "Iteration 85: Best score = 0.5272\n",
        "Iteration 90: Best score = 0.5272\n",
        "Iteration 95: Best score = 0.5272\n",
        "Iteration 100: Best score = 0.5272\n",
        "Best parameters: OrderedDict({'n_neighbors': 10, 'p': 1, 'weights': 'distance'})\n",
        "Best score: 0.5272\n",
        "\n",
        "Optimizing Lasso...\n",
        "Iteration 5: Best score = 0.5264\n",
        "Iteration 10: Best score = 0.5264\n",
        "Iteration 15: Best score = 0.5264\n",
        "Iteration 20: Best score = 0.5278\n",
        "Iteration 25: Best score = 0.5278\n",
        "Iteration 30: Best score = 0.5278\n",
        "Iteration 35: Best score = 0.5278\n",
        "Iteration 40: Best score = 0.5278\n",
        "Iteration 45: Best score = 0.5278\n",
        "Iteration 50: Best score = 0.5278\n",
        "Iteration 55: Best score = 0.5278\n",
        "Iteration 60: Best score = 0.5290\n",
        "Iteration 65: Best score = 0.5297\n",
        "Iteration 70: Best score = 0.5297\n",
        "Iteration 75: Best score = 0.5297\n",
        "Iteration 80: Best score = 0.5297\n",
        "Iteration 85: Best score = 0.5297\n",
        "Iteration 90: Best score = 0.5297\n",
        "Iteration 95: Best score = 0.5297\n",
        "Iteration 100: Best score = 0.5297\n",
        "Best parameters: OrderedDict({'alpha': 0.015848691168076898})\n",
        "Best score: 0.5297\n",
        "\n",
        "Optimizing Ridge...\n",
        "Iteration 5: Best score = 0.5256\n",
        "Iteration 10: Best score = 0.5256\n",
        "Best parameters: OrderedDict({'alpha': 6.2893244081003825})\n",
        "Best score: 0.5256\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "3c167bce-dda7-4b67-9dc5-d9d1c0999750",
      "metadata": {},
      "source": [
        "RandomForestRegressor... \n",
        "Best parameters: OrderedDict({'max_depth': 46, 'min_samples_leaf': 7, 'min_samples_split': 4, 'n_estimators': 37})\n",
        "Best score: 0.5381\n",
        "\n",
        " XGBRegressor...\n",
        "Best parameters: OrderedDict({'colsample_bytree': 0.9227515368399839, 'learning_rate': 0.04029469909025679, 'max_depth': 31, 'n_estimators': 28, 'subsample': 0.7983208166471775})\n",
        "Best score: 0.5364\n",
        "\n",
        " LGBMRegressor...\n",
        "Best parameters: OrderedDict({'learning_rate': 0.0634974604459163, 'max_depth': 8, 'min_child_samples': 100, 'n_estimators': 195, 'num_leaves': 185})\n",
        "Best score: 0.5311\n",
        "\n",
        "\n",
        " SVR...\n",
        "Best parameters: OrderedDict({'C': 0.10270990995387987, 'epsilon': 0.0010205465165897824, 'kernel': 'linear'})\n",
        "Best score: 0.5390\n",
        "\n",
        " KNNRegressor...\n",
        "Best parameters: OrderedDict({'n_neighbors': 10, 'p': 1, 'weights': 'distance'})\n",
        "Best score: 0.5272\n",
        "\n",
        "Lasso...\n",
        "Best parameters: OrderedDict({'alpha': 0.015848691168076898})\n",
        "Best score: 0.5297\n",
        "\n",
        "Ridge...\n",
        "Best parameters: OrderedDict({'alpha': 6.2893244081003825})\n",
        "Best score: 0.5256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ad4ca0-1636-43e3-9362-2fcfe874d6e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "models_opt = {}\n",
        "\n",
        "models_opt['RandomForestRegressor'] = {\n",
        "    'estimator': RandomForestRegressor(**{'max_depth': 46, 'min_samples_leaf': 7, 'min_samples_split': 4, 'n_estimators': 37}),\n",
        "    'best_params_': {'max_depth': 46, 'min_samples_leaf': 7, 'min_samples_split': 4, 'n_estimators': 37},\n",
        "    'best_score_': 0.5381\n",
        "}\n",
        "\n",
        "models_opt['XGBRegressor'] = {\n",
        "    'estimator': XGBRegressor(**{'colsample_bytree': 0.9227515368399839, 'learning_rate': 0.04029469909025679, 'max_depth': 31, 'n_estimators': 28, 'subsample': 0.7983208166471775}),\n",
        "    'best_params_': {'colsample_bytree': 0.9227515368399839, 'learning_rate': 0.04029469909025679, 'max_depth': 31, 'n_estimators': 28, 'subsample': 0.7983208166471775},\n",
        "    'best_score_': 0.5364\n",
        "}\n",
        "\n",
        "models_opt['LGBMRegressor'] = {\n",
        "    'estimator': LGBMRegressor(**{'learning_rate': 0.0634974604459163, 'max_depth': 8, 'min_child_samples': 100, 'n_estimators': 195, 'num_leaves': 185}),\n",
        "    'best_params_': {'learning_rate': 0.0634974604459163, 'max_depth': 8, 'min_child_samples': 100, 'n_estimators': 195, 'num_leaves': 185},\n",
        "    'best_score_': 0.5311\n",
        "}\n",
        "\n",
        "models_opt['SVR'] = {\n",
        "    'estimator': SVR(**{'C': 0.10270990995387987, 'epsilon': 0.0010205465165897824, 'kernel': 'linear'}),\n",
        "    'best_params_': {'C': 0.10270990995387987, 'epsilon': 0.0010205465165897824, 'kernel': 'linear'},\n",
        "    'best_score_': 0.5390\n",
        "}\n",
        "\n",
        "models_opt['KNNRegressor'] = {\n",
        "    'estimator': KNeighborsRegressor(**{'n_neighbors': 10, 'p': 1, 'weights': 'distance'}),\n",
        "    'best_params_': {'n_neighbors': 10, 'p': 1, 'weights': 'distance'},\n",
        "    'best_score_': 0.5272\n",
        "}\n",
        "\n",
        "models_opt['Lasso'] = {\n",
        "    'estimator': Lasso(**{'alpha': 0.015848691168076898}),\n",
        "    'best_params_': {'alpha': 0.015848691168076898},\n",
        "    'best_score_': 0.5297\n",
        "}\n",
        "\n",
        "models_opt['Ridge'] = {\n",
        "    'estimator': Ridge(**{'alpha': 6.2893244081003825}),\n",
        "    'best_params_': {'alpha': 6.2893244081003825},\n",
        "    'best_score_': 0.5256\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2929f25b-6fd5-4a6b-b6c3-7e199901134d",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {}\n",
        "for name, opt in models_opt.items():\n",
        "    model_class = type(opt[\"estimator\"])  # Get the model class (e.g., RandomForestRegressor)\n",
        "    best_params = opt[\"best_params_\"]  # Get the best parameters from the optimization\n",
        "\n",
        "    # Add the model with the best parameters to the new dictionary\n",
        "    models[f\"{name} opt\"] = model_class(**best_params)\n",
        "\n",
        "# Add a baseline model manually (if needed)\n",
        "#models['Random Forest Baseline'] = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ccfc3a-5982-4d45-afcc-2ca7a9df2159",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression \n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76e0f37-8a0d-448e-bc15-b8d9d7fc1199",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1810a10f-1174-4f7d-b24f-3c299d2d2bd0",
      "metadata": {},
      "source": [
        "Si train sont significativement plus courtes que tests signifie que le modèle surajuste (overfitting). Il fonctionne bien sur l'entraînement, mais moins bien sur les données de test. (inversement underfitting)\n",
        "\n",
        "Si train et test sont similaires en longueur, cela signifie que le modèle généralise bien à la fois sur les données d'entraînement et de test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39bef9b-695c-4aa8-a585-5edb10017b01",
      "metadata": {},
      "source": [
        "# Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cbb1e7-371b-440e-8773-c5edae6c638c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort the models by their best score in descending order\n",
        "top_regressors = sorted(models_opt.items(), key=lambda x: x[1][\"best_score_\"], reverse=True)\n",
        "# Filter only classifiers from the sorted models\n",
        "\n",
        "top_2_regressors = [(name, opt[\"estimator\"]) for name, opt in top_regressors if isinstance(opt[\"estimator\"], RegressorMixin)][:2]\n",
        "\n",
        "# Print the top 2 regressors for verification\n",
        "print(\"Top 2 regressors used in VotingRegressor:\")\n",
        "for name, estimator in top_2_regressors:\n",
        "    print(f\"{name}: {type(estimator).__name__}\")\n",
        "\n",
        "# Define the voting ensemble using the top 2 regressors\n",
        "voting_regressor_ensemble = VotingRegressor(estimators=top_2_regressors)\n",
        "\n",
        "\n",
        "bagging_ensemble = BaggingRegressor(\n",
        "    estimator=RandomForestRegressor(),\n",
        "    n_estimators=10,\n",
        "    max_samples=0.8,\n",
        "    max_features=0.8,\n",
        "    bootstrap=True,\n",
        "    bootstrap_features=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "models = {\n",
        "    'bagging_ensemble': bagging_ensemble,\n",
        "    'voting_regressor_ensemble': voting_regressor_ensemble,\n",
        "      'Random Forest Baseline': RandomForestRegressor()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1886781d-c8c6-4790-839b-65e0147b3474",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run cross-validation for regression models\n",
        "results = run_multi_model_cv(X, y, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e16b48e-2771-443c-9a9c-7a48dc815d83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot MSE results for regression models\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2748f03-d505-4489-9a0a-e3d367b10aa0",
      "metadata": {},
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1e9b0a-1deb-4285-9dee-259772f4ded5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate regressors and classifiers from models_opt\n",
        "regressor_estimators = [(name, opt[\"estimator\"]) for name, opt in models_opt.items() if isinstance(opt[\"estimator\"], RegressorMixin)]\n",
        "\n",
        "\n",
        "# Définition des meta-modèles\n",
        "ridge_regressor = Ridge()\n",
        "random_forest_regressor = RandomForestRegressor()\n",
        "xgb_regressor = XGBRegressor()\n",
        "lasso_regressor = Lasso()\n",
        "svr_regressor = SVR()\n",
        "lgbm_regressor = LGBMRegressor()\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "# Stacking Regressor avec Ridge comme meta-modèle\n",
        "stacking_regressor_ridge = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=ridge_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec RandomForest comme meta-modèle\n",
        "stacking_regressor_rf = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=random_forest_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec XGBRegressor comme meta-modèle\n",
        "stacking_regressor_xgb = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=xgb_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec Lasso comme meta-modèle\n",
        "stacking_regressor_lasso = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=lasso_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec SVR comme meta-modèle\n",
        "stacking_regressor_svr = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=svr_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec LGBMRegressor comme meta-modèle\n",
        "stacking_regressor_lgbm = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=lgbm_regressor\n",
        ")\n",
        "\n",
        "# Stacking Regressor avec KNeighborsRegressor comme meta-modèle\n",
        "stacking_regressor_knn = StackingRegressor(\n",
        "    estimators=regressor_estimators,\n",
        "    final_estimator=knn_regressor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28219816-e7c4-42a7-a449-2e16544bd2bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Stacking Regressor (Ridge Meta)': stacking_regressor_ridge,\n",
        "    'Stacking Regressor (RandomForest Meta)': stacking_regressor_rf,\n",
        "    'Stacking Regressor (XGBRegressor Meta)': stacking_regressor_xgb,\n",
        "    'Stacking Regressor (Lasso Meta)': stacking_regressor_lasso,\n",
        "    'Stacking Regressor (SVR Meta)': stacking_regressor_svr,\n",
        "    'Stacking Regressor (LGBMRegressor Meta)': stacking_regressor_lgbm,\n",
        "    'Stacking Regressor (KNeighborsRegressor Meta)': stacking_regressor_knn\n",
        "}\n",
        "\n",
        "results = run_multi_model_cv(X, y, models)\n",
        "\n",
        "plot_multi_model_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0431ff6-0e5d-4ead-8735-84e1a728ef11",
      "metadata": {},
      "source": [
        "### Submission:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe4a5d9-be2e-40f1-b1b4-59cac5e7197b",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_train = pd.read_csv('module6_exercise_train.csv', index_col='index')\n",
        "X_test = pd.read_csv('module6_exercise_test.csv', index_col='index')\n",
        "y_train = data_train.pop('end_of_day_return')\n",
        "X_train = data_train.copy()\n",
        "data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db2ea4cd-5463-41ac-b49a-6ec38252b5b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on complete data (X_train, y_train) and predict on X_test\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "best_model=  SVR(  C=0.10270990995387987,    epsilon=0.0010205465165897824, kernel='linear')\n",
        "\n",
        "\n",
        "best_model.fit(X_train, y_train)\n",
        "cv = cross_validate(best_model, X_train, y_train, cv=5, scoring=make_scorer(weighted_accuracy))\n",
        "\n",
        "print(cv['test_score'].mean())\n",
        "pred=best_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9877df-0e30-40ed-91a0-69511d359033",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "submission = pd.DataFrame({\n",
        "    'index': X_test.index,\n",
        "    'end_of_day_return':pred\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d162216-2f3a-4d63-bbbb-66f0a4ce6bcb",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

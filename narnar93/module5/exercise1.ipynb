{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c76e221-8579-458e-8676-de09bbabc73a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6ac68e-352d-4dab-9a64-cab473331541",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb98c7eb-6d0e-4164-bf22-77a1cf6f111a",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(df_train.info(),df_train.nunique())\n",
        "df_test.info(),df_test.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {},
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b2ef10c-3f64-41b3-905b-90d224702500",
      "metadata": {},
      "source": [
        "#### Make a complete analysis on data preprocessing\n",
        "# Inconsistencies\n",
        "# Duplicates (data.duplicated().sum())\n",
        "# Missing values (data.isnull().sum())\n",
        "# Categorical\n",
        "# Outliers\n",
        "# Feature Engineering\n",
        "# Feature Selection and/or Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.concat([df_train, df_test], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c046b07a-845c-460b-a692-27a97ec3d613",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a111913e-2525-4310-a9cb-d22091bcef2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7699612-bcef-409a-b185-35cb2c4d8b26",
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(data.info())\n",
        "print(data.nunique)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9593d84-1ae0-4382-9f29-60a1cdcb71dc",
      "metadata": {},
      "source": [
        "# Inconsistencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49360dbe-300c-458a-9e13-0cc20553ef21",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['weather_condition'] = data['weather_condition'].str.lower().str.strip()\n",
        "data['oil_brent_price_indicator'] = data['oil_brent_price_indicator'].str.lower().str.strip()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eb3509-3186-4634-ac9a-e851137312df",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4465289b-a24c-49e6-addb-8a53cafa0826",
      "metadata": {},
      "source": [
        "# Duplicates (data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a02053-0760-45d1-99cd-2e042ef1ebad",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Dates dupliquées : {data.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcd0633-d508-45a9-ad23-7042927fb9f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "data=data.drop_duplicates(subset=['date'], keep='last')\n",
        "print(f\"Dates dupliquées : {data.duplicated().sum()}\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "    \n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "    \n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a88994b-6609-4229-b248-285b46d4684a",
      "metadata": {},
      "source": [
        "# Missing values (data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2daae63f-9b78-4104-9f05-ee77e9d7c5d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_missing_data(df, target_column='electricity_demand'):\n",
        "    # Print the first few rows of the DataFrame to understand its structure\n",
        "    print(\"DataFrame head:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Basic info about the dataset\n",
        "    print(\"\\nDataFrame info:\")\n",
        "    df.info()\n",
        "\n",
        "    # Summarize missing values per column\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    missing_data = df.isnull().sum()\n",
        "    print(missing_data)\n",
        "\n",
        "    # Percentage of missing values per column\n",
        "    print(\"\\nPercentage of missing values per column:\")\n",
        "    percent_missing = df.isnull().mean() * 100\n",
        "    print(percent_missing)\n",
        "\n",
        "    # Visualizing missing values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "    plt.title('Missing Value Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualizing percentage of missing values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    percent_missing.plot(kind='bar', color='dodgerblue')\n",
        "    plt.title('Percentage of Missing Values Per Column')\n",
        "    plt.ylabel('Percentage Missing')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compute correlation between missingness and target variable\n",
        "    missingness_correlation = {}\n",
        "    for column in df.columns:\n",
        "        if column != target_column and df[column].isnull().sum() > 0:\n",
        "            correlation = df[column].isnull().corr(df[target_column])\n",
        "            missingness_correlation[column] = correlation\n",
        "\n",
        "    print(\"\\nCorrelation between missingness and target variable:\")\n",
        "    for column, correlation in missingness_correlation.items():\n",
        "        print(f\"{column}: {correlation:.4f}\")\n",
        "\n",
        "    # Visualize correlation between missingness and target variable\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(missingness_correlation.keys(), missingness_correlation.values())\n",
        "    plt.title(f'Correlation between Missingness and {target_column}')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the analysis\n",
        "analyze_missing_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['date'] = pd.to_datetime(data['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"humidity\"] = data[\"humidity\"].interpolate(method=\"linear\")\n",
        "data  = data.ffill().bfill() # Timeserie stamp\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['wind_speed']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78d6c6d-206b-40eb-9317-d00b8a92aecc",
      "metadata": {},
      "source": [
        "# Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583ec1b8-a2ae-4ef2-a48c-605eed08510e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify unique values\n",
        "print(\"Unique values in each column:\")\n",
        "columns = [\"weather_condition\", \"oil_brent_price_indicator\"]\n",
        "for column in columns:\n",
        "    print(f\"{column}: {data[column].nunique()} - {data[column].unique()}\")"
      ]
    },
    {
      "cell_type": "raw",
      "id": "031651e8-a0da-45dd-81ca-a086da61aa2b",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "# transformation en variable ordinal weather_condition\n",
        "#encoder = OrdinalEncoder(categories=[['snowy','rainy','cloudy', 'sunny']])\n",
        "#data['weather_condition'] = encoder.fit_transform(data[['weather_condition']])\n",
        "encoder2 = OrdinalEncoder(categories=[['very low','low','moderate','high','very high']])\n",
        "data['oil_brent_price_indicator'] = encoder2.fit_transform(data[['oil_brent_price_indicator']])\n",
        "print(data['oil_brent_price_indicator'].unique())\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d7882e-35ad-4d40-9649-305d14a89b95",
      "metadata": {},
      "outputs": [],
      "source": [
        "#data['weather_condition']= pd.Categorical(data['weather_condition'] categories=['snowy','rainy','cloudy', 'sunny'], ordered=True)\n",
        "data['oil_brent_price_indicator'] = pd.Categorical(data['oil_brent_price_indicator'], categories=['very low','low','moderate','high','very high'], ordered=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd20e376-1782-4077-a242-f948811c7984",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outliers\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for i in range(data.shape[1]):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    sns.boxplot(x=data.iloc[:, i])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19981fb7-cbff-427a-ad74-55ef1237f8f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_iqr(data):\n",
        "    Q1 = data.quantile(0.25)\n",
        "    Q3 = data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data < lower_bound) | (data > upper_bound)]\n",
        "\n",
        "outliers = detect_outliers_iqr(data['humidity'])\n",
        "print(outliers)\n",
        "outliers = detect_outliers_iqr(data['electricity_demand'])\n",
        "print(\"\\n\", outliers)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "2273d423-3a7c-482d-abf2-b8564af4bec9",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "data ['year'] = data['date'].dt.year\n",
        "data ['month'] = data['date'].dt.month\n",
        "data ['day'] = data['date'].dt.day\n",
        "#df_train['dayofweek'] = df_train['date'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'electricity_demand', '2017-01-01', '2019-09-07')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'humidity', '2016-06-01', '2016-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {},
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86971ab4-1ef8-464b-afb5-0d750a8c4035",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide a complete data preprocessing transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Handle Inconsistencies\n",
        "def handle_inconsistencies(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.copy()\n",
        "    # fonction pour m/s ->km/h\n",
        "    def handle_wind(x):\n",
        "        wind = str(x).split(' ')\n",
        "        if wind[-1] == \"km/h\":\n",
        "            value = float(wind[0])\n",
        "        elif wind[-1] == \"m/s\":\n",
        "            value = float(wind[0]) *3.6\n",
        "        else:\n",
        "            value = float(wind[0])\n",
        "        return value\n",
        "\n",
        "    X_train['wind_speed'] = X_train['wind_speed'].apply(handle_wind)\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.copy()\n",
        "\n",
        "        X_val['wind_speed'] = X_val['wind_speed'].apply(handle_wind)\n",
        "        return X_train, y_train, X_val\n",
        "    else:\n",
        "        return X_train, y_train\n",
        "\n",
        "\n",
        "# 2. Handling Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    X_train_clean = X_train.copy()\n",
        "    y_train_clean = y_train.copy()\n",
        "\n",
        "    X_train_clean = X_train.drop_duplicates()\n",
        "    y_train_clean = y_train.loc[X_train_clean.index]\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val_clean = X_val.copy()\n",
        "\n",
        "        X_val_clean = X_val.drop_duplicates()\n",
        "        return X_train_clean, y_train_clean, X_val\n",
        "    else:\n",
        "        return X_train_clean, y_train_clean\n",
        "\n",
        "# 3. Handling Missing Values\n",
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.copy()\n",
        "    # forwad fill ensuite backward fill pour les valeurs manquantes\n",
        "    X_train_imputed = X_train.ffill().bfill() \n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.copy()\n",
        "\n",
        "        X_val_imputed = X_val.ffill().bfill()\n",
        "        return X_train_imputed, X_val_imputed\n",
        "    else:\n",
        "        return X_train_imputed\n",
        "\n",
        "\n",
        "# 4. Handling Categorical Values\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "    mapping = {'Very Low': 0, 'Low': 1, 'Moderate': 2, 'High': 3, 'Very High': 4}\n",
        "    X_train_encoded = pd.get_dummies(X_train.copy(), columns=['weather_condition'], dummy_na=False)\n",
        "    X_train_encoded['oil_brent_price_indicator'] = X_train_encoded['oil_brent_price_indicator'].map(mapping)\n",
        "    if X_val is not None:\n",
        "        X_val_encoded = pd.get_dummies(X_val.copy(), columns=['weather_condition'], dummy_na=False)\n",
        "        X_val_encoded['oil_brent_price_indicator'] = X_val_encoded['oil_brent_price_indicator'].map(mapping)\n",
        "        X_val_encoded = X_val_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
        "        return X_train_encoded, X_val_encoded\n",
        "    else:\n",
        "        return X_train_encoded\n",
        "\n",
        "# 5. Handling Outliers\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.copy()\n",
        "\n",
        "    def impute_outliers(X):\n",
        "        Q1 = X.quantile(0.25)\n",
        "        Q3 = X.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5*IQR\n",
        "        upper_bound = Q3 + 1.5*IQR\n",
        "\n",
        "        impute_value = X.median()\n",
        "        data_imputed = X.copy()\n",
        "        data_imputed[(X < lower_bound) | (X > upper_bound)] = impute_value\n",
        "        return data_imputed\n",
        "\n",
        "    X_train['humidity'] = impute_outliers(X_train['humidity']) # outlier dans la colonne humidity\n",
        "    y_train = impute_outliers(y_train) # df_train['Electricity_demand']\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.copy()\n",
        "\n",
        "        X_val['humidity'] = impute_outliers(X_val['humidity'])\n",
        "        return X_train, y_train, X_val\n",
        "    else:\n",
        "        return X_train, y_train\n",
        "\n",
        "\n",
        "# 6. Feature Engineering\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    def datatime(df):\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df['year'] = df['date'].dt.year\n",
        "        df['month'] = df['date'].dt.month\n",
        "        df['day'] = df['date'].dt.day\n",
        "        df['dayofweek'] = df['date'].dt.dayofweek\n",
        "        df['dayofmonth'] = df['day'] / df['date'].dt.days_in_month\n",
        "        df['isweekend'] =df['dayofweek'].isin([5, 6]).astype(int)\n",
        "\n",
        "        return df\n",
        "    X_train = datatime(X_train.copy())\n",
        "    if X_val is not None:\n",
        "        X_val = datatime(X_val.copy())\n",
        "        return X_train, y_train, X_val\n",
        "    else:\n",
        "        return X_train, y_train\n",
        "\n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    #print(X_train.columns.drop(['date', 'day', 'year','month']))\n",
        "    #X_train.columns.drop(['date', 'day', 'year','month'])\n",
        "    #print(X_train.columns.drop(['date', 'day', 'year','month']))\n",
        "    #selected_columns = X_train.columns.drop(['date', 'day', 'year','month'])\n",
        "    selected_columns =  X_train.columns.drop(['date', 'day', 'year','month'])\n",
        "    if X_val is not None:\n",
        "        return X_train[selected_columns] ,X_val[selected_columns]\n",
        "    else:\n",
        "        return X_train[selected_columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a52dc7-d755-4a92-8c81-aff505536d50",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### call transformations here, if there is no learning and no need to be crossval\n",
        "    X, y = handle_inconsistencies(X, y)\n",
        "    X, y = handle_duplicates(X, y)\n",
        "    X  = handle_missing_values(X, y)\n",
        "    X= handle_categorical(X, y)\n",
        "    X, y = handle_outliers(X, y)\n",
        "    X, y = feature_engineering(X, y)\n",
        "    X = feature_selection(X, y)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    #model = XGBRegressor() #  Validation MSE:  436.0203, Max: 541.9537, Min: 379.3908 np.float64(436.0203269697978)\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "    \n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        # X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        #X_train, y_train, X_val = handle_duplicates(X_train, y_train, X_val)\n",
        "        # X_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        #X_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        # X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        #X_train, y_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        #X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    \n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y) #np.float64(1109.3930508876085)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {},
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and submit your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X_train and y_train from your data\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call serve_model to train and predict\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

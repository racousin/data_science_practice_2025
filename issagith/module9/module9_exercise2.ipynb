{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEAUjjhOb136"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uVgWUZjpb137"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!pip install gymnasium==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oa03cAjLb138"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZwAAf2b139"
   },
   "source": [
    "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQQPZhpb139"
   },
   "source": [
    "### Objective\n",
    "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_Q_TABLE = None\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, q_table=None):\n",
    "        self.env = env\n",
    "        if q_table is not None:\n",
    "            self.q_table = np.array(q_table, dtype=np.float64)\n",
    "        else:\n",
    "            if TRAINED_Q_TABLE is None:\n",
    "                raise RuntimeError(\n",
    "                    \"Q-table not trained yet. Execute the training cell before instantiating Agent.\"\n",
    "                )\n",
    "            self.q_table = TRAINED_Q_TABLE.copy()\n",
    "        self.policy = np.argmax(self.q_table, axis=1)\n",
    "\n",
    "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
    "        if observation is None:\n",
    "            return self.env.action_space.sample()\n",
    "        state = int(observation)\n",
    "        if state >= self.q_table.shape[0]:\n",
    "            return self.env.action_space.sample()\n",
    "        q_values = self.q_table[state]\n",
    "        if np.allclose(q_values, 0.0):\n",
    "            return self.env.action_space.sample()\n",
    "        return int(self.policy[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
    "\n",
    "Holes in the ice are distributed in set locations.\n",
    "\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
    "\n",
    "The environment is based on :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after 394 iterations with delta=9.62e-10.\n"
     ]
    }
   ],
   "source": [
    "training_env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
    "num_states = training_env.observation_space.n\n",
    "num_actions = training_env.action_space.n\n",
    "transition_model = training_env.unwrapped.P\n",
    "\n",
    "# Value iteration hyperparameters\n",
    "max_iterations = 10_000\n",
    "threshold = 1e-9\n",
    "gamma = 0.99\n",
    "\n",
    "value_function = np.zeros(num_states, dtype=np.float64)\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    delta = 0.0\n",
    "    for state in range(num_states):\n",
    "        current_value = value_function[state]\n",
    "        action_values = np.zeros(num_actions, dtype=np.float64)\n",
    "        for action in range(num_actions):\n",
    "            for prob, next_state, reward, terminated in transition_model[state][action]:\n",
    "                continuation = 0.0 if terminated else value_function[next_state]\n",
    "                action_values[action] += prob * (reward + gamma * continuation)\n",
    "        value_function[state] = np.max(action_values)\n",
    "        delta = max(delta, abs(current_value - value_function[state]))\n",
    "    if delta < threshold:\n",
    "        print(f\"Value iteration converged after {iteration + 1} iterations with delta={delta:.2e}.\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Warning: value iteration reached the iteration limit before convergence.\")\n",
    "\n",
    "q_table = np.zeros((num_states, num_actions), dtype=np.float64)\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        for prob, next_state, reward, terminated in transition_model[state][action]:\n",
    "            continuation = 0.0 if terminated else value_function[next_state]\n",
    "            q_table[state, action] += prob * (reward + gamma * continuation)\n",
    "\n",
    "training_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward over evaluation episodes: 0.637\n",
      "Success rate: 63.70%\n",
      "Objective satisfied: mean reward is above 0.35.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(q_values, episodes=5_000):\n",
    "    eval_env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
    "    rng = np.random.default_rng(2025)\n",
    "    total_reward = 0.0\n",
    "    successes = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state, _ = eval_env.reset(seed=int(rng.integers(0, 1_000_000)))\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action_values = q_values[state]\n",
    "            if np.allclose(action_values, action_values[0]):\n",
    "                action = eval_env.action_space.sample()\n",
    "            else:\n",
    "                best_value = np.max(action_values)\n",
    "                best_actions = np.flatnonzero(np.isclose(action_values, best_value))\n",
    "                action = int(best_actions[rng.integers(0, len(best_actions))]) if best_actions.size else eval_env.action_space.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                total_reward += reward\n",
    "                if reward > 0:\n",
    "                    successes += 1\n",
    "\n",
    "    eval_env.close()\n",
    "    mean_reward = total_reward / episodes\n",
    "    success_rate = successes / episodes\n",
    "    return mean_reward, success_rate\n",
    "\n",
    "mean_reward, success_rate = evaluate_policy(q_table)\n",
    "print(f\"Mean reward over evaluation episodes: {mean_reward:.3f}\")\n",
    "print(f\"Success rate: {success_rate:.2%}\")\n",
    "\n",
    "if mean_reward >= 0.35:\n",
    "    print(\"Objective satisfied: mean reward is above 0.35.\")\n",
    "else:\n",
    "    print(\"Warning: mean reward is below the 0.35 target, consider tuning hyperparameters.\")\n",
    "\n",
    "global TRAINED_Q_TABLE\n",
    "TRAINED_Q_TABLE = q_table.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before submit\n",
    "Test that your agent has the right attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
    "agent = Agent(env)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "reward, terminated, truncated, info = None, False, False, None\n",
    "rewards = []\n",
    "while not (terminated or truncated):\n",
    "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "print(f'Cumulative Reward: {sum(rewards)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

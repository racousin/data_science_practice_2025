{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uVgWUZjpb137"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhSGPUHZwzQL"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mf_3yhyewzQL"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        action = self.env.action_space.sample() # your logic here\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL1s9OQiwzQM"
      },
      "source": [
        "### Description\n",
        "\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
        "\n",
        "Holes in the ice are distributed in set locations.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
        "\n",
        "The environment is based on :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TQ8tdnBxwzQM"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agent.py\n",
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        gamma = 0.99\n",
        "        tol = 1e-10\n",
        "        max_iter = 10000\n",
        "\n",
        "        nS = env.observation_space.n\n",
        "        nA = env.action_space.n\n",
        "        P = getattr(getattr(env, \"unwrapped\", env), \"P\", None)\n",
        "\n",
        "        if P is not None:\n",
        "            V = np.zeros(nS, dtype=float)\n",
        "\n",
        "            def q_of(s, a, Vvec):\n",
        "                total = 0.0\n",
        "                for (p, ns, r, done) in P[s][a]:\n",
        "                    total += p * (r + (0.0 if done else gamma * Vvec[ns]))\n",
        "                return total\n",
        "\n",
        "            for _ in range(max_iter):\n",
        "                delta = 0.0\n",
        "                for s in range(nS):\n",
        "                    qs = [q_of(s, a, V) for a in range(nA)]\n",
        "                    best = max(qs)\n",
        "                    delta = max(delta, abs(best - V[s]))\n",
        "                    V[s] = best\n",
        "                if delta < tol:\n",
        "                    break\n",
        "\n",
        "            policy = np.zeros(nS, dtype=int)\n",
        "            for s in range(nS):\n",
        "                qs = [q_of(s, a, V) for a in range(nA)]\n",
        "                policy[s] = int(np.argmax(qs))\n",
        "            self.policy = policy.tolist()\n",
        "        else:\n",
        "            # fallback：快速 Q-learning 预训练\n",
        "            Q = np.zeros((nS, nA), dtype=float)\n",
        "            eps, eps_end, eps_decay = 1.0, 0.05, 0.999\n",
        "            alpha = 0.6\n",
        "\n",
        "            for _ in range(8000):\n",
        "                s, _ = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    if np.random.rand() < eps:\n",
        "                        a = env.action_space.sample()\n",
        "                    else:\n",
        "                        a = int(np.argmax(Q[s]))\n",
        "                    s2, r, terminated, truncated, _ = env.step(a)\n",
        "                    done = terminated or truncated\n",
        "                    Q[s, a] += alpha * (r + (0.0 if done else gamma * np.max(Q[s2])) - Q[s, a])\n",
        "                    s = s2\n",
        "                eps = max(eps_end, eps * eps_decay)\n",
        "\n",
        "            self.policy = np.argmax(Q, axis=1).tolist()\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info=None):\n",
        "        return int(self.policy[observation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-fw1Zjoz5wN",
        "outputId": "fb6afe47-b627-4951-d76a-b5ca69ccdea4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the existence of agent\n",
        "!ls -l agent.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TICtlGK10F5o",
        "outputId": "f3fb8608-1e5c-4d27-e00f-4a7a8be76877"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 2133 Nov 11 03:05 agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zyQRqzgwzQM",
        "outputId": "c143ba51-b688-49e8-8fb6-79c134349bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runs: 300 | mean_reward_per_run: 6.283 / 10\n",
            "Per-attempt success rate: 62.83%\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "from agent import Agent  # 如果你在同目录保存为 agent.py\n",
        "\n",
        "def evaluate_agent(n_runs=200, max_steps=200, seed=123):\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")  # 默认 is_slippery=True\n",
        "    env.reset(seed=seed)\n",
        "    agent = Agent(env)\n",
        "\n",
        "    def play_one_episode():\n",
        "        obs, info = env.reset()\n",
        "        terminated = truncated = False\n",
        "        steps = 0\n",
        "        while not (terminated or truncated):\n",
        "            a = agent.choose_action(obs)\n",
        "            obs, r, terminated, truncated, info = env.step(a)\n",
        "            steps += 1\n",
        "            if steps >= max_steps:\n",
        "                # Gymnasium TimeLimit 通常会自己截断；这里兜底\n",
        "                truncated = True\n",
        "        # 成功到达 G 的回报为 1，否则 0\n",
        "        return r\n",
        "\n",
        "    run_rewards = []\n",
        "    for _ in range(n_runs):\n",
        "        successes = sum(play_one_episode() for __ in range(10))\n",
        "        run_rewards.append(successes)\n",
        "\n",
        "    run_rewards = np.array(run_rewards, float)\n",
        "    mean_reward = run_rewards.mean()          # 每 run 的平均成功次数（0~10）\n",
        "    success_rate = mean_reward / 10.0         # 单次尝试的平均成功率（0~1）\n",
        "\n",
        "    print(f\"Runs: {n_runs} | mean_reward_per_run: {mean_reward:.3f} / 10\")\n",
        "    print(f\"Per-attempt success rate: {100*success_rate:.2f}%\")\n",
        "    return mean_reward, success_rate\n",
        "\n",
        "_ = evaluate_agent(n_runs=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIwHdTSKwzQN"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4Q90HZKwzQN",
        "outputId": "5b640962-9f4c-4372-e89a-7689bd2c1a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "agent = Agent(env)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from agent import Agent\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
        "agent = Agent(env)\n",
        "\n",
        "def play_one_episode(max_steps=200):\n",
        "    obs, _ = env.reset()\n",
        "    terminated = truncated = False\n",
        "    steps = 0\n",
        "    r_final = 0.0\n",
        "    while not (terminated or truncated):\n",
        "        a = agent.choose_action(obs)\n",
        "        obs, r, terminated, truncated, _ = env.step(a)\n",
        "        r_final = r          # 只有到终点时这一步是 1，其余都是 0\n",
        "        steps += 1\n",
        "        if steps >= max_steps:\n",
        "            truncated = True\n",
        "    return r_final           # 成功=1，失败=0\n",
        "\n",
        "# —— 按题目：1 次 run = 10 个 episode，奖励=成功次数 ——\n",
        "successes = sum(play_one_episode() for _ in range(10))\n",
        "print(\"Reward for this run (10 tries):\", successes, \"/ 10\")\n"
      ],
      "metadata": {
        "id": "pgoRKpHW0z63",
        "outputId": "88c94d49-8d36-4afa-93de-17486a8b5221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward for this run (10 tries): 8.0 / 10\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
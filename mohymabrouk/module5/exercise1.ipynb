{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {},
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.concat([df_train, df_test], axis=0)\n",
        "\n",
        "df_train.shape\n",
        "\n",
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "    \n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7711aff3",
      "metadata": {},
      "source": [
        "##### Exploring Inconsistencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7d94506",
      "metadata": {},
      "source": [
        "##### Mixed Type Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b675f65f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for column in data.columns:\n",
        "    print(column, data[column].apply(type).unique())\n",
        "\n",
        "print(\"\\nWeather Condition Values\\n\")\n",
        "print(data[\"weather_condition\"].unique())\n",
        "\n",
        "print(\"\\nWind Speed Non str Values\\n\")\n",
        "print(data[\"wind_speed\"][data[\"wind_speed\"].apply(type) == float].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3281f132",
      "metadata": {},
      "source": [
        "- `weather_condition` must be cast into 4 0-1 columns for `['Cloudy' 'Sunny' 'Rainy' 'Snowy' nan]`\n",
        "- `wind_speed` missing values must be handled, must be cast to float"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0125e3b5",
      "metadata": {},
      "source": [
        "##### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da112c5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "missing_counts = data.isnull().sum().sort_values(ascending=False)\n",
        "missing_counts = missing_counts[missing_counts > 0]\n",
        "plt.figure(figsize=(8, 4))\n",
        "missing_counts.plot(kind='bar')\n",
        "plt.title('Bar Chart of Missing Values Count')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.show()\n",
        "\n",
        "print(\"Percentage of missing values per column:\")\n",
        "print((data.isnull().sum() / len(data) * 100).round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb1acc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'humidity', '2000-01-01', '2020-09-01')\n",
        "plot_feature_over_time(data, 'humidity', '2018-01-01', '2018-09-01')\n",
        "plot_feature_over_time(data, 'temperature_station1', '2017-01-01', '2017-09-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37017771",
      "metadata": {},
      "source": [
        "- `temperature_station` missing values can be handled with a 3-nearest average\n",
        "- `weather_condition` should not be too much of a problem if we split in into three 0-1 columns anyway\n",
        "- `humidity` by linear interpolation, aberant values must be trimmed beforehand\n",
        "- `wind_speed` must be cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
      "metadata": {},
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9621c65",
      "metadata": {},
      "source": [
        "- `humidity` missing values : interpolation\n",
        "- `wind_speed` unify units and cast to float\n",
        "- `oil_bent_price_indicator` to ordered category\n",
        "- `temparature_station*` interpolate missing values ?\n",
        "- `date` must be split to reflect seasonal changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['wind_speed']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6faf741",
      "metadata": {},
      "source": [
        "- `wind_speed` unify units and cast to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'electricity_demand', '2017-01-01', '2019-09-07')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80e726f",
      "metadata": {},
      "source": [
        "- `electricity_demand` : trim (far) outliers, interpolate missing or trimmed values "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4fb717",
      "metadata": {},
      "source": [
        "**TODO** : Compare `weather_condition` with `humidity` ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f44de44",
      "metadata": {},
      "source": [
        "#### Summary\n",
        "\n",
        "- `date` must be split to reflect seasonal changes\n",
        "- `weather_condition` must be cast into 4 0-1 columns for `['Cloudy' 'Sunny' 'Rainy' 'Snowy' nan]`, missing values should then not be a major problem\n",
        "- `wind_speed` missing values must be handled, must be cast to float, units unified\n",
        "- `electricity_demand` : trim (far) outliers, interpolate missing or trimmed values \n",
        "- `humidity` by linear interpolation, aberant values must be trimmed beforehand\n",
        "- `oil_brent_price_indicator` to ordered category `['Moderate', 'High', 'Low', 'Very Low', 'Very High']`\n",
        "- `temperature_station` missing values can be handled with a 3-nearest average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebc9cc7",
      "metadata": {},
      "outputs": [],
      "source": [
        "data[\"oil_brent_price_indicator\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {},
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import (\n",
        "    Optional\n",
        ")\n",
        "\n",
        "import re\n",
        "from scipy import stats\n",
        "import datetime\n",
        "\n",
        "# 1. Handle Inconsistencies\n",
        "\n",
        "\n",
        "def _handle_inconsistencies(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Parse and convert wind_speed.\"\"\"\n",
        "    data = data.copy()\n",
        "\n",
        "    wind_speed_regex = r\"^\\s*(\\d+\\.\\d+)\\s*(km\\/h|m\\/s)\\s*$\"\n",
        "\n",
        "    def parse_wind_speed(wind_speed: str | float) -> float:\n",
        "        if not isinstance(wind_speed, str):\n",
        "            return np.nan\n",
        "        result = re.search(wind_speed_regex, wind_speed)\n",
        "        if not result:\n",
        "            return np.nan\n",
        "        value, unit = result.groups()\n",
        "\n",
        "        try:\n",
        "            value = float(value)\n",
        "        except Exception:\n",
        "            return np.nan\n",
        "\n",
        "        match unit:\n",
        "            case \"km/h\":\n",
        "                temporal=value/3\n",
        "                return temporal\n",
        "            case \"m/s\":\n",
        "                return value\n",
        "            case _:\n",
        "                raise Exception(\"Bad regex stupid\")\n",
        "\n",
        "    data[\"wind_speed\"] = data[\"wind_speed\"].apply(parse_wind_speed)\n",
        "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def handle_inconsistencies(X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame | None = None):\n",
        "    if X_val is not None:\n",
        "        return _handle_inconsistencies(X_train), y_train, _handle_inconsistencies(X_val)\n",
        "    else:\n",
        "        return _handle_inconsistencies(X_train), y_train\n",
        "\n",
        "\n",
        "# 2. Handling Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return X_train.copy(), y_train, X_val.copy()\n",
        "    else:\n",
        "        X_train_no_duplicates = X_train.copy()\n",
        "        y_train_no_duplicates = y_train.loc[X_train_no_duplicates.index]\n",
        "        return X_train_no_duplicates, y_train_no_duplicates\n",
        "\n",
        "\n",
        "def _handle_missing_values(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Interpolate all numeric columns.\"\"\"\n",
        "    data = data.copy()\n",
        "    for column in data.select_dtypes(include=\"number\").columns.tolist():\n",
        "        data[column] = data[column].interpolate(limit_direction=\"both\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# 3. Handling Missing Values\n",
        "\n",
        "\n",
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    # Linear interpolation should do it\n",
        "    # weather condition is not handled, we'll do it later\n",
        "    if X_val is not None:\n",
        "        return _handle_missing_values(X_train), _handle_missing_values(X_val)\n",
        "    else:\n",
        "        return _handle_missing_values(X_train)\n",
        "\n",
        "# 4. Handling Categorical Values\n",
        "\n",
        "\n",
        "def _handle_categorical(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Make weather_condition into 4 boolean columns then drop it. Cast oil_brent_price_indicator to ordered categories.\"\"\"\n",
        "    data = data.copy()\n",
        "\n",
        "    data[\"oil_brent_price_indicator\"] = data[\"oil_brent_price_indicator\"].map({\n",
        "        \"Very Low\": 1,\n",
        "        \"Low\": 2,\n",
        "        \"Moderate\": 3,\n",
        "        \"High\": 4,\n",
        "        \"Very High\": 5\n",
        "    })\n",
        "\n",
        "    data[\"sunny\"] = (data[\"weather_condition\"] == \"Sunny\").astype(int)\n",
        "    data[\"cloudy\"] = (data[\"weather_condition\"] == \"Cloudy\").astype(int)\n",
        "    data[\"rainy\"] = (data[\"weather_condition\"] == \"Rainy\").astype(int)\n",
        "    data[\"snowy\"] = (data[\"weather_condition\"] == \"Snowy\").astype(int)\n",
        "    data = data.drop(\"weather_condition\", axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "\n",
        "    if X_val is not None:\n",
        "        return _handle_categorical(X_train), _handle_categorical(X_val)\n",
        "    else:\n",
        "        return _handle_categorical(X_train)\n",
        "\n",
        "# 5. Handling Outliers\n",
        "\n",
        "\n",
        "def _handle_outliers(data: pd.DataFrame | pd.Series) -> pd.DataFrame | pd.Series:\n",
        "    \"\"\"Replace numeric values 5 sigmas from their mean.\"\"\"\n",
        "    data = data.copy()\n",
        "\n",
        "    if isinstance(data, pd.Series):\n",
        "        z_scores = np.abs(stats.zscore(data))\n",
        "        data[z_scores > 5] = data.median()\n",
        "    else:\n",
        "        for column in data.select_dtypes(include=\"number\").columns.tolist():\n",
        "            z_scores = np.abs(stats.zscore(data[column]))\n",
        "            data.loc[z_scores > 5, column] = data[column].median()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "\n",
        "    if X_val is not None:\n",
        "        return _handle_outliers(X_train), _handle_outliers(y_train), _handle_outliers(X_val)\n",
        "    else:\n",
        "        return _handle_outliers(X_train), _handle_outliers(y_train)\n",
        "\n",
        "# 6. Feature Engineering\n",
        "\n",
        "\n",
        "def _feature_engineering(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Encode year's day into a rotating representation. Drop various temperature stations but add their average.\"\"\"\n",
        "    data = data.copy()\n",
        "\n",
        "    data[\"day_sin\"] = np.sin(\n",
        "        2 * np.pi * data[\"date\"].apply(lambda date: date.timetuple().tm_yday) / 366)\n",
        "    data[\"day_cos\"] = np.cos(\n",
        "        2 * np.pi * data[\"date\"].apply(lambda date: date.timetuple().tm_yday) / 366)\n",
        "    data = data.drop(\"date\", axis=1)\n",
        "\n",
        "    data[\"temperature_avg\"] = np.mean(\n",
        "        data[[column for column in data.columns if \"temperature_station\" in column]], axis=1)\n",
        "    data = data.drop(\n",
        "        [column for column in data.columns if \"temperature_station\" in column], axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "    if X_val is not None:\n",
        "        return _feature_engineering(X_train), y_train, _feature_engineering(X_val)\n",
        "    else:\n",
        "        return _feature_engineering(X_train), y_train\n",
        "\n",
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "\n",
        "\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "\n",
        "    if X_val is not None:\n",
        "        return X_train, X_val\n",
        "    else:\n",
        "        return X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6291d038",
      "metadata": {},
      "outputs": [],
      "source": [
        "_data = _feature_engineering(_handle_outliers(_handle_categorical(_handle_missing_values(_handle_inconsistencies(data)))))\n",
        "_data.describe()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### call transformations here, if there is no learning and no need to be crossval\n",
        "    X, y = handle_inconsistencies(X, y)\n",
        "    X, y = handle_duplicates(X, y)\n",
        "    X  = handle_missing_values(X, y)\n",
        "    X = handle_categorical(X, y)\n",
        "    X, y = handle_outliers(X, y)\n",
        "    X, y = feature_engineering(X, y)\n",
        "    X = feature_selection(X, y)\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    \n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "    \n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    \n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {},
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

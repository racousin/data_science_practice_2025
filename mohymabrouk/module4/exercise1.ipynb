{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6d52b-732b-4462-8f1d-75741067ecba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ee8dd-5dae-4900-8f89-2f57a02231ff",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7b6c3-5df9-4f2c-87e9-38d9a812cbcc",
   "metadata": {},
   "source": [
    "### Files sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "19b8c48c-9f5c-4796-b25e-cd52e1a5dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Neighborhood_Market_data.csv from https://www.raphaelcousin.com/modules/data-science-practice/module4/exercise/Neighborhood_Market_data.csv\n",
      "Downloaded module4_exercise_train.zip from https://www.raphaelcousin.com/modules/data-science-practice/module4/exercise/module4_exercise_train.zip\n",
      "Unzipped file module4_exercise_train.zip into current working directory\n"
     ]
    }
   ],
   "source": [
    "# URLs of the files\n",
    "train_datas_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module4/exercise/module4_exercise_train.zip'\n",
    "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module4/exercise/Neighborhood_Market_data.csv'\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(url, file_name):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f'Downloaded {file_name} from {url}')\n",
    "\n",
    "def unzip_file(file_name):\n",
    "    with zipfile.ZipFile(file_name, mode=\"r\") as archive:\n",
    "        archive.extractall()\n",
    "    print(f\"Unzipped file {file_name} into current working directory\")\n",
    "\n",
    "# Downloading the files\n",
    "download_file(test_data_url, 'Neighborhood_Market_data.csv')\n",
    "download_file(train_datas_url, 'module4_exercise_train.zip')\n",
    "unzip_file('module4_exercise_train.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72397636-457a-4f62-818c-18d4967396d1",
   "metadata": {},
   "source": [
    "#### CityMart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa4dce-b3ac-4b52-939c-081445f17494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"CityMart_data.csv\"\n",
    "df_city_mart = pd.read_csv(\"CityMart_data.csv\", index_col=0, parse_dates=[\"last_modified\"], dtype={\"store_name\": \"category\"})\n",
    "df_city_mart.fillna({\"package_volume\": -1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d16f20-40d2-457c-a3fc-5f7a416be0aa",
   "metadata": {},
   "source": [
    "#### Greenfield_Grocers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620b628-b30d-4670-a2d5-6576c009f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \"Greenfield_Grocers_data.csv\"\n",
    "df_greenfield_grocers = pd.read_csv(\"Greenfield_Grocers_data.csv\", delimiter=\"|\", header=3, index_col=0, parse_dates=[\"LAST_MODIFIED\"], dtype={\"STORE_NAME\": \"category\"})\n",
    "df_greenfield_grocers.columns = [name.lower() for name in df_greenfield_grocers.columns]\n",
    "df_greenfield_grocers.drop([\"1\", \"unnamed: 12\"], axis=1, inplace=True)\n",
    "df_greenfield_grocers.dropna(how=\"all\", inplace=True)\n",
    "df_greenfield_grocers.fillna({\"dimension_length\": -1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db4848-6a0f-4742-b4ce-4afe50fd1c09",
   "metadata": {},
   "source": [
    "#### Outlet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7738fac-9711-4cc7-9b12-e564c539ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs_supersaver_outlet =  pd.read_excel(\"SuperSaver_Outlet_data.xlsx\", sheet_name=None)\n",
    "df_supersaver_outlet_quantity = dfs_supersaver_outlet[\"Quantity\"]\n",
    "df_supersaver_outlet_info = dfs_supersaver_outlet[\"Info\"]\n",
    "df_supersaver_outlet_quantity.set_index(\"item_code\", inplace=True)\n",
    "column_names = list(df_supersaver_outlet_info.columns)\n",
    "column_names.pop(0)\n",
    "column_names.append(\"to_be_dropped\")\n",
    "df_supersaver_outlet_info.columns = column_names\n",
    "df_supersaver_outlet_info.drop(\"to_be_dropped\", axis=1, inplace=True)\n",
    "df_supersaver_outlet_info.columns = [name.replace(\" \", \"_\") for name in df_supersaver_outlet_info.columns]\n",
    "df_supersaver_outlet_info.set_index(\"item_code\", inplace=True)\n",
    "df_supersaver_outlet = pd.merge(df_supersaver_outlet_info, df_supersaver_outlet_quantity, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e716a08-25b0-4554-9627-285c4c03212c",
   "metadata": {},
   "source": [
    "#### HighStreet_Bazaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fad164-436e-46db-acba-04a634c0b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_highstreet_bazaar = pd.read_json(\"HighStreet_Bazaar_data.json\")\n",
    "df_highstreet_bazaar.set_index(\"item_code\", inplace=True)\n",
    "df_highstreet_bazaar[\"store_name\"] = df_highstreet_bazaar[\"store_name\"].astype(\"category\")\n",
    "df_highstreet_bazaar[\"last_modified\"] = pd.to_datetime(df_highstreet_bazaar[\"last_modified\"], unit=\"ms\")\n",
    "df_highstreet_bazaar.fillna({\n",
    "    \"days_since_last_purchase\": -1\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851e40e-4644-4b3f-9636-04ec5010ba89",
   "metadata": {},
   "source": [
    "#### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_city_mart, df_greenfield_grocers, df_supersaver_outlet, df_highstreet_bazaar], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9bbff-1f14-48f3-a6f8-76ac701644cf",
   "metadata": {},
   "source": [
    "#### Simple baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "143b8d21-b305-4e5a-a83a-8d1d3359d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "def get_simple_baseline(data, fillna_value=-1, drop_cols=None, k_fold=5, scaler='standard', model='linear', metric='mae', target_col=None, X_data_test=None):\n",
    "    \n",
    "    data = data.copy()\n",
    "    # Handle missing values\n",
    "    data.fillna(fillna_value, inplace=True)\n",
    "    if X_data_test is not None:\n",
    "        X_data_test = X_data_test.copy()\n",
    "        X_data_test.fillna(fillna_value, inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    if drop_cols:\n",
    "        data.drop(drop_cols, axis=1, inplace=True)\n",
    "        if X_data_test is not None:\n",
    "            X_data_test.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    # Split data into features (X) and target (y)\n",
    "    y = data[target_col]\n",
    "    X = data.drop(target_col, axis=1)\n",
    "\n",
    "    # Feature scaling\n",
    "    if scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    if scaler:\n",
    "        X = scaler.fit_transform(X)\n",
    "        if X_data_test is not None:\n",
    "            X_data_test = scaler.transform(X_data_test)\n",
    "\n",
    "    # Initialize the model\n",
    "    if model == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif model == 'logistic':\n",
    "        model = LogisticRegression()\n",
    "    elif model == 'random_forest':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # Train and evaluate using k-fold cross-validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate using the specified metric\n",
    "        if metric == 'mae':\n",
    "            score = mean_absolute_error(y_test, y_pred)\n",
    "        elif metric == 'accuracy':\n",
    "            score = accuracy_score(y_test, np.round(y_pred))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported metric\")\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    if X_data_test is not None:\n",
    "        model.fit(X, y)\n",
    "        return np.mean(scores), model.predict(X_data_test)\n",
    "    \n",
    "    # Return the average score\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7a46a-7c5e-4288-be2d-43269d0e859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_simple_baseline(data, fillna_value=-1, drop_cols=['store_name', 'last_modified'], k_fold=5, scaler='standard', model='linear', metric='mae', target_col='quantity_sold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e0db3-f0c3-4a9e-b032-a355e23e866c",
   "metadata": {},
   "source": [
    "### API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3436ce-88b5-49de-8209-05291c260ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api(endpoint_url):\n",
    "    try:\n",
    "        # Make the GET request to the mock API\n",
    "        response = requests.get(endpoint_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(data[\"message\"])\n",
    "            return data['data']\n",
    "        else:\n",
    "            print(f\"Failed to retrieve volume data. Status code: {response.status_code}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "password = get_api(\"https://www.raphaelcousin.com/api/exercise/auth\")[\"password\"]\n",
    "prices = get_api(f\"https://www.raphaelcousin.com/api/exercise/{password}/prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a0066-38cc-46ce-8bcb-ab70c2ac8bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_prices = pd.DataFrame.from_dict(prices, orient=\"index\", columns=[\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0880c2-2338-4e43-a04c-fdaf90275592",
   "metadata": {},
   "source": [
    "#### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3484b-5ebc-4769-9889-90253a05d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, df_prices, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb325b3-06f6-4c01-965e-08a833ed7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_simple_baseline(data, fillna_value=-1, drop_cols=['store_name', 'last_modified'], k_fold=5, scaler='standard', model='linear', metric='mae', target_col='quantity_sold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500a918-74a0-44eb-8a95-ec35cd3617ae",
   "metadata": {},
   "source": [
    "### Scrapping sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f373c5-452d-408c-a9a4-5e905b361175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the Selenium WebDriver (e.g., Chrome)\n",
    "driver = webdriver.Chrome()  # Make sure ChromeDriver is installed\n",
    "# driver = webdriver.Firefox()\n",
    "# driver = webdriver.Edge()\n",
    "# driver = webdriver.Safari()\n",
    "\n",
    "# Open the URL\n",
    "url = 'https://www.raphaelcousin.com/module4/scrapable-data'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to fully load (increase time if needed)\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the fully rendered page source\n",
    "html = driver.page_source\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "exercise_data = []\n",
    "\n",
    "# Find both tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# Close the Selenium WebDriver\n",
    "driver.quit()\n",
    "\n",
    "def parse_row(cols):\n",
    "    assert len(cols) == 4\n",
    "\n",
    "    return {\n",
    "        \"item_code\": cols[0].text,\n",
    "        \"customer_score\": int(cols[1].text),\n",
    "        \"total_reviews\": int(cols[2].text),\n",
    "        \"updated_timestamp\": int(cols[3].text),\n",
    "    }\n",
    "\n",
    "# Scrape the second table (Exercise Data)\n",
    "course_table = tables[1]\n",
    "for row in course_table.find('tbody').find_all('tr'):\n",
    "    cols = row.find_all('td')\n",
    "    exercise_data.append(parse_row(cols))\n",
    "\n",
    "# Convert the lists to pandas DataFrames\n",
    "df_exercise = pd.DataFrame(exercise_data)\n",
    "df_exercise.set_index(\"item_code\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666fdbc-4735-4758-a3fe-8a1c87eff295",
   "metadata": {},
   "source": [
    "#### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "20d8a96a-aad2-4103-92b8-8635987bfa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, df_exercise, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab523c-53e6-43f7-8602-55bb32592305",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_simple_baseline(data, fillna_value=-1, drop_cols=['store_name', 'last_modified'], k_fold=5, scaler='standard', model='linear', metric='mae', target_col='quantity_sold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032415ae-f5d6-4689-9835-44124fa4afa2",
   "metadata": {},
   "source": [
    "### Generating Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebcf1c-dd7a-4dbd-8b74-326c45c81872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read\n",
    "df_storn =  pd.read_csv(\"Neighborhood_Market_data.csv\", sep=\",\", index_col='item_code')\n",
    "df_storn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada6c87-c1a3-4cfb-bd1c-326908e94276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storn = pd.merge(df_storn, df_prices, left_index=True, right_index=True, how='left')\n",
    "df_storn = pd.merge(df_storn, df_exercise, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5c235-2415-42eb-848e-b1fe02800531",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_loss, x_pred = get_simple_baseline(data, fillna_value=-1, drop_cols=['store_name', 'last_modified'], k_fold=5, scaler=\"standard\", model='linear', metric='mae', target_col='quantity_sold', X_data_test = df_storn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d179ac-e982-4a06-931f-8e6fdf24dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_loss > 20:\n",
    "    print(f\"To fix the x loss: : {x_loss}\")\n",
    "else:\n",
    "    submission = pd.DataFrame({\n",
    "        'item_code': df_storn.index,\n",
    "        'quantity_sold': x_pred\n",
    "    })\n",
    "\n",
    "    submission.to_csv('submission.csv', index=False, sep=',')\n",
    "    submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6314701b-8e9a-4984-be12-6b67ed11eb5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b02c5c8-5383-4f41-8eec-baa16e5b3300",
      "metadata": {},
      "source": [
        "### Data Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850f0188-75e0-4591-bfb2-430be0f5f089",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# URLs of the files\n",
        "train_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_train.csv'\n",
        "test_data_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module5/exercise/module5_exercise_test.csv'\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_name):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure we notice bad responses\n",
        "    with open(file_name, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'Downloaded {file_name} from {url}')\n",
        "\n",
        "# Downloading the files\n",
        "download_file(train_data_url, 'module5_exercise_train.csv')\n",
        "download_file(test_data_url, 'module5_exercise_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec8aa5-d188-407d-8422-cf4d54ccac63",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "df_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60fa867-ddfe-403d-ba84-071792339e6f",
      "metadata": {},
      "source": [
        "### Data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823a4916-1a3a-4f43-989e-4b9441cc142d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#### Make a complete analysis on data preprocessing\n",
        "# Inconsistencies\n",
        "# Duplicates (data.duplicated().sum())\n",
        "# Missing values (data.isnull().sum())\n",
        "# Categorical\n",
        "# Outliers\n",
        "# Feature Engineering\n",
        "# Feature Selection and/or Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2a9ca3-d867-41aa-9cd2-67aadf0df23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.concat([df_train, df_test], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a852e1b0-224e-4db6-921e-3ac3df414bec",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c046b07a-845c-460b-a692-27a97ec3d613",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bed93e3-c3df-44a1-ab90-9b35157ffa24",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_feature_over_time(df, feature, date_id_start, date_id_end):\n",
        "    df_filtered = df[(df['date'] >= date_id_start) & (df['date'] <= date_id_end)]\n",
        "    \n",
        "    if feature not in df_filtered.columns:\n",
        "        print(f\"Feature '{feature}' not found in the DataFrame.\")\n",
        "        return\n",
        "    \n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df_filtered['date'], df_filtered[feature], label=feature, linestyle='-')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(feature)\n",
        "    plt.title(f'{feature} from {date_id_start} to {date_id_end}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fe2013-d460-46c4-a461-b9dfed5478f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['date'] = pd.to_datetime(data['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baeaef1e-284b-416c-9cae-91948a7b6878",
      "metadata": {},
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc88499-aa6c-4bf6-84c6-04a4a266602e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data['wind_speed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec12450c-af79-42c4-9b7e-2ef9a1366fb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'electricity_demand', '2017-01-01', '2019-09-07')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5efed-7530-4934-92ea-60ec12bf00ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_feature_over_time(data, 'humidity', '2016-06-01', '2016-12-01')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead6fd47-1d5a-4e79-8bbc-959bf08f78e8",
      "metadata": {},
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ea769-000a-4897-9a2f-863591bf00d9",
      "metadata": {},
      "source": [
        "### Inconsistencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc86bb0f-6954-4093-bf6c-1f3b98da1079",
      "metadata": {},
      "outputs": [],
      "source": [
        "df= data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c264f6-f371-4e2a-862f-eeaa4220c383",
      "metadata": {},
      "outputs": [],
      "source": [
        "# shape du data\n",
        "print(\"Forme du dataframe\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9416613-4ed5-4cec-b6f9-b0d06f836e7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des 10 premières valeurs\n",
        "print(\"Visualisation des 10 premières valeurs du dataframe\\n\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2acf8ecf-910c-486e-94a0-e7cc10a56b60",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation des 10 dernières valeurs\n",
        "print(\"Visualisation des 10 premières valeurs du dataframe\\n\")\n",
        "df.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9d19d1-f24a-4756-a7c3-8820b53c0553",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualisation de 10 lignes aléatoire\n",
        "print(\"Visualisation de 10 lignes aléatoire\\n\")\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f69787-e504-4615-8aee-d5d454bd1c9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Infos générales sur le data\n",
        "print(\"Infos générales sur le data\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a967e207-d44e-46b4-9b3f-f9802234c348",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistique descriptive des colonnes de type string\n",
        "# count= cardinal de var,unique les valeurs prises,top=valeur la plus fréquente et freq son nbre de frequence\n",
        "df.describe(include=['object'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2812bb4c-8466-4a28-9c26-8b350596b71b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Noms des colonnes de type string\n",
        "nom_cols_string = df.select_dtypes(include=['object', 'string']).columns\n",
        "nom_cols_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65ba527-b3b2-48ae-970d-c68391c1673e",
      "metadata": {},
      "outputs": [],
      "source": [
        "for column in ['weather_condition', 'oil_brent_price_indicator']:\n",
        "    print(f\"{column}: {df[column].nunique()} - {df[column].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be33ea1d-bf97-48e9-a9ad-0264c12d135d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regrouper les valeur d'une variable en fonction des types \n",
        "for col in df.columns:\n",
        "    print(f\"Colonne: {col}\")\n",
        "    print(\"Nombre d'éléments par type de données :\")\n",
        "    print(df[col].apply(type).value_counts())\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea61991-7bfe-4998-8f97-c7162293029a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Affiche toutes les n lignes où la valeur n'est pas un str\n",
        "df[df['wind_speed'].apply(lambda x: not isinstance(x, str))].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79ec26ca-e817-4f1a-b8c6-ed4a59893d4e",
      "metadata": {},
      "source": [
        "### Handling Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74bddaa-0bb7-4e80-bb31-3ffecb00ab34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# nombre de copies uniquement\n",
        "nb_doublons_copies = df.duplicated().sum()\n",
        "print(\"Nombre de doublons (copies uniquement) :\", nb_doublons_copies)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b075d7b-1040-41f8-b978-b3746d6ab30b",
      "metadata": {},
      "source": [
        "### Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850ec992-fd20-44cf-872e-ecc34fd2dc6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nombre de valeurs manquantes par colonne\n",
        "print(\"Nombre de valeurs manquantes par colonne:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb07282-b13f-4fee-bfd3-402d582dbac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nombre de lignes avec au moins une valeur manquante\n",
        "print(\"Nombre de lignes avec au moins une valeur manquante\")\n",
        "df.isnull().any(axis=1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60982bb9-bc05-45a9-bb97-23009a739e01",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Percentage of missing values per column\n",
        "print(\"\\nPercentage of missing values per column:\")\n",
        "percent_missing = df.isnull().mean() * 100\n",
        "print(percent_missing)\n",
        "\n",
        "# Visualizing missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
        "plt.title('Missing Value Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b6e04c5-1faa-49bc-85d1-acff961e04a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# permet de savoir si il faut utiliser le knn\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scatter_matrix(df.select_dtypes(include='number'), figsize=(10, 10), diagonal='hist')\n",
        "plt.suptitle(\"Scatter matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a74f3da-d5aa-43e6-86ca-33ff2d80c6e4",
      "metadata": {},
      "source": [
        "### Handling Categorical Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce6e23e-6c07-4c7c-b313-fc6cf8f57a12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handling Categorical Values\n",
        "for column in ['weather_condition', 'oil_brent_price_indicator']:\n",
        "    print(f\"{column}: {df[column].nunique()} - {df[column].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e0857-e623-422b-a133-74b5b1bc7b7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "weather_to_num = {'Snowy': 0, 'Rainy': 1, 'Cloudy': 2, 'Sunny': 3}\n",
        "oil_indicator_to_num = {'Very Low': 0, 'Low': 1, 'Moderate': 2, 'High': 3, 'Very High': 4}\n",
        "\n",
        "def transfer_categorical(X):\n",
        "    X_new = X.copy()\n",
        "\n",
        "    # Remplacer les NaN par le mode de la colonne\n",
        "    if 'weather_condition' in X_new.columns:\n",
        "        mode_weather = X_new['weather_condition'].mode(dropna=True)[0]\n",
        "        X_new['weather_condition'] = X_new['weather_condition'].fillna(mode_weather)\n",
        "        X_new['weather_condition'] = X_new['weather_condition'].map(lambda x: weather_to_num.get(x, -1))\n",
        "\n",
        "    if 'oil_brent_price_indicator' in X_new.columns:\n",
        "        mode_oil = X_new['oil_brent_price_indicator'].mode(dropna=True)[0]\n",
        "        X_new['oil_brent_price_indicator'] = X_new['oil_brent_price_indicator'].fillna(mode_oil)\n",
        "        X_new['oil_brent_price_indicator'] = X_new['oil_brent_price_indicator'].map(lambda x: oil_indicator_to_num.get(x, -1))\n",
        "\n",
        "    return X_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c946a6ca-4f57-4192-8d9d-9974044fc94a",
      "metadata": {},
      "source": [
        "### Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f9e0130-0d92-492f-a3c9-4b267eedffce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detecte le nbre de valeurs abérantes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d00dd628-b436-4f3b-829d-38b18589a12b",
      "metadata": {},
      "source": [
        "### Data Preprocessing Evaluation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86971ab4-1ef8-464b-afb5-0d750a8c4035",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provide a complete data preprocessing transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c937fe8-f003-4467-a95f-3757820d4d97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Handle Inconsistencies\n",
        "def handle_inconsistencies(X_train, y_train, X_val=None):\n",
        "    \n",
        "    def conversion_km_m(X):\n",
        "        X = X.copy()  # Éviter de modifier le DataFrame original\n",
        "\n",
        "        # Extraire les valeurs numériques et les unités de wind_speed\n",
        "        X['value'] = X['wind_speed'].str.extract(r'([\\d.]+)').astype(float)\n",
        "        X['unit'] = X['wind_speed'].str.extract(r'([a-zA-Z/]+)')\n",
        "\n",
        "        # Convertir toutes les vitesses en m/s\n",
        "        def convert_to_mps(value, unit):\n",
        "            if unit == 'km/h':\n",
        "                return value / 3.6\n",
        "            elif unit == 'm/s':\n",
        "                return value\n",
        "            else:\n",
        "                return None  # ou NaN si tu veux\n",
        "\n",
        "        X['wind_speed'] = X.apply(lambda row: convert_to_mps(row['value'], row['unit']), axis=1)\n",
        "\n",
        "        # Supprimer les colonnes temporaires\n",
        "        X.drop(columns=['value', 'unit'], inplace=True)\n",
        "\n",
        "        return X\n",
        "\n",
        "    # Nettoyage des colonnes catégorielles\n",
        "    for df in [X_train] + ([X_val] if X_val is not None else []):\n",
        "        df['weather_condition'] = df['weather_condition'].astype(str).str.strip().str.lower()\n",
        "        df['oil_brent_price_indicator'] = df['oil_brent_price_indicator'].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # Conversion des vitesses\n",
        "    X_train_clean = conversion_km_m(X_train)\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val_clean = conversion_km_m(X_val)\n",
        "        return X_train_clean, y_train.copy(), X_val_clean\n",
        "    else:\n",
        "        return X_train_clean, y_train.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89c4946-3cb1-45a2-ae5c-57b505966880",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Handling Duplicates\n",
        "def handle_duplicates(X_train, y_train, X_val=None):\n",
        "    # Supprimer les doublons dans X_train\n",
        "    X_train_no_duplicate = X_train.drop_duplicates()\n",
        "    # Garder les mêmes indices dans y_train que dans X_train_no_duplicate\n",
        "    y_train_no_duplicate = y_train.loc[X_train_no_duplicate.index]\n",
        "\n",
        "    if X_val is not None:\n",
        "        # Supprimer les doublons dans X_val\n",
        "        X_val_no_duplicate = X_val.drop_duplicates()\n",
        "        return X_train_no_duplicate, y_train_no_duplicate, X_val_no_duplicate\n",
        "    else:\n",
        "        return X_train_no_duplicate, y_train_no_duplicate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b83dde-8892-4fba-9afa-bd0f78594f90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Handling Missing Values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def handle_missing_values(X_train, y_train, X_val=None):\n",
        "    def impute(X):\n",
        "        # Séparer les colonnes numériques\n",
        "        X_num = X.select_dtypes(include=['number'])\n",
        "        X_cat = X.drop(columns=X_num.columns)  # colonnes non numériques\n",
        "\n",
        "        # Standardisation\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=X_num.columns, index=X.index)\n",
        "\n",
        "        # Imputation KNN\n",
        "        imputer = KNNImputer(n_neighbors=3)\n",
        "        X_imputed_scaled = pd.DataFrame(imputer.fit_transform(X_scaled), columns=X_num.columns, index=X.index)\n",
        "\n",
        "        # Retour à l’échelle originale\n",
        "        X_imputed = pd.DataFrame(scaler.inverse_transform(X_imputed_scaled), columns=X_num.columns, index=X.index)\n",
        "\n",
        "        # Reconstruire le DataFrame avec les colonnes catégorielles\n",
        "        X_new = pd.concat([X_imputed, X_cat], axis=1)\n",
        "\n",
        "        return X_new\n",
        "\n",
        "    # Imputer X_train\n",
        "    X_train = impute(X_train)\n",
        "\n",
        "    # Si X_val existe, l’imputer aussi\n",
        "    if X_val is not None:\n",
        "        X_val = impute(X_val)\n",
        "        return X_train, y_train.copy(), X_val\n",
        "    else:\n",
        "        return X_train, y_train.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62193821-7d65-40a3-8225-58044bf7d472",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def handle_categorical(X_train, y_train, X_val=None):\n",
        "    X_train = X_train.copy()\n",
        "    X_train = transfer_categorical(X_train)\n",
        "\n",
        "    if X_val is not None:\n",
        "        X_val = X_val.copy()\n",
        "        X_val = transfer_categorical(X_val)\n",
        "        \n",
        "        return X_train, X_val\n",
        "    else:\n",
        "        return X_train\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e56949-62a1-4115-aadf-db6c8448f069",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Handling Outliers\n",
        "def handle_outliers(X_train, y_train, X_val=None):\n",
        "    def find_outliers_iqr(series, threshold=1.5):\n",
        "        \"\"\"\n",
        "        Détecte les outliers d'une série avec la méthode IQR.\n",
        "        Retourne les index (et non les positions).\n",
        "        \"\"\"\n",
        "        series = pd.Series(series).dropna()\n",
        "        q1 = np.percentile(series, 25)\n",
        "        q3 = np.percentile(series, 75)\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - threshold * iqr\n",
        "        upper = q3 + threshold * iqr\n",
        "        return series[(series < lower) | (series > upper)].index\n",
        "\n",
        "    # Colonnes numériques à traiter\n",
        "    selected_columns = ['humidity', 'wind_speed', 'temperature_station1',\n",
        "        'temperature_station2', 'temperature_station3', 'temperature_station4',\n",
        "        'temperature_station5', 'temperature_station6', 'temperature_station7',\n",
        "        'temperature_station8', 'temperature_station9', 'temperature_station10']\n",
        "\n",
        "    # Collecte des index à supprimer\n",
        "    outlier_indices = pd.Index([])\n",
        "\n",
        "    for col in selected_columns:\n",
        "        if col in X_train.columns:\n",
        "            outlier_indices = outlier_indices.union(find_outliers_iqr(X_train[col]))\n",
        "\n",
        "    # Ajouter les outliers dans y_train\n",
        "    outlier_indices = outlier_indices.union(find_outliers_iqr(y_train))\n",
        "\n",
        "    # Supprimer les lignes correspondantes\n",
        "    X_train_cleaned = X_train.drop(index=outlier_indices, errors='ignore')\n",
        "    y_train_cleaned = y_train.drop(index=outlier_indices, errors='ignore')\n",
        "\n",
        "    if X_val is not None:\n",
        "        return X_train_cleaned, y_train_cleaned, X_val.copy()\n",
        "    else:\n",
        "        return X_train_cleaned, y_train_cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d95a2e-f958-47a9-b051-6881965dd58b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Feature Engineering\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "def feature_engineering(X_train, y_train, X_val=None):\n",
        "\n",
        "    def add_time_features(data):\n",
        "        new_data = data.copy()\n",
        "        new_data['date'] = pd.to_datetime(new_data['date'], errors='coerce')  # robustesse\n",
        "        new_data['dayofyear'] = new_data['date'].dt.dayofyear\n",
        "        new_data['month'] = new_data['date'].dt.month\n",
        "        new_data['day'] = new_data['date'].dt.day\n",
        "        new_data['hour'] = new_data['date'].dt.hour\n",
        "        new_data['dayofweek'] = new_data['date'].dt.dayofweek\n",
        "        new_data['is_weekend'] = new_data['dayofweek'].isin([5, 6]).astype(int)\n",
        "        new_data['week_of_year'] = new_data['date'].dt.isocalendar().week.astype(int)\n",
        "        new_data['month_progress'] = new_data['date'].dt.day / new_data['date'].dt.days_in_month\n",
        "        return new_data\n",
        "\n",
        "    def add_aggregate_features(data):\n",
        "        new_data = data.copy()\n",
        "\n",
        "        temp_cols = [f'temperature_station{i}' for i in range(1, 11)]\n",
        "        new_data['avg_temp'] = new_data[temp_cols].mean(axis=1)\n",
        "\n",
        "        # Polynomial features\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "        try:\n",
        "            poly_features = poly.fit_transform(new_data[['avg_temp', 'humidity']])\n",
        "            poly_feature_names = poly.get_feature_names_out(['avg_temp', 'humidity'])\n",
        "            new_data[poly_feature_names] = poly_features\n",
        "        except KeyError:\n",
        "            print(\"⚠️ Colonne(s) manquante(s) pour les features polynomiales : 'avg_temp' ou 'humidity'\")\n",
        "\n",
        "        # Log de wind_speed\n",
        "        if 'wind_speed' in new_data.columns:\n",
        "            new_data['log_windspeed'] = np.log1p(new_data['wind_speed'])\n",
        "        return new_data\n",
        "\n",
        "    # Apply to training data\n",
        "    X_train_new = X_train.copy()\n",
        "    X_train_new = add_time_features(X_train_new)\n",
        "    X_train_new = add_aggregate_features(X_train_new)\n",
        "\n",
        "    # Apply to validation data (if exists)\n",
        "    if X_val is not None:\n",
        "        X_val_new = X_val.copy()\n",
        "        X_val_new = add_time_features(X_val_new)\n",
        "        X_val_new = add_aggregate_features(X_val_new)\n",
        "        return X_train_new, y_train.copy(), X_val_new\n",
        "    else:\n",
        "        return X_train_new, y_train.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c0d2c71-4cc8-4b7c-855b-9cfa19106d1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Feature Selection and Dimensionality Reduction\n",
        "def feature_selection(X_train, y_train, X_val=None):\n",
        "    expected_columns = [\n",
        "        'humidity', 'weather_condition', 'wind_speed', 'oil_brent_price_indicator',\n",
        "        'temperature_station1', 'temperature_station2', 'temperature_station3',\n",
        "        'temperature_station4', 'temperature_station5', 'temperature_station6',\n",
        "        'temperature_station7', 'temperature_station8', 'temperature_station9', 'temperature_station10',\n",
        "        'dayofyear', 'month', 'day', 'hour', 'dayofweek', 'is_weekend',\n",
        "        'week_of_year', 'month_progress', 'avg_temp', 'avg_temp^2',\n",
        "        'avg_temp humidity', 'humidity^2', 'log_windspeed'\n",
        "    ]\n",
        "\n",
        "    # Filtrer seulement les colonnes existantes\n",
        "    available_columns = [col for col in expected_columns if col in X_train.columns]\n",
        "\n",
        "    if X_val is not None:\n",
        "        # S'assurer que X_val a aussi les colonnes disponibles\n",
        "        common_columns = [col for col in available_columns if col in X_val.columns]\n",
        "        return X_train[common_columns].copy(), X_val[common_columns].copy()\n",
        "    else:\n",
        "        return X_train[available_columns].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b18081c-17f2-4809-bdf6-7181aac77199",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_pipeline(X, y, n_splits=5):\n",
        "\n",
        "    ### call transformations here, if there is no learning and no need to be crossval\n",
        "    X, y = handle_inconsistencies(X, y)\n",
        "    X, y = handle_duplicates(X, y)\n",
        "    X, y = handle_missing_values(X, y)\n",
        "    X = handle_categorical(X, y)  # On réutilise bien X\n",
        "    X, y = handle_outliers(X, y)\n",
        "    X, y = feature_engineering(X, y)\n",
        "    X = feature_selection(X, y)\n",
        "\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    \n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    \n",
        "    train_scores = []\n",
        "    val_scores = []\n",
        "    \n",
        "    for fold, (train_index, val_index) in enumerate(tscv.split(X)):\n",
        "        print(f\"Processing fold {fold + 1}/{n_splits}...\")\n",
        "        \n",
        "        # Split data into train and validation sets\n",
        "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
        "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
        "\n",
        "        ### call transformations here, if there is learning\n",
        "        # X_train, y_train, X_val = handle_inconsistencies(X_train, y_train, X_val)\n",
        "        #X_train, y_train, X_val = handle_duplicates(X_train, y_train, X_val)\n",
        "        # X_train, X_val = handle_missing_values(X_train, y_train, X_val)\n",
        "        #X_train, X_val = handle_categorical(X_train, y_train, X_val)\n",
        "        # X_train, y_train, X_val = handle_outliers(X_train, y_train, X_val)\n",
        "        #X_train, y_train, X_val = feature_engineering(X_train, y_train, X_val)\n",
        "        # X_train, X_val = feature_selection(X_train, y_train, X_val)\n",
        "        \n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Predict on training set\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        train_scores.append(train_mse)\n",
        "        \n",
        "        # Predict on validation set\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "        val_scores.append(val_mse)\n",
        "        \n",
        "        print(f\"Fold {fold + 1} Train MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}\")\n",
        "    \n",
        "    # Compute mean, max, and min values for train and validation MSE\n",
        "    mean_train_mse = np.mean(train_scores)\n",
        "    max_train_mse = np.max(train_scores)\n",
        "    min_train_mse = np.min(train_scores)\n",
        "    \n",
        "    mean_val_mse = np.mean(val_scores)\n",
        "    max_val_mse = np.max(val_scores)\n",
        "    min_val_mse = np.min(val_scores)\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nTrain MSE:\")\n",
        "    print(f\"Mean: {mean_train_mse:.4f}, Max: {max_train_mse:.4f}, Min: {min_train_mse:.4f}\")\n",
        "    \n",
        "    print(\"\\nValidation MSE:\")\n",
        "    print(f\"Mean: {mean_val_mse:.4f}, Max: {max_val_mse:.4f}, Min: {min_val_mse:.4f}\")\n",
        "    \n",
        "    return mean_val_mse  # Return mean validation MSE as the overall score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe454f33-ddd9-4e94-aa65-6da0e6b7b8a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67a4532-14bc-4590-90ed-d39044dfc6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X = df_train.copy().drop(columns=['electricity_demand'], axis=1)\n",
        "y = df_train.copy().pop('electricity_demand')\n",
        "\n",
        "# Run the evaluation\n",
        "evaluate_pipeline(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e27e3b-7641-4107-be8c-50104d473cd9",
      "metadata": {},
      "source": [
        "### Generating Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49733a5-e2f6-4839-8063-2a6f5b2dfc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and submit your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81488d3e-2dde-4904-ac69-430e55df0cc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X_train and y_train from your data\n",
        "df_train =  pd.read_csv(\"module5_exercise_train.csv\", sep=\",\")\n",
        "\n",
        "X_train = df_train.drop(columns=['electricity_demand'], axis=1)\n",
        "y_train = df_train['electricity_demand']\n",
        "\n",
        "X_test =  pd.read_csv(\"module5_exercise_test.csv\", sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f558b85-7970-4c24-95a8-fd6a37da930b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_predict_to_submit(X_train, y_train, X_test):\n",
        "    model = LinearRegression()\n",
        "    \n",
        "    X_train, y_train, X_test = handle_inconsistencies(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_duplicates(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_missing_values(X_train, y_train, X_test)\n",
        "    X_train, X_test = handle_categorical(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = handle_outliers(X_train, y_train, X_test)\n",
        "    X_train, y_train, X_test = feature_engineering(X_train, y_train, X_test)\n",
        "    X_train, X_test = feature_selection(X_train, y_train, X_test)\n",
        "\n",
        "    # Train the model on the entire training set\n",
        "    print(f\"Training model on entire dataset of shape: {X_train.shape}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predict on the test set\n",
        "    print(f\"Predicting on test dataset of shape: {X_test.shape}\")\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a7efc0-16fa-41f9-a8d9-6e90ba3c8bb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call serve_model to train and predict\n",
        "y_test_pred = train_and_predict_to_submit(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538cf936-7872-46ad-b02f-422a0aec3806",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating Submission File\n",
        "submission = pd.DataFrame({\n",
        "    'date': X_test['date'],\n",
        "    'electricity_demand': y_test_pred\n",
        "})\n",
        "\n",
        "# Save the submission file\n",
        "submission.to_csv('submission.csv', index=False, sep=',')\n",
        "print(\"Submission file saved as 'submission.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbcf48b0-a285-4286-ba28-decf2bdd1ccf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa467f09-ea48-4d24-adfd-25d620962608",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970512be-dde5-41dc-8e68-918c80e7601f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

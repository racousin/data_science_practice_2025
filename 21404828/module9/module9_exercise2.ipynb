{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVgWUZjpb137"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "  observation is position, given by current_row * ncols + current_col\n",
        "  reward is 0 if not on end 1 otherwise\n",
        "\n",
        "  action :  0: Move left\n",
        "            1: Move down\n",
        "            2: Move right\n",
        "            3: Move up\n",
        "            If on an left edge and select, do not move, except if random sliding"
      ],
      "metadata": {
        "id": "kGHsB33d90Qf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcRHqUqmuoDK"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_Q(\n",
        "    env,\n",
        "    episodes=100000,\n",
        "    learning_rate=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01,\n",
        "    verbose=True,\n",
        "    Q=None\n",
        "):\n",
        "    if Q is None:\n",
        "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    else:\n",
        "        Q = np.copy(Q)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        reward = 0\n",
        "\n",
        "        observation, _ = env.reset()\n",
        "        action_reward, terminated, truncated, info = None, False, False, None\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            if np.random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[observation])\n",
        "\n",
        "            next_observation, action_reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            q = action_reward + (0 if (terminated or truncated) else gamma * np.max(Q[next_observation]))\n",
        "            Q[observation, action] += learning_rate * (q - Q[observation, action])\n",
        "\n",
        "            observation = next_observation\n",
        "            reward += action_reward\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(rewards[-1000:])\n",
        "            print(f\"episode={episode + 1} reward={avg_reward:.3f} (epsilon={epsilon:.3f})\")\n",
        "\n",
        "    env.close()\n",
        "    return Q, np.mean(rewards[-1000:])"
      ],
      "metadata": {
        "id": "nScY0tu1ntgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def optimize_learn_Q(\n",
        "    env,\n",
        "    episodes=10000,\n",
        "    *,\n",
        "    learning_rate,\n",
        "    gamma,\n",
        "    epsilon,\n",
        "    epsilon_decay,\n",
        "    epsilon_min,\n",
        "    verbose=True\n",
        "):\n",
        "    best_params = None\n",
        "    best_reward = 0\n",
        "    best_Q = None\n",
        "\n",
        "    for candidate_learning_rate, candidate_gamma, candidate_epsilon, candidate_epsilon_decay, candidate_epsilon_min in itertools.product(learning_rate, gamma, epsilon, epsilon_decay, epsilon_min):\n",
        "        params=dict(\n",
        "            learning_rate=candidate_learning_rate,\n",
        "            gamma=candidate_gamma,\n",
        "            epsilon=candidate_epsilon,\n",
        "            epsilon_decay=candidate_epsilon_decay,\n",
        "            epsilon_min=candidate_epsilon_min\n",
        "        )\n",
        "\n",
        "        Q, reward = learn_Q(\n",
        "            env,\n",
        "            episodes=episodes,\n",
        "            verbose=False,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        if reward > best_reward:\n",
        "          best_reward = reward\n",
        "          best_Q = Q\n",
        "          best_params = params\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"reward={reward:.4f} {repr(params)}\")\n",
        "\n",
        "    return best_Q, best_reward, best_params\n"
      ],
      "metadata": {
        "id": "y88lzoOOw8N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_Q(\n",
        "    env,\n",
        "    Q,\n",
        "    n=100,\n",
        "    lives=10\n",
        "):\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(n):\n",
        "        reward = 0\n",
        "\n",
        "        for life in range(lives):\n",
        "            observation, _ = env.reset()\n",
        "            action_reward, terminated, truncated, info = None, False, False, None\n",
        "\n",
        "            while not (terminated or truncated):\n",
        "                action = np.argmax(Q[observation])\n",
        "                observation, action_reward, terminated, truncated, info = env.step(action)\n",
        "                reward += action_reward\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return np.mean(rewards), np.std(rewards)\n",
        "\n"
      ],
      "metadata": {
        "id": "WtmPUEdCpzJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
        "\n",
        "# Q = learn_Q(env, episodes=200000, epsilon_decay=0.999975)\n",
        "\n",
        "# Q, reward, params = optimize_learn_Q(\n",
        "#     env,\n",
        "#     learning_rate=[0.05, 0.1, 0.15, 0.2],\n",
        "#     gamma=[0.9, 0.95, 0.99],\n",
        "#     epsilon=[1.0],\n",
        "#     epsilon_decay=[0.9999, 0.99995, 0.999975],\n",
        "#     epsilon_min=[0.01],\n",
        "# )\n",
        "\n",
        "# Q, reward, params = optimize_learn_Q(\n",
        "#     env,\n",
        "#     learning_rate=[0.01, 0.025, 0.05, 0.075],\n",
        "#     gamma=[0.97],\n",
        "#     epsilon=[1.0],\n",
        "#     epsilon_decay=[0.9999],\n",
        "#     epsilon_min=[0.01],\n",
        "# )\n",
        "\n",
        "Q, reward = learn_Q(env, episodes=50000, epsilon_decay=0.9999, learning_rate=0.075, gamma=0.97)\n"
      ],
      "metadata": {
        "id": "rRh1nwmVqxlm",
        "outputId": "c465be0c-0b10-4b31-993a-1b2bec91cf52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_Q(env, Q)"
      ],
      "metadata": {
        "id": "7TF11NPaweV7",
        "outputId": "2702cde6-798e-47a7-df74-24a9f174a0ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_Q = Q\n",
        "\n",
        "Q, reward = learn_Q(env, episodes=50000, epsilon=0.25, epsilon_decay=0.9999, learning_rate=0.075, gamma=0.97, Q=best_Q)"
      ],
      "metadata": {
        "id": "FzmkChHH4TxC",
        "outputId": "4f266f09-cce3-49fe-bc85-564f486adca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_Q(env, Q)"
      ],
      "metadata": {
        "id": "KbLL6GhKwzZ0",
        "outputId": "4ab6163d-f18c-455f-cabe-b01369cca6be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"Q.npy\", Q)"
      ],
      "metadata": {
        "id": "N-Iz6lrf5M88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.Q = np.load(\"Q.npy\")\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info = None):\n",
        "        return np.argmax(self.Q[observation])\n",
        "\n"
      ],
      "metadata": {
        "id": "tnCqc9AJwPuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFKW8mEuoDL"
      },
      "source": [
        "### Description\n",
        "\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
        "\n",
        "Holes in the ice are distributed in set locations.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
        "\n",
        "The environment is based on :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-OCGL5EuoDL"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrdAtvs5uoDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l53utGduoDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbk7EDJ0uoDM"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jb3w9PluoDM",
        "outputId": "847185d3-5cf7-4e9d-9979-55ada9882e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "\n",
        "agent = Agent(env)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    print(action)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

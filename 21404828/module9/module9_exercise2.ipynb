{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAUjjhOb136"
      },
      "source": [
        "### Run in collab\n",
        "<a href=\"https://colab.research.google.com/github/racousin/data_science_practice/blob/master/website/public/modules/data-science-practice/module9/exercise/module9_exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uVgWUZjpb137"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install swig==4.2.1\n",
        "!pip install gymnasium==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Oa03cAjLb138"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJZwAAf2b139"
      },
      "source": [
        "# module9_exercise2 : ML - Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "  observation is position, given by current_row * ncols + current_col\n",
        "  reward is 0 if not on end 1 otherwise\n",
        "\n",
        "  action :  0: Move left\n",
        "            1: Move down\n",
        "            2: Move right\n",
        "            3: Move up\n",
        "            If on an left edge and select, do not move, except if random sliding"
      ],
      "metadata": {
        "id": "kGHsB33d90Qf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQQPZhpb139"
      },
      "source": [
        "### Objective\n",
        "Get at list an agent running on ML-Arena <a href=\"https://ml-arena.com/viewcompetition/5\" target=\"_blank\"> FrozenLake Competition</a> with mean reward upper than 0.35 (ie 35%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcRHqUqmuoDK"
      },
      "source": [
        "You should submit an agent file named `agent.py` with a class `Agent` that includes at least the following attributes:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_Q(\n",
        "    env,\n",
        "    episodes=100000,\n",
        "    learning_rate=0.1,\n",
        "    gamma=0.99,\n",
        "    epsilon=1.0,\n",
        "    epsilon_decay=0.995,\n",
        "    epsilon_min=0.01,\n",
        "    verbose=True,\n",
        "    Q=None\n",
        "):\n",
        "    if Q is None:\n",
        "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    else:\n",
        "        Q = np.copy(Q)\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        reward = 0\n",
        "\n",
        "        observation, _ = env.reset()\n",
        "        action_reward, terminated, truncated, info = None, False, False, None\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            if np.random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(Q[observation])\n",
        "\n",
        "            next_observation, action_reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            q = action_reward + (0 if (terminated or truncated) else gamma * np.max(Q[next_observation]))\n",
        "            Q[observation, action] += learning_rate * (q - Q[observation, action])\n",
        "\n",
        "            observation = next_observation\n",
        "            reward += action_reward\n",
        "\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if verbose and (episode + 1) % 1000 == 0:\n",
        "            avg_reward = np.mean(rewards[-1000:])\n",
        "            print(f\"episode={episode + 1} reward={avg_reward:.3f} (epsilon={epsilon:.3f})\")\n",
        "\n",
        "    env.close()\n",
        "    return Q, np.mean(rewards[-1000:])"
      ],
      "metadata": {
        "id": "nScY0tu1ntgu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "def optimize_learn_Q(\n",
        "    env,\n",
        "    episodes=10000,\n",
        "    *,\n",
        "    learning_rate,\n",
        "    gamma,\n",
        "    epsilon,\n",
        "    epsilon_decay,\n",
        "    epsilon_min,\n",
        "    verbose=True\n",
        "):\n",
        "    best_params = None\n",
        "    best_reward = 0\n",
        "    best_Q = None\n",
        "\n",
        "    for candidate_learning_rate, candidate_gamma, candidate_epsilon, candidate_epsilon_decay, candidate_epsilon_min in itertools.product(learning_rate, gamma, epsilon, epsilon_decay, epsilon_min):\n",
        "        params=dict(\n",
        "            learning_rate=candidate_learning_rate,\n",
        "            gamma=candidate_gamma,\n",
        "            epsilon=candidate_epsilon,\n",
        "            epsilon_decay=candidate_epsilon_decay,\n",
        "            epsilon_min=candidate_epsilon_min\n",
        "        )\n",
        "\n",
        "        Q, reward = learn_Q(\n",
        "            env,\n",
        "            episodes=episodes,\n",
        "            verbose=False,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        if reward > best_reward:\n",
        "          best_reward = reward\n",
        "          best_Q = Q\n",
        "          best_params = params\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"reward={reward:.4f} {repr(params)}\")\n",
        "\n",
        "    return best_Q, best_reward, best_params\n"
      ],
      "metadata": {
        "id": "y88lzoOOw8N_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_Q(\n",
        "    env,\n",
        "    Q,\n",
        "    n=100,\n",
        "    lives=10\n",
        "):\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(n):\n",
        "        reward = 0\n",
        "\n",
        "        for life in range(lives):\n",
        "            observation, _ = env.reset()\n",
        "            action_reward, terminated, truncated, info = None, False, False, None\n",
        "\n",
        "            while not (terminated or truncated):\n",
        "                action = np.argmax(Q[observation])\n",
        "                observation, action_reward, terminated, truncated, info = env.step(action)\n",
        "                reward += action_reward\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return np.mean(rewards), np.std(rewards)\n",
        "\n"
      ],
      "metadata": {
        "id": "WtmPUEdCpzJi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\")\n",
        "\n",
        "# Q = learn_Q(env, episodes=200000, epsilon_decay=0.999975)\n",
        "\n",
        "# Q, reward, params = optimize_learn_Q(\n",
        "#     env,\n",
        "#     learning_rate=[0.05, 0.1, 0.15, 0.2],\n",
        "#     gamma=[0.9, 0.95, 0.99],\n",
        "#     epsilon=[1.0],\n",
        "#     epsilon_decay=[0.9999, 0.99995, 0.999975],\n",
        "#     epsilon_min=[0.01],\n",
        "# )\n",
        "\n",
        "# Q, reward, params = optimize_learn_Q(\n",
        "#     env,\n",
        "#     learning_rate=[0.01, 0.025, 0.05, 0.075],\n",
        "#     gamma=[0.97],\n",
        "#     epsilon=[1.0],\n",
        "#     epsilon_decay=[0.9999],\n",
        "#     epsilon_min=[0.01],\n",
        "# )\n",
        "\n",
        "Q, reward = learn_Q(env, episodes=50000, epsilon_decay=0.9999, learning_rate=0.075, gamma=0.97)\n"
      ],
      "metadata": {
        "id": "rRh1nwmVqxlm",
        "outputId": "c465be0c-0b10-4b31-993a-1b2bec91cf52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode=1000 reward=0.003 (epsilon=0.905)\n",
            "episode=2000 reward=0.003 (epsilon=0.819)\n",
            "episode=3000 reward=0.009 (epsilon=0.741)\n",
            "episode=4000 reward=0.011 (epsilon=0.670)\n",
            "episode=5000 reward=0.016 (epsilon=0.607)\n",
            "episode=6000 reward=0.017 (epsilon=0.549)\n",
            "episode=7000 reward=0.038 (epsilon=0.497)\n",
            "episode=8000 reward=0.035 (epsilon=0.449)\n",
            "episode=9000 reward=0.069 (epsilon=0.407)\n",
            "episode=10000 reward=0.065 (epsilon=0.368)\n",
            "episode=11000 reward=0.090 (epsilon=0.333)\n",
            "episode=12000 reward=0.114 (epsilon=0.301)\n",
            "episode=13000 reward=0.130 (epsilon=0.273)\n",
            "episode=14000 reward=0.118 (epsilon=0.247)\n",
            "episode=15000 reward=0.168 (epsilon=0.223)\n",
            "episode=16000 reward=0.197 (epsilon=0.202)\n",
            "episode=17000 reward=0.206 (epsilon=0.183)\n",
            "episode=18000 reward=0.189 (epsilon=0.165)\n",
            "episode=19000 reward=0.235 (epsilon=0.150)\n",
            "episode=20000 reward=0.228 (epsilon=0.135)\n",
            "episode=21000 reward=0.277 (epsilon=0.122)\n",
            "episode=22000 reward=0.312 (epsilon=0.111)\n",
            "episode=23000 reward=0.294 (epsilon=0.100)\n",
            "episode=24000 reward=0.313 (epsilon=0.091)\n",
            "episode=25000 reward=0.333 (epsilon=0.082)\n",
            "episode=26000 reward=0.297 (epsilon=0.074)\n",
            "episode=27000 reward=0.370 (epsilon=0.067)\n",
            "episode=28000 reward=0.357 (epsilon=0.061)\n",
            "episode=29000 reward=0.396 (epsilon=0.055)\n",
            "episode=30000 reward=0.405 (epsilon=0.050)\n",
            "episode=31000 reward=0.379 (epsilon=0.045)\n",
            "episode=32000 reward=0.389 (epsilon=0.041)\n",
            "episode=33000 reward=0.456 (epsilon=0.037)\n",
            "episode=34000 reward=0.423 (epsilon=0.033)\n",
            "episode=35000 reward=0.419 (epsilon=0.030)\n",
            "episode=36000 reward=0.473 (epsilon=0.027)\n",
            "episode=37000 reward=0.443 (epsilon=0.025)\n",
            "episode=38000 reward=0.401 (epsilon=0.022)\n",
            "episode=39000 reward=0.473 (epsilon=0.020)\n",
            "episode=40000 reward=0.472 (epsilon=0.018)\n",
            "episode=41000 reward=0.514 (epsilon=0.017)\n",
            "episode=42000 reward=0.406 (epsilon=0.015)\n",
            "episode=43000 reward=0.511 (epsilon=0.014)\n",
            "episode=44000 reward=0.525 (epsilon=0.012)\n",
            "episode=45000 reward=0.469 (epsilon=0.011)\n",
            "episode=46000 reward=0.560 (epsilon=0.010)\n",
            "episode=47000 reward=0.478 (epsilon=0.010)\n",
            "episode=48000 reward=0.520 (epsilon=0.010)\n",
            "episode=49000 reward=0.494 (epsilon=0.010)\n",
            "episode=50000 reward=0.543 (epsilon=0.010)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_Q(env, Q)"
      ],
      "metadata": {
        "id": "7TF11NPaweV7",
        "outputId": "2702cde6-798e-47a7-df74-24a9f174a0ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(3.93), np.float64(1.5378881623837282))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_Q = Q\n",
        "\n",
        "Q, reward = learn_Q(env, episodes=50000, epsilon=0.25, epsilon_decay=0.9999, learning_rate=0.075, gamma=0.97, Q=best_Q)"
      ],
      "metadata": {
        "id": "FzmkChHH4TxC",
        "outputId": "4f266f09-cce3-49fe-bc85-564f486adca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode=1000 reward=0.166 (epsilon=0.226)\n",
            "episode=2000 reward=0.141 (epsilon=0.205)\n",
            "episode=3000 reward=0.174 (epsilon=0.185)\n",
            "episode=4000 reward=0.211 (epsilon=0.168)\n",
            "episode=5000 reward=0.275 (epsilon=0.152)\n",
            "episode=6000 reward=0.235 (epsilon=0.137)\n",
            "episode=7000 reward=0.288 (epsilon=0.124)\n",
            "episode=8000 reward=0.243 (epsilon=0.112)\n",
            "episode=9000 reward=0.281 (epsilon=0.102)\n",
            "episode=10000 reward=0.296 (epsilon=0.092)\n",
            "episode=11000 reward=0.322 (epsilon=0.083)\n",
            "episode=12000 reward=0.295 (epsilon=0.075)\n",
            "episode=13000 reward=0.395 (epsilon=0.068)\n",
            "episode=14000 reward=0.343 (epsilon=0.062)\n",
            "episode=15000 reward=0.378 (epsilon=0.056)\n",
            "episode=16000 reward=0.396 (epsilon=0.050)\n",
            "episode=17000 reward=0.422 (epsilon=0.046)\n",
            "episode=18000 reward=0.391 (epsilon=0.041)\n",
            "episode=19000 reward=0.452 (epsilon=0.037)\n",
            "episode=20000 reward=0.407 (epsilon=0.034)\n",
            "episode=21000 reward=0.365 (epsilon=0.031)\n",
            "episode=22000 reward=0.422 (epsilon=0.028)\n",
            "episode=23000 reward=0.482 (epsilon=0.025)\n",
            "episode=24000 reward=0.472 (epsilon=0.023)\n",
            "episode=25000 reward=0.489 (epsilon=0.021)\n",
            "episode=26000 reward=0.435 (epsilon=0.019)\n",
            "episode=27000 reward=0.534 (epsilon=0.017)\n",
            "episode=28000 reward=0.486 (epsilon=0.015)\n",
            "episode=29000 reward=0.525 (epsilon=0.014)\n",
            "episode=30000 reward=0.483 (epsilon=0.012)\n",
            "episode=31000 reward=0.498 (epsilon=0.011)\n",
            "episode=32000 reward=0.514 (epsilon=0.010)\n",
            "episode=33000 reward=0.448 (epsilon=0.010)\n",
            "episode=34000 reward=0.507 (epsilon=0.010)\n",
            "episode=35000 reward=0.512 (epsilon=0.010)\n",
            "episode=36000 reward=0.523 (epsilon=0.010)\n",
            "episode=37000 reward=0.490 (epsilon=0.010)\n",
            "episode=38000 reward=0.410 (epsilon=0.010)\n",
            "episode=39000 reward=0.471 (epsilon=0.010)\n",
            "episode=40000 reward=0.529 (epsilon=0.010)\n",
            "episode=41000 reward=0.455 (epsilon=0.010)\n",
            "episode=42000 reward=0.532 (epsilon=0.010)\n",
            "episode=43000 reward=0.557 (epsilon=0.010)\n",
            "episode=44000 reward=0.538 (epsilon=0.010)\n",
            "episode=45000 reward=0.547 (epsilon=0.010)\n",
            "episode=46000 reward=0.531 (epsilon=0.010)\n",
            "episode=47000 reward=0.448 (epsilon=0.010)\n",
            "episode=48000 reward=0.492 (epsilon=0.010)\n",
            "episode=49000 reward=0.563 (epsilon=0.010)\n",
            "episode=50000 reward=0.506 (epsilon=0.010)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_Q(env, Q)"
      ],
      "metadata": {
        "id": "KbLL6GhKwzZ0",
        "outputId": "4ab6163d-f18c-455f-cabe-b01369cca6be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(5.68), np.float64(1.5612815249018992))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"Q.npy\", Q)"
      ],
      "metadata": {
        "id": "N-Iz6lrf5M88"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.Q = np.load(\"Q.npy\")\n",
        "\n",
        "    def choose_action(self, observation, reward=0.0, terminated=False, truncated=False, info = None):\n",
        "        return np.argmax(self.Q[observation])\n",
        "\n"
      ],
      "metadata": {
        "id": "tnCqc9AJwPuS"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGFKW8mEuoDL"
      },
      "source": [
        "### Description\n",
        "\n",
        "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world [7,7].\n",
        "\n",
        "Holes in the ice are distributed in set locations.\n",
        "\n",
        "The player makes moves until they reach the goal or fall in a hole.\n",
        "\n",
        "Each run will consist of 10 attempts to cross the ice. The reward will be the total amount accumulated during those trips. For example, if your agent reaches the goal 3 times out of 10, its reward will be 3.\n",
        "\n",
        "The environment is based on :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U-OCGL5EuoDL"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrdAtvs5uoDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l53utGduoDM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbk7EDJ0uoDM"
      },
      "source": [
        "### Before submit\n",
        "Test that your agent has the right attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-jb3w9PluoDM",
        "outputId": "847185d3-5cf7-4e9d-9979-55ada9882e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "Cumulative Reward: 1.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', map_name=\"8x8\")\n",
        "\n",
        "agent = Agent(env)\n",
        "\n",
        "observation, _ = env.reset()\n",
        "reward, terminated, truncated, info = None, False, False, None\n",
        "rewards = []\n",
        "while not (terminated or truncated):\n",
        "    action = agent.choose_action(observation, reward=reward, terminated=terminated, truncated=truncated, info=info)\n",
        "    print(action)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    rewards.append(reward)\n",
        "print(f'Cumulative Reward: {sum(rewards)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}